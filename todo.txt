It's best to think of tower as a blend of
 - C with familiar syntax for blocks, control flow, loops
 - C++ with familiar concepts to methods and statics
 - C# with the focus on ease of use and simplicity of syntax
 - Rust with the traits and how you can implement them for types not your own (extensions)
 - TypeScript interfaces and types as it's own expression language: (A | B) means it could be A or B

Tower however introduces novel concepts on top:
 - BNF parse rules can be written and extended in tower code itself to extend the language syntax at any pointer
 - The compiler and runtime are combined and can safely execute compiled code during compilation allowing us to use code in tower to generate more tower code, eliminating the need for templates, generics, and other complicated compile time syntaxes.

Tower moves away from the typical AST walking approach when compiling to use a more novel dependency graph with tasks that wait for other tasks to complete.
This allows the Tower compiler to compile full sections of code all the way to code-gen and runtime execution, while another piece of code is waiting for a type check.

It is also important 

Tower is a type safe language with duck typing features that compiles to approaching native performance.

By separating out all steps of the 
By targeting WebAssembly like sandboxes, our compiler can run safely in a sandbox and execute code


------------------------------
When a parse rule matches, do we really want to perform callbacks right there?
 - Or just build the parse tree?
 - We really just want the AST out of it, and then afterwards we can do stuff with the AST
 - But I like the idea of transformers
 - We kind of want any type of logic...
   - Rewrites, transforms, or just running code directly
 - Cool examples of transforms:
   - Calling operator functions: a + b => a.add(b)
   - for loops, etc

We can do everything in passes, but I think a cool generic architecture might be that we just run code until it hits a dependency that isn't met yet, and then like a closure yield, it waits there until the dependency is satisfied and continues

That can even go for type names, like if you refer to a type name but it's not found, as the compiler writer you can choose to defer and wait for the type name to become available

Basically it's symbol lookup (possibly with a predicate?)
If anything is left unsolved, then we omit an error
The symbol we're looking for should be fully qualified, and if that symbol is ever discovered we resume

// This is OK if they are both pointers

class Foo {
  var Bar; // Here we don't know what Bar is, so processing for the AST/parse tree of "var Bar;" hangs until we find Bar
}

// We register Bar here, so know var Bar can resume
class Bar {
  var Foo;
}

We can wait on a fully qualified symbol (by name basically) or for any of the parts of the AST to fully resolve
 - That's how we can do inference
 - Lets think about rust style

fn test() {
  let mut x;
  x = 10;
}

Here the type of 'x' should be incomplete
 - the line "x = 10" should apply a "preferred type" or something like that
 - basically we register an operation to be "inferred"
 - and then later things can register what they prefer
 - and your code handles taking all the preferred usages and turning them to inferred




Lets pretend to implement a basic language



Wasm here, declaraitons and code that runs directly

All code instances run in isolation, but are given access to the parse tree
 - All parse nodes should just be property bags
 - Otherwise it would get unmanagable pretty quick
 - That doesn't mean everything in the langauge needs to be variants
 - Eventually we write an object that binds itself and looks like the parse tree (expose it)

Maybe instead of using strings, we make it easier with 64bit ascii 8 byte "strings"


tower_ast_child(parent_node, index)
tower_ast_remove(node) // Deletes node and all children below it

tower nodes can have properties that point at other nodes, and we will serialize it all properly

Our language should definately support "extern" types that are variably sized and have their own meta interface
 - This way we can bind tower nodes properly

(data (i32.const 0) "Hi")

So part of implementating the language is implementing a generic definition of these AST nodes
 - As well as the parsing algorithm, and WASM emitting / execution
 - I like that this may be the entirety of the bootstrapping for this language
 - Our "Standard" will eventually include the exact details of the LALR(1) parser
 - And then we can make a meta binding library per language, one for C/C++, one for Rust, etc
 - Easy to bind to each
 - Maybe we can translate our language to each language, so we don't need to update, just generate bindings


Like most compilers, we need passes over the AST, it can't all be one pass
 - And we have this idea for the "solver"
 - So how do we register with wasm that we need to execute more passes?
 - In order to think this through, I need to think about how we even compile or include files
 - Because in general the compiler should start with a "prelude" that includes all the core compiler stuff
 - When we add a new rule, we may want to run some wasm immediately to modify the AST
   - But we also want to run some WASM later when we walk the tree (register a tree handler)
   - I had several passes in zilch, including a final code-gen pass
   - Passes solve a lot and are easy to understand, but they have limits and they don't handle general solving well
 - Ideally if you introduce a new type of parse node, you should have to handle it in certain passes
 - We can make up the passes for now, but it's basically like
   - Constant folding
   - Collecting type names first (module style)
   - Assigning types
   - Borrow checker (as an example)
   - Bytecode generation
 - We can blur the lines between these passes by exposing all features needed at all times
   - Maybe we can register our own passes with depencies (must be before, must be after, etc)
   - At any point, technically any of them can code-gen (tower_emit?)
 - Lots of ast helpers (find any parent, etc)
 - In general, I want a super generic solver that works somehow with the AST
   - Imagine that a type is assigned to a node, we can wait for that type
   - Support Rust style inference
 - And then we automatically emit errors if we don't find any
   - Tower nodes should directly serialize to JSON, basically just JSON without any complicated rules
   - They also don't need to be our "variant" in the language, we can handle that later, since we have wasm
 - Unfolded form wast example: https://stackoverflow.com/questions/61399299/how-can-i-make-this-wasm-function-easily-readable-in-its-native-language
 - Might just add an extension to wast for our i64 constants: i64.conststr "test"
   - Could even get away with 9 characters or more if we don't go for ascii encoding
   - If we only allow lowercase and some symbols, 5 bits per letter (26 + 6 extra, underscore, etc)
   - We could fit up to 12 characters with 5 bit encoding, and possibly up to 15 if we get clever with only 27 symbols)
 - https://github.com/WebAssembly/stringref/blob/main/proposals/stringref/Overview.md
   - Can also possibly just support first party strings, might make writing our language easier too
Tower nodes are generic and take an i64 key
 - Tower nodes can also hold a value, basically a variant
 - Can hold integers, floats, any wasm primitive data type, and strings
 - Can also hold children (and be an object) - can't be both an value and object at the same time (keep it JSON)
 - Can also be an array, really just treat a tower node like a JSON object
 - Maybe just take a look at quick js and duktape to see what their api looks like, keep our simple

Since tower nodes are meant to be parse nodes, we may have them point at ranges of begin/end in the text they were created from, as well as possibly a pointer to the source text (or make sure we have some way to read the substrings)
 - So a tower node has some implicit parse node data on the root of it
 - How does that turn into JSON?
 - Technically generated parse nodes wouldn't necessarily need a line associated
 - But it's amazing for errors to always be able to derive a "stack" of where a node came from
   - Basically a member that is an array of infos that tell us how this node was created during tower_node_create()
 - So nodes basically should implicitly store a way to know what tokens they were derived from
 - And a way to print out a stack of how they were created (in expanding rule, etc etc)
 - Most of this should be read only / managed by the compiler, so users can't muck with it
 - Maybe all this information should be savable in the JSON, but it's in a separate table
 - Even if it's json, it would just mean every node is an object, but we have a "value":
 - Ok that's all fine then, does the API use our key value system or is it a new API?
   - Maybe we just reserve those keys, you're not allowed to write over a reserved key
   - Introduced a new collision problem for ourselves, but that's why we have language version bumps
   - It's going to have to happen anyways as we introduce new rules in the language
 - token_start, token_end
 - The root might have more, like source info, etc
 - TypeScript interfaces would give order to this chaos
 - But again, we have many helper methods that assumes we're a JSON style tree
 - tower_find_up, etc treat it like a game engine finding components on an object tree
 - We only store the minimal data we need, no duplicates
 - Tower nodes should be able to safely point at other nodes, but like a weak pointer (node ids)
 - Our types should just be tower nodes too
 - All files are compiled under a single wasm context (maybe we can split this up later by module or something)

So we want the solver to be able to wait for a specific tower node member to be created
 - Basically we want a condition to be satisfied, and once it's satisfied we want our code to run
 - Very simple to think of an idea how we can register a "wait" for this type of node to be attached
 - Does our code run on attachment (potentially during our own code running and making an attachment)
   - or does it run once all code finishes running, like a task
   - I think task based is most suitable for a solver
 - So whatever condition we do triggers a task potentially being activated to run
   - But tasks should have conditions, and the conditions should always be re-checked before we run
   - Since whatever triggered the "check if this task should be run"
 - Tasks have a condition, as well as a trigger
   - The trigger loosely follows the condition, but may trigger more often than the condition
   - The condition's responsibility to only allow it to run if exactly a condition is met
   - The trigger is basically an optimization for not having to call conditions for every task every frame
 - We can do the "every frame" simple solution for now
 - On top of the parse tree, we also want a sort of module / type / member tree
   - It's often a merging and trimming of parse trees, basically a simplified tree that's joined from multiple files
 - Maybe the root has different types of trees, and we can have a 'type' tree
   - How language implementers can do a::b::c nesting
 - Up to your language what kind of extra trees you will need
 - So if we typed in a type name that we wanted to be resolved, we could put a dependency on:
   - root::types::<typename>
   - Or something like that, if it's allowed to be resolved in your module, parent module, etc then
     we could have a multi-depenedency task where "any" condition that gets met satisfies it
   - But not multiple, if we ever see multiple, it's an ambiguity error, unless we have a resolve function
 - Makes our language design really nice I think

If I wanted to make a scripting language that could do this:
```
let p: Player;
struct Player {
}
```

When we came to a node that referenced Player (let p: Player) then we would go to our type tree
and attempt to look up a type, which we would not find yet

Because what I'm going to do is first look up the type by name, and if I find it I will then
tag my own node with that type. So the task I would register would have a condition to find that
type first

And if the task never completes at the end it would emit an error
 - Ideally we have a way to identify that tasks are depending upon the same thing, or upon each other
 - So we don't just emit a huge spam of errors for every single task

So our rule would be like 

Var = "let" name(Id) ":" typename(Id) ";" => register a task with a condition on finding a typename of Id and the code it runs when the condition is met (the side effect) is to add a 'type' member to the Var node (point at the type node)

Conditions can produce a value - e.g. the node that we were waiting on
 - So the side effect code can use that value, like get the type
 - Conditions could be generic wasm, but we might just have a set of supported conditions that we can optimize
   - Maybe support a generic condition... ideally run in a sandbox

Here we actually don't care about "let", ":", or ";" so we should find a way to drop all those as nodes
 - as well as name children nodes

```
let x;
x = 0x01; // consider x as i8
x = 2000; // consider x as i32
```
Since i32 fits both, we use both

We want to be able to implicitly type x as i32 here, rust style
So we also want a way to say that a node 
 
```rust
  let mut x = vec![];

  fn test<T: core::fmt::Debug + Copy>(vec: &Vec<T>) -> T {
    println!("Test: {:?}", vec);
    vec[0]
  }

  let mut y = test(&x);

  let mut z = test(&x);

  y = z;

  //z = 5;
```

5 => run(
)

When I uncomment `z = 5;`, rust can trace the dependency chain of types

y depeneds on the return type of test
but since the return type of test is generic, it depends
 - it's really just being able to solve the chain anywhere along the way

I want our language to be able to solve types like this
 - As well as TypeScript style situations

When I write the line z = 5, we know or can assume 5 is i32
 - and since Z is a local var reference and is not typed yet, we can set the type of z to be i32
 - ideally this triggers typing for call to `test(&x)` somehow
 - Because the line `let mut z = test(&x);` test's return type should also depend on the type of z
 - Seems like a cycle of dependencies

But at the end of the day, all we really care about is that everyone gets a type
 - So maybe tasks can be satisified and cancelled

like y's type depends on the return type of test, but test's return type also depends on y and z
Since z gets solved, then test's return type gets solved, which solves y's type too

I imagine the task for assigning the type of y to the return type of `test(&x)`
 - test is a function call expression, and it should have a result_type
 - We wait for that result type, and our action sets our node's result type to the same type
 - Every local variable node also waits for their parent's result type
 - But when we do `z = 5;` the assignment operator would check if the types match
   and would see that z doesn't have a type yet, it would assign it to rhs type
 - rhs may not have a type yet, so we might need to wait on that too
 - Our assignment operator would wait on a type for the rhs, and optionally get a type for lhs
 - Since we hit a condition that matches rhs type, but no lhs type, we run and assign
   a type to z
 - Since z gets assigned a type, it cancels waiting on the return type of test
 - Note that y is still waiting on the return type of test
 - Since test's return type also has a dependency on the type of both y and z
 - z got assigned a type, so that triggers and sets the return type of test to i32
 - this also cancels test's return type waiting on test's parameter type
 - And test's parameter type depends on the return type too, so that fires
 - which goes all the way back to the vec
 - y also depends upon test's return type, so it fires and resolves

 - We basically want these conditions / waits to be coroutine style pauses/yields

Conditions are basically like:

What we want to accomplish (adding type, cancels if it's already done by someone else)
What we're waiting on to accomplish this

ideally we keep the way we register these as simple so we can establish a clear dependency chain
 - we know this task will add this member, we know this other task is waiting on that member, etc
 - can be multiple tasks that solve the same problem in different ways, so it's more a graph

and then we get stellar errors at the end when we can't complete something

And we just register wasm functions to run based on these conditions

a compile(expression) feature that runs all the code of an expression (including block or closure)
 - it depends upon the bitcode portion of an AST to run (dependency)
 - WASI functions have well defined ways that they run at compile time (and hooks you can override)
   - Can see any folder under the compiled directory, command flags for adding more, etc

A cycle of dependencies is detected when we can no longer run any of the tasks, but there are still tasks pending
 - But it would be nice to explicitly build these dependencies so we know the second a cycle task is added

It would be great if the whole compiler could fit within the runtime too, since the parser is surely part of it
 - I like the idea that there isn't exactly "compile time" but just steps, like you can precompile a bit
 - and then compile more later with more data
 - But ultimately, the runtime can ask if the current call is during a compile step



  // Rust style type inference is actually simple, the first time someone wants to use an inferred node
  // as a certain type, we just automatically register it as that type, and any future changes error

  // Make cleaner way to print out tables
  // Check if predict rules are correct (try in our own Welp too)
  // https://www.usna.edu/Users/cs/wcbrown/courses/F20SI413/firstFollowPredict/ffp.html
  // Things to discover:
  // - Does adding a new production invalidating things we did before? I assume so
  // - Is there any way to incrementally add a production?
  // Next implement BNF + Sets to NFA
  // And then NFA to DFA with shift/reduce stack
  // Basically get this parser actually working with inputs
  // Not sure how much I love rust currently, I mean it's alright but the readability is bleh
  // I think typescript has been the cleanest language for reading so far
  // Just thinking about how much easier this would have been to write in TypeScript...
  // And easier to write UI for debugging...
  // Graphviz, etc
  // Make the whole thing in VIZ
  // I want the first thing we do to be a very minimal implementation of the compiler

the traits in rust are much easier that Typescript interfaces
 - Because they can have default implementations
 - And "classes" in typescript have so many annoying requirements
 - I love that there is no "new", it's just static methods and you have to constructor the class
 - and no private, just internal / crates / modules

but typescript union / subtract, etc is also awesome

Just like we have compile() to evaluate an expression, we should be able to write any type expressions
type1.union(type2)

so if we compile() an expression, and it returns a typeid, we can use that as a T

function foo(bar: i32, baz: compile(if name.contains("a") { typeid(i32) } else { typeid(i64) })
we should also be able to eval(string) which executes codes with the current set of rules where eval lives

----------------------------------------------

The worst part about the TypeScript implementation was the annoying Map behavior
 - If we could make custom object types with hash and equality
   it would be much cleaner than having to use unique strings
The worst part about the rust implementation is that it's extra verbose and lines are long
 - Because of having to get a mutable reference to data, it meant all the pieces of the sets were
   constructed individually and only combined at the end. This isn't bad at all, but having to take
   each by reference, as well as "has_changed" was super annoying. It really does mean do less with objects
 - I also thought all the RefCell and other garbage caused it to be somewhat unreadable
 - I can see how if I understood rust to the T, I would read it like butter
 - But I think the need to be explicit with nearly everything is taking away from what the actual program is
   (the idea of the program, which I should easily get from reading good code)

To be fair, maybe there are much better implementations now that I understand the rust one better
 - Maybe it's easy to clean up and someone could show me the right way
 - But having to like... deref,deref_mut,borrow, borrow_mut... it's just taking away from the code idea itself,
   it's safety and optimizations rolled into one

Next step is BNF Sets + BNF Grammar to NFA, then NFA to DFA

Once we have that, we can start authoring the nodes

I really just feel like I should support LALR(1) only, but the old Welp grammar parsing was interesting

It's a language whose parser is part of the language
 - I think most languages end up hand rolling their parsers so they can do special tricks


----------------------------------------


// I think instead of trying to store weak nodes within nodes (it kind of gets confusing)
// lets just support weak nodes as children (graph nodes basically)
// they aren't considered owned when attached/detached/deleted
// In all our find functions, we should take options, whether we only consider owned nodes or not
// we also want to make sure find never cycles if it indeed is a graph
// I wonder if we should remove "kind/parse_start/parse_end" and just make those members
// basically we can't declare structs yet

/*

AssemblyScript when I try and write A | B:
  Not implemented: union types

This seems like the entire point: we do traits/interfaces plus union types
 - where possible, we don't use interfaces and just deal directly with data
 - and maybe our vtable even has "virtual member offsets" so two structs with different layouts but same members can work
 - basically members can be direct offsets, or get/set, but the compiler oversees this internal detail

possibility to have stack handles, handles to members, handles to values inside an array, etc
 - All properly checked when we dereference the pointer, and we can only use the pointer lifetime temporarily
 - and we focus on the optimizer eliding dereferences and reducing ref count operations as much as possible
 - Find out how to make arrays and slices safe and fast

no garbage collector, but you can leak
 - we could maybe do something like not allowing cycles of pointers

I like the idea that you have to write a:  A | null
 - We make you be explicit about nullability

And we attempt typescript style:
 if (a) { A } else { null }

And we also need dynamic objects too, like indexable typescript objects, so our meta should have dynamic access operators too

Focus on the language being pleasant to use and easy to understand
all reference counted, with explicit delete, and hard locks
easy thread safe models too
100% safe language with no data races, determanism (no undefined / unpredictable behavior, even for containers)
 - not native performance, but still stellar perf for tight loops, etc
 - basically give C# a run for it's money
 - and make it easily embeddable in other languages (small runtime, wasm interpreter, expose wasi functions)
 - make sure the resulting runtime and compiler binary are small too, run on embeddable



Ok, even if in the future we port this structure over, we still want it
It's still a great generic tree structure, and maybe in the future we'll merge it with our object / trait model
I basically want rust, but with everything being dyn traits
and do it C#/Zilch style, structs are copy, but everything else is allocated automatically
 - it just simplifies move/copy behavior
 - but maybe we can reign in handles a bit
 - typescript style where any matching interface is an interface
 - and interfaces can require data (can they add data too, I always loved that idea)
   - Might make ABI compatability annoying, but at that point you're already in C land
 - And we want to be able to do typescript style A | B


In some ways we would want to write in our own language
tower nodes are generic, support virtual behavior, easy to generically iterate over
but could we just create tower nodes in our language as a struct
 - just a bunch of pointer and arrays (would want a dynamic sizing vector at least)

Fascinating idea, we may be able to solve tasks WHILE parsing
 - if we make the parser re-entrant so it's iterative
 - when we resolve a type, we could potentially even allow the type to introduce it's own parse rules
 - it would have to be after some operator or something on the type, "test".
 - I like this idea as a way we could introduce swizzle operators but entirely using grammar rules
 - maybe we can design it where some member access operators get their own rules that temporarily apply
   - but they type would have to be wholly known, no inference
   - let a;
   - a.something // no idea what happens here since we haven't parsed a=64 yet
   - a = 64;
 - Actually as much as I love this, I think it's better to do everything with a "dynamic" access operator that resolves members
   - It can also error, which sends up a compiler error
   - So we can allow any code to handle any member it wants, e.g. swizzles for .xyzw
 - I like this better, it fits that a type can also be generated at compile time


// Maybe all tower nodes should be owned by a parse object, which holds the source code strings (context)
// Here string may just be i64 with a specific compressed ascii encoding to fit 15 chars and underscores

For our sanity, we almost want a way to describe tower node interfaces (like TypeScript / JsonSchemas)
 - and have them be checked / validated
 - maybe this comes later as we advance the language and start writing things in our own language

tower_node_create(context: tower*) -> node*
tower_node_destroy(context: tower*, node: node*) // panic on double destroy, check free list header
tower_node_attach(context: tower*, child: node*, parent: node*) // automatically unlink and relink
tower_node_attach_member(context: tower*, child: node*, parent: node*, member_name: string) // automatically unlink and relink, any member of the same name is removed
tower_node_get_parent(context: tower*, child: node*) -> node* // or null if it's the root
tower_node_get_child_count(context: tower*, parent: node*) -> u32
tower_node_get_child(context: tower*, parent: node*, index: u32) -> node* // or null if not found, panic if out of range
tower_node_get_child_member(context: tower*, parent: node*, member_name: string) -> node* // or null if not found
tower_node_get_kind(context: tower*, node: node*) -> string
tower_node_get_parent_member_name(context: tower*, child: node*) -> string // returns empty string if not named

enum type {
i32,
i64,
f32,
f64,
string,
bytes,
weak node*, // safe, nullable
strong node*, // panic if we attempt to delete the other node without releasing this
}

tower_node_set_value_T(node: node*, value: T)
tower_node_has_value_T(node: node*) -> bool
tower_node_get_value_T(node: node*, default: T) -> T // uses default if T is not valid
tower_node_expect_value_T(node: node*) -> T // error if the value is not correct

// all the traversal orders, probably better correct names for this
// any combination of these
bitfield order {
  self,
  up,
  children,
  down_breadth_first,
  down_depth_first,
  siblings,
  ancestors_children,
  etc
  // do some orders that walk up, but also visit siblings
}

tower_node_find(node: node*, order: order, predicate: callback) -> node*
tower_node_find_kind(node: node*, order: order, kind: string) -> node*
tower_node_find_type(node: node*, order: order, type: type) -> node*
tower_node_find_member(node: node*, order: order, member_name: string) -> node*

We should find a way to make all the find functions take a previous value, so they can find the "next" one
 - We can use this for iteration
 - callback?

tower values are kind of already arrays because they can have indexed children
they are also sort of string maps and variants too

Solver:
 - We want the solver to be able to wait on children of a specific kind, type, members, etc
 - Almost want paths, or relative paths, to a specific node
 - We want these dependencies to be easily analyzed, so we can track cyclic dependencies and have better errors
 - We can still run manual code to check dependencies more specifically (a test function)
 - Some tasks can run on nodes with no dependencies, basically what it looks like when we do => at the end of a rule or production
   - That defines a task to run when the production completes, but it has no dependencies
   - I guess you could say it has a dependency on the node's existance
   - If someone else runs before and deletes the sub-tree, that task shouldn't run anymore
 - So it just has a dependence on the completed "sub tree root" node itself
   - Kind of the same feature as the cancel, tasks can have conditions in which they self cancel
   - One of those conditions is basically always that the task's associated node gets deleted
     - But maybe we can expand this concept to require multiple nodes, etc

 - Generally when I look at the types of zilch walkers, we really just needed the basic ones that ran on the nodes themselves
 - And then the ones that ran once we figured out the types
 - 

Tasks should run in order of those with 0 dependencies, and the order they were scheduled
 - If it makes sense we can add a priority to differentiate within those levels, but I hate priorities
 - Dependencies make much better priorities
 - The run order makes sense, for example literal constant nodes would run first
 - Local variable references would try and find an associated variable up the stack
   - Is that a dependence?
   - I imagine in Zilch I had something where as I walked down, I pushed things onto a scope stack
   - We could also search the tree, but it's a bit annoying as it involves searching 

{
  let a;
  {
    let b;
    a + b;
  }
}

The reference to a here has to scan up it's parent until it find's any body that can hold local variable declarations
 - Then it must scan all the statements to find a local variable reference
 - I believe in Zilch we did a few passes, one where local variable declarations added themselves to scopes
 - And another where as we walked down, those scopes pushed onto a stack where we could look them up easily
 - This would be a really interesting language toolkit thing, but how do we solve this with our nodes?

`let a` could create a task that only has a dependency upon it's parent
 - That task could push itself into some sort of parent array of in scope items
 - But that's a hard thing to depend on, ideally someone else would want to depend on like "all scope items complete"
 - But that's not a concrete thing, that array would just keep getting filled
 - So maybe another feature is needed here, this is kind of like passes, we need to make sure an entire pass is completed
 - it's like having a dependency on a group, maybe that's the better idea
 - we can group tasks under a single group (or group name) and you can have a dependency upon that group being completed
 - That sort of eliminates the idea of passes, and a group is really kind of just an "and" dependency
   - Maybe that's it, our dependencies can be an expression not just a node
   - And for convenience, we can just label tasks with a group name and depend upon that group names (automatically depeneds on all those tasks)
 - rather than passing scope down as we walk down the tree, because we no longer "walk down", we just execute tasks
 - then we really just need good tree find functions
 - So 'a' really needs to make a task that's dependent upon the scope it's within
   - But that's not an immediate parent
   - Maybe dependencies can be fluid like that, they can output a node...
   - Like dependencies are re-evaluated every time, parents can change, etc
   - And we can build a dependency graph from evaluated node pointers
 - We only need to re-evaluate these when the graph changes, maybe we can keep track of who needs re-evaluation instead
 - I like this as the primary thing
   - Maybe we can even use the traversal's in our dependencies
   - When we add, delete, detach, reattach nodes, we need a way to know who needs to be updated / notified
   - Maybe when evaluating a dependency, we mark all nodes in the path used to get there
     - And if we change anything about those nodes, we know who needs to be updated

I can imagine that tower nodes can actually be pretty complicated, because they know who needs to be updated if they update

I'm imaginging that every time we evaluate a path, we record all the nodes used to get there
 - paths are evaluated left to right, on a find first basis
 - if the path can't be completed (e.g. nodes don't exist) then the last node we found we register a "waiting for this"
 - that way, if the node is ever modified we know who needs to be updated
 - we should call those 'partial evaluated paths' or 'complete evaluated paths'
 - when we modify the structure

this seems cool, but for now lets go with a much less complicated "ask the tasks"

yeah, part of the point of tasks was to say what they output too

Ok maybe dependencies are just functions, but they are expected to output evaluated paths
and since we actually evaluate node, the result is one or more (remember conditions) of evaluated paths

evaluated path:
 - the most recent node* we found
 - the rest of the path that we didn't get to

paths should always be normalized as they will be compared

so when we attempt to ask the + operator if it's task is ready to run, it checks all it's dependencies
 - it may have lhs and rhs, but maybe they haven't evaluated types yet
 - so it returns the most recent pointer to lhs, with a remaining ".type" to be evaluated
 - another task can say that it outputs "self.type", and since it's the child node
   - we evaluate self easily, but ".type" has not been output yet so the partial path is emitted
   - We can match up the dependency with the output to build a graph
 - dependencies and outputs are parts of tasks that we always ask for
   - it may fluidly change with each iteration, so maybe it's best if we don't try to make an efficent graph for now
 - at some point we can make optimizations to the solver algorithm if we need to

the tower_find functions should output partial or complete paths
 - if it's complete, we keep going, if it's partial, we stop and output that (like a rust Err check)

instead of saying ancestor's immediate children, why don't we just allow find to evaluate paths
 - so basically for every node, in the find order, it will attempt to find another path
 - first to evaluate wins

+ operator:
  dependencies:
    self.lhs.type
    self.rhs.type
  outputs:
    self.type

local variable reference:
  dependencies:
    find(up, kind(scope), find(children, kind(local_variable), .name{a}))
  outputs:
    self.type

there's not one single scope, so I guess when we evaluate the partials we need to output an "any of these"
 - the whole point is better error messages, and basically what we're trying to do is point out all the places
   that we might find a local variable
 - I almost think it's better if we maintain some sort of scope concept
 - Maybe this isn't really a dependency, it's not something we really output and say hey we're waiting for this

What would scope look like as a concept
 - maybe there are scope types that are just string identifiers
 - a local variable declaration would add itself to the nearest parent scope
 - we'd need to define if the language allows shadowing, all that, kinda gets messy
 - we could always just do find first / nearest, and implementors can make separate tasks to emit shadowing errors
 - this is also where "using" could come in
 - basically scope is a short hand symbol table
 - I kind of like this concept, especially since they are just nodes in the scope table


dependency:
 - on a path
 - on a node
 - on a group of nodes
 - on a group name
 - on a and/or condition
 - find(order) + path


tower_task(target: node*, test_callback_no_side_effects, callback)


Maybe we also check outputs, and if an output is satisfied, then we don't run it / cancel the task
 - can be an option on tasks

tower nodes can be registered to act as scope collectors
 - tasks can add to the scope
 - local variable declarations could just walk up to their parent scope and add declarations in
 - makes sense, since:
   using foo;
   bar();
   using bar;
 - Here, bar should not be visible, so we should do them in order (top to bottom)
 - Technically just by parse order we will visit things in this order
 - Using declarations should just bring in weak pointers to nodes
 - basically we want a tree that is just visible scopes
 - technically some of the things may not be AST nodes, they could be just symbols
   - that's really what we want types to be

What things do tower nodes need?
 - the start and end indices from parsing
 - the string they parsed? not really, can technically get that from start/end and the source str
   - but a helper to get that string would be really good
 - ability to store values and constants
 - A tower node has children, but it's sort of a variant too
 - how do we know what an "expression" node is? is there a type string on the tower node?

 - I almost want tower nodes to attach whatever binary data they want
 - In json, an object only has children if it has a member for children

 - Start/end for parsing, when we create a new node we may not have a start/end
 - But we can require it for all nodes, it might make it easier
 - Technically all nodes should be derived from some sort of source, even if generated
 - Should nodes just have a pointer directly into the string? I kinda like that, and a length


rust separates out traits and structs, do we even need to?
 - what if there were only traits, and traits could have data members, and you can implement traits for other traits
 - what does this look like
 - this means that our sizes of our structs change as we compile more things in
   - meaning that loading code at runtime might be odd since it causes structs to change size
   - maybe if an interface needs data, we have to create it with & including that data

trait Enemy {
  lives: i32;
  health: i32;

  fn hurt(amount: i32) {
    this.health -= amount;
    if (this.health < 0) {
      --this.lives;
      this.health = 100;
    }
  }
}

trait Named {
  name: string;

  fn hello() {
    console.printline("hello from `this.name`");
  }
}

trait Health {
  health: i32 = 200; // can have defaults
}

let e = Enemy {
  health: 100,
  lives: 3
};
e.hurt(12);

Enemy {  lives: 3 }; // error, no health
Enemy & Health {  lives: 3 }; // ok, since Health has a default

let h: Health = e; // ok, because even though we never implemented Health, it implicitly matches as an interface

trait Debug {
  fn print(self: Self);
}

impl Debug for Enemy {
  fn print(self: Self) {
    console.printline("health: `self.health`");
    console.printline("lives: `self.lives`");
  }
}

e.print();

// Since Debug is a functional only trait, and more importantly Enemy implements everything Debug does, then it shows up
let d: Debug = e;
d.print();
d.lives = 5; // error

impl Named for Enemy {
  // override / custom implementation
  fn hello() {
    console.printline("roar I'm `this.name`");
  }
}

let n: Named = e; // does not work, since Named adds data and e was not allocated with that data

let e2 = Named & Enemy {
  name: "boss",
  lives: 5,
  health: 1000,
};

let n2: Named = e2; // legal now

let eref: Enemy = e2; // sliced off Named from the type
let nref: Named = eref; // dynamically pulls the trait/interface out

and we have 'instanceof' and the other kind of same stuff


I do really like the idea of anyone within the same module / crate as being able to add data without requiring the allocation
 - but we'd have to know everything up front
 - we'd also have to have default values, what are they allocated as?
 - it's more explicit if we have to force the user to allocate the types
 - but I really like that we just don't really have types


It's possible that we effectively get the diamond of death now:

impl Test for A;
impl Test for B;

let x = A & B {};

impl Test for A & B; // can we have this? I don't see why not, we're being way less strict with types
 - it would have to be something where we basically normalize a type (meaning all A & B are the same)
 - and we then have some specificity rule, eg "x as Test" will use this
 - what about like, A & B & C? ideally since A & B is more specific it will use that logic

let t: Test = x; // here this would be an error, or maybe we have some algorithm for choosing which one
 - We can try and enforce the rust style rule for who gets to implement
 - I don't mind some rule about choosing one, and it involves "closeness" to the call site
   - e.g. we pick implementations within the same module first,

Technically we have two different problems:
 - two different people from different modules implementing a trait for a common type
 - but also since we now allow union types, union types introduce multiple of the same trait implementations
 - Maybe the order of the union actually matters, e.g. A | B, vs B | A, it just means when we dynamically
   pull off an interface, we'll go in order of the types as declared when we constructed it
   a = A & B {}
   a = B & A {}
 - I still think we're going to end up in scenarios where it's not all that clear
 - we can always double cast for explicitness, e.g. `a as B as Test` or maybe even `a as B:Test` (B's version of Test)
 - technically we only care about call sites where we extract the interface, but that call site could be in internal code
 - but maybe we care about who is calling... like the actual call stack?

 - I actually don't mind the idea that it's based on the order of A & B, or B & A, when allocated
 - the & forms it's own tree, and we basically just go order first, (e.g. if A is also a combo)
 - we also go most derived first, e.g.


trait A implements X {
}

trait A implements Test {
}

trait X implements Test {
}

trait B implements Test {
}

// Can do this
trait A implements X & Copy {...}
// Or separately
trait A implements Copy;

the only question becomes if we get inherited data, because previously we said that you had to do A & B
 - but I don't want you to have to always write Player & Component {} to create a component
 - this is why data gets weird, because if you have to be explicit every time...

Maybe we can allow you to add data, but if any instances of a class exist then you can't

adding data I think would annoy people, since we can suddenly add data to primitives and standard library types
 - which is cool, but like, also annoying
 - but also cool in its own way, just bringing libraries in might be kinda gross


let a = A & B {};

let t: Test = a; // Here we get A's version of Test, since A is first, and lowest on the tree

and we can't have cycles of trait impls, e.g trait Test implements

it's a little more loosy goosy

how do we decide the whole class/struct thing? if all we have is traits...
 - we can determine if something is implicitly copyable pretty easily (only primitives, no handles / ref counts)
 - we can use & or ref, but I wish we didn't have to, I don't like extra syntax if we can avoid it
 - maybe we can say on a trait, and when we use that trait
 - or maybe all traits are allocated, but we can say "implements Copy" or other special traits
 - And that means the value is now copyable, but we can't have any other non copyable values
   - we can't add any non-copyable members then
 - I don't like the idea that something we do can perminantly change usage everywhere
 - or fine, if you suddenly turn a trait into Copy, we have to update all places in code

The whole idea that we change code and it changes everywhere makes all sorts of weird problems
 - we basically can't compile code in a static way because new members have to be addable (and defaulted, otherwise it breaks code)
 - that means you really can't ever "compile" a library, we're always going back through when types are modified
 - that is unless we make it some sort of dynamic type, but that defeats the purpose, we want like 80% native performance
 - adding members would change sizes, initialization code, etc, it would have to be recompiled everywhere it's used

why do I want that model?
 - basically it's the same as having ability to implement traits for any type and having collision resolution
 - but that happens at runtime, and potentially we can make that into a call so we abstract away the behavior and don't need to recompile
 - e.g. the wasm stays exactly the same since it's all dynamic dispatch as a boundary
 - but suddenly when we change sizes, the actual code itself has to recompile, especially since initialization logic has to run
 - unless initialization logic is like pre-constructor in zilch (sets all values to defaults)
 - but still sizes change, and wasm doesn't have structs, so you really have to do structs manually

we also could get clever and cache instances of compilations, so we don't need to keep recompiling because we know the size
  hasn't changed (but if you change that struct, it causes recompile, just basic minimimal recompilation)

any time dependency-libraries change dependent libraries (e.g. my lib changes std lib) causes recompile, all parts of the tree
 - you can dynamically load libraries into an existing runtime, but if they change parent libraries then it requires recompile
 - if there are existing types... well
 - we could do the zilch thing where we rebase and reinitialize all heap types / new members
   - would have to support default initialization always then

is there ever a case where the standard library needs to recompile
 - I'm not a huge fan of the fact that adding Copy suddenly changes how things compile
 - like previously even if I change sizes (which would mean we need to re-emit bitcode)
 - it did not cause me to re-check types, as we we're only able to add to types (and you could say the whole it has to be in scope thing too)
 - 

if we were able to add to std library traits, can that ever cause a compile error or ambiguity in the standard library?
 - yes in that

module Std {
  interface Test {
    fn test(): i32;
  }

  interface Standard implements Test {
    fn test(): i32 {
      return 1;
    }
  }
}

// Standard brings in Test because the only definition we see of it shows Test
// basically the only ones that see our new additions are those who also see our module

trait Foo implements A & B {} // error, it's known at compile time that these conflict
// Maybe this syntax makes less sense anyway, we're trying to jam two things together (which is which?)
// or maybe A & B is basically a unique type

interface BetterTest {
  fn test(): f32;
}

interface std.Standard implements BetterTest {
  fn test(): f32 {
    return 123.0;
  }
}

// but now this is an error
let s = std.Standard{...};
s.test(); // error, ambiguous, which one does it call? since both are in scope and only change return value
// we can make it so overloading always requires disambiguation for now, or can make some sort of select algorithm
(s as BetterTest).test();

trait Foo implements A {}
trait Foo implements B {}

so generally no, adding a new member will never cause a compile error because the base module doesn't even see it
 - even when it adds size
 - but maybe we can only say implements Copy in the same module it first appears in
 - so some traits that control compiler details must be implemented at the declaring module level
 - means we need to code-gen again, but not semantic analysis
 - note that operator overloading should still work since a + b is just a.add(b), so implementing the add trait is easy

we can only declare copy in the declaring module, but anywhere within the module

right now we can basically do single dispatch with dynamically pulling out interfaces
- e.g. virtualism: sphere prints radius, cube prints extents, etc
- maybe we do allow overloading, and our overloading technique can handle dynamic dispatch
- and since anyone can implement the trait anywhere, free dynamic dispatch

trait Shape;

trait Collider {
  fn collide(a: Shape, b: Shape): bool {
    return false;
  }
}

trait Collider {
  fn collide(a: Sphere, b: Box) {
  }
}

// since there's no self we can call it like a static...
Collider.collide()

collide(a, b) // pick the right version of collide?
// we have to use some algorithm for specificity, but it's cool to provide it
// overloads can just be a way to provide dynamic dispatch, and since anyone can implmement the overload anywhere...

since we have union types and all that, 

What if we do everything like this syntax:

type Collider {
};

Ignore all the += stuff, basically what we're saying is that any {} is a type like in TypeScript
 - but we alias it with names for convenience
 - but also, name aliases act as the group holder for interfaces

Can we grab any interface "just because it works"?
 - well if it imposes no restrictions on the type (it matches) and has default functions, then yes
 - in typescript interfaces don't have defaults (function implementation, default values, etc)
 - so it's not a worry since casting interfaces is always checking that data matches
   - it only makes sense for an interface with function defaults, but members/data make no sense

The way we do type and interface mapping should be entirely by which members fit
 - so for example, when we say type A implements B, what we're really saying is here is an implementation for B
 - that works with anything that looks like an A (it can even have more members than an A, e.g. A & C, or just a new type D that quacks)
 - That means that type names are actually irrelevant, we just use them to identify types
 - so a big operator we need to implement is "does this interface / data layout match this interface"
   - or is there an implementation of it, or can we cover 
 - for allocated objects, why don't we make the pointer at the base of the object able to attach components / new interfaces at runtime
   - component based design basically
   - nah I think rather than making that something we just magically do for users, I like just overloading the operator
   - we can build a standard Composition interface

when we do implements, all data members must be initialized

type X implements Y; // that's just a statement basically, Y must be complete

the component idea is basically instead of actually extending the type with new members, we attach the members as a component interface
 - but then our language would need to bridge the gap between "owner" and "self"
 - I guess in interfaces, that could be a thing, self means the interface itself, and owner means the one composing it
 - But, I think the part we're missing here is that we don't just want to be able to get C# style interfaces where a thing "IS" that interface
 - we want it to work where anything that can be, IS
 - true duck typing, but still very type safe
 - and use strict style TypeScript for explicit null checks, eliminate that issue
 - and add some sort of cool match expression in to match interfaces and destructure
   - we can play to rust strengths

// in type parsing:
// Y {}
// means taking an abstract or complete interface Y and implementing it (all functions must be implemented)


type Collider implements Copyable;

// legal syntax
type Collider implements {fn foo();} {
  fn foo() {
  }
}

we could even do

type {health: i32} implements {
  print(self) {
    console.log("I have `self.health` health");
  }
}

let monster = {
  health: 100
};

// Now any type with a health: i32 matches the definition and gets a print function (if this is in scope)

So that means we should be able to basically do this


let f : {fn: print()} = monster; // legal cast, since we implemented print and it's in scope

normalize all types and make them all comparable



Basically as a parsed type, {} is a trait/interface/struct

traits are all basically compared purely on "if it matches it works"
 - which means that type names are really only a convenience
 - but no, we really do want type names to be a single trait, since we want to attach things to it later
 - and 

I can extend a type anywhere

type Collider += A + B;

anywhere that has an unimplemented interface needs an in place implementation, maybe that's just a { } following type expressions

think of this like JSON and typescript

I want to be able to do:

let a = {
  a: 5,
  test: (a: i32) => {
    console.log("a", a);
  }
};

typescript interfaces are all compile time, there is no "dynamic cast" which is unfortunate
 - we'd love the data to be validated and to fit within the interface

typescript/javascript have classes, and that has instanceof, but that's only a single chain of inheritance check
 - basically what we want is data compatability checks and a sort of as
 - but functions make that kinda weird, since it's like "static data" (kind of the same as two constants)
   - ok so this is a new paradigm, where we can pull out interfaces dynamically based on the data
   - it's sort of like, imagine taking every allocated struct with all it's data members
   - and then meticulously go through all named interfaces and generate a cast if it can be
   - and the confusion comes from when we can have more than one implementation of a thing

we should also be able to implement the is/as/has operator ourselves - component lookup
 - only called when the built in interface lookup fails

// typescript
let a: number | string;
if (typeof a === "number") {
}
let b = a as number;

if (a is number) {
 // a is a number here
}

now ast nodes would make more sense as just compositions that hold interfaces
that seems a lot nicer

basically now we just need to optimize interface lookup
 - and ideally anything that can be statically figured out should be

maybe a "fat" handle to a container locks it, and modifying it means 
 - containers can implement returning a 'fat handle', maybe just an i32 + handle to container
 - any existing fat handle locks the original value (can't call any mutating interface?)

we always return a vtable basically for an interface

so part of our language's algorithm should be to match up vtables
 - but the weird part is that we can extract them even not knowing if they implement that or have it
 - sort of makes sense, ultimately you can in typescript and javascript
 - in C# you can pull any interface off a class, or any derived class (or null if it doesn't implement it)

we do need the allocated object to have some sort of vtable pointer, so we can recast
 - basically we just need to know with any piece of data sitting wherever it's allocated, what is the type
 - because we may be pointing at the data through a random interface pointer and need to change
 - so it's like, what's the actual data type, the memory layout, etc (member types)
   - in a way, do we even care about the type (e.g. any methods) or is it really just data?

lets imagine we're making a player and an enemy, right now the only difference is what they say

type Player = {
  speak(self) {
    ...
  }
};

// OR

// This is partial because it has no implementation for speak, so speak must be implemented (no default)
type Speaker = {
  speak(self: Self);
};

type HasColor += {
  getColor(self: Self): i32;
};

type Player += {
  name: string;
  speak(self: Self) {
    console.printline("Hi my name is `self.name` how are you?");
  }
}

Ok this whole idea is stupid and flawed, we're not unioning types when we implement an interface for a type
 - implementing an interface for a type is fundamentally different than extending the type
 - i'm implementing an interface FOR a type
 - in C#, only the class itself could do this
 - in TypeScript, there is no such thing really, it's all dynamic under the hood
 - what I really want is that "anyone who looks like this can fit in this interface"
 - but interfaces can also be allocated, it's not like C# where they aren't full types
 - so in many ways, no difference than structs (data) and functions
 - but then we ask what it means to implement an interface for another interface
   - especially because interfaces can have full function implementations in them
   - 


let t: HasColor = p; // error since Player does not implement getColor / does not match the interface
uld implicitly cast to Speaker because of the speak function
type Player += Speaker;

type Player += HasColor; // error, missing implementation for getColor

type Player += HasColor {
}; // error, getColor was not implemented

type Player &= HasColor {
}; // error, getColor was not implemented

When you write a type expression, and then you put {} after it, ALL must be redeclared, no partials
you can re-declare without an implementation however (e.g. fn foo();)

type Enemy = {
  health: i32;
  speak(self: Player) {
    console.printline("arrrrgg I only have `self.health` hp!");
  }
// technically OK, but a little silly since player already implements speak, this is basically a NOP
// Player already co
}

let e = Enemy { health: 100 };
let p = Player { name: "Bob" };
let s: Speaker = p; // implicit cast to an interface


// basically, we want our language to be almost as easy to use as a dynamic language with little casts or anything
// but still as close to native as we can get with eliding and assuming as much as we can
// so a 100% safe language, but running as fast as we can make it go without unsafe behavior (like a scripting language)



Lets come up with some examples of how we want interface selection to work

type A = {
  health: i32;
};

type A implements {
  speak: fn(self) {
    console.log("hi `self.health`");
  }
};


// In the same module or if the above was in any of our dependencies, this would be an error
// Rust avoids this by saying you can only implement your traits for other types, or other traits for your types, or if they're both yours
// We could adopt that same rule, but it doesn't prevent the case where there are multiple interfaces that satisfy the same thing
type A implements {
  speak: fn(self) {
    console.log("yoooo `self.health`");
  }
};

if the above were in another module, now we have two conflicting definitions of speak
even if we gave the interfaces names... they are compared member by member
 - maybe we could say something like, if a type is named, then if you pull it out by name it will attempt to pull a complete
   implemented interface registered to that name first, and then member by member (duck typing)
 - I kinda like that, it's a little odd but it's complete
 - but I'm also not sure I like the idea of giving names more meaning
 - or we could say something along the lines of, an exact interface match when doing implements/runtime lookup
   will always prioritize over member by member matching
 - I mean, we could even start with that for a long time, implementation's only (exact matches)

 - the beauty of not just doing typescript style interfaces is that we can implmement interfaces for other classes without
   necessarily adding data
 - like in TypeScript, trying to turn a type into an iterator means most likely wrapping it in a whole data class
 - but here we can probably just implement an iterator interface (no extra allocation, it's all static)

all type expressions have a defined order that they walk to pull out interfaces

basically what we're saying, when you pull an interface out of a type, you're really just doing it member by member
 the fact that you said A implements B, and then you pulled out B later, it's not actually pulling out B, it's pulling out
 every available member by matching name and type

So it means that we basically need to worry about making members and their types unique
 - because we're saying if the members match exactly, then they are considered as implementing the interface
 - conflicts can become pretty strange, I think

Ok so the algorithm first prioritizes exact interface matches first
 - and since we want anyone to extend from anywhere, and we are ignoring type names, then


hmm, if we do it this way we're basically tossing out type names (they are just aliases)

it's also a little odd since we're saying X implements Y, but if Y adds members to X, does everyone
who creates X automatically have to allocate the members of Y, (any type that looks like X?)
 - it makes more sense when we're using type names and it's not just "anyone who matches"
 - X implements Y, X could always have to be a type name and not just a type?



module X
A implements B

module Y
A implements B

module X
casts A to B, we should get module X's B implementation

module X
calls into std code and passes A
std code casts A to B, we should get module X's B implementation

basically we want the "closest to the call site"
 - since STD has no implementation and sees no implementation
 - module X was the last caller that had a clear result

one reason we want to do it like this where names are just aliases is because we already have type expressions
 - and it's weird to have full types with names, and then also a random expression language

we could require that all things we want to "impl" we have to alias with type names
 - like we're attaching to the type names themselves, not to the types
 - then we could enforce the rust rule about impl

I think the closesness to callsite example would be best
 - or we just make it an error for two modules to be imported that both make the same change
 - maybe you just have to resolve it, or maybe it's resolve automatically by module import order
 - kinda sucks but it's less runtime than "by the call stack"

what we're basically going to say is, hey we have this allocated interface with members laid out in whatever form
 - are there any exact matches to the interface that we're looking up
 - if there are more than one exact matches, which one is closest to the call site
 - ideally we do this all in constant time
   - every module has a table that is "if you ask X for Y, this is the Y (or none)"
   - we just walk up the module call stack

if interfaces couldn't add data, (but could add getter/setters) then this might be ok
 - it's not weird because we're never actually allocating anything for them
 - basically when we go to implement an interface that has a member, we can do it with members, or with getters/setters
 - that also eliminates the issue of how we set the values of those added members (default initialization, pre-constructors, etc)
 - in rust traits don't need that because they don't have members

maybe I drop the idea of extending other types with members
 - because without separating data, it complicates that question

something like

type Bridge = {
  length: i32;
};

type Bridge implements {
  height: i32;
};

It looks like I just added height to bridge, but really if I stick to my definition of the language
then I technically added it to ANY type that looks like a Bridge, e.g. has a length: i32
That means it suddenly might add that height to arrays

Maybe this is a reason to use type names
 - We can use type expressions everywhere, but we're encouraged to use type names because they hold the interface bindings

OR!!

when we do type X implements Y {}; you're only allowed to add members in the implementation if X is a type name
 - otherwise it has to be a getter/setter
 - note that means when we do interface lookup, that's a specific scenario, hmm actually not really
 - it's nothing more special than looking up an interface on a type, but we just need to know the TypeName itself has its own that we also need to look through
 - yeah that's it basically, a shared type may have other interfaces registered for it (if it looks like X, it also implements Y)

ideally should have no difference between closures, methods, and functions
 - functions are effectively just statics
 - static means it's part of the type, instance means it's a data member (like a closure)
 - by default it's instance mutable, vs static const
 - normally in C++ a static method meant it's part of the type, but the only reason we had to do that
   was because you don't declare this/self, so static means no this
 - But here it's static because we only need one definition of it
 - single vs shared vs static, which one, unqiue? type (since it's part of the type?)
 - per_instance vs per_type

 - visibility is just a function too, takes type ids
 - default is public (all access allowed)

type Player implements {
  public unique const max_lives: i32 = 64;

  unique const speak = fn(self: Self): void {
    console.log("hi", self.length);
  }

  fn speak() { // shorthand for unique const speak = fn(): void {
  }

  private instance mutable length: i32;
};

If you only ever use a class on the stack, we should find a way to avoid allocation


// this is saying there is a data member foo that is default initialized to a closure

type Foo implements {
  instance mutable foo = fn(a: i32): i64 {
    return (a * 2) as i64;
  }
}

it means it also can be changed
does const imply static/shared/single/unique/type?
 - because what is the point of having a const
 - well really const is just saying the interface only supports a getter for this
 - so if it's a const instance, that means it's a getter

 and any const static function that takes self will appear on the interface too
 since we can access static members from an instance too, then 

a = {
  mutable instance speak: fn(): void {}
}

type X implements {
  fn speak() {}; // this is const static
}

let x: X = a;
 - this is OK?
 x.speak();

I like how simple this language is
lets talk about safety, is it all going to work

how do we safely point at stack types?

we want to be able to basically pass everything by pointer, including structs
 (except small enough data types / primitives)

if I have a Player with a position, I want to be able to pass it in to every function as a reference

the only way that's safe is if either we "lock" the object so it can't be deleted
or we have to increment the reference count

Lets not support "delete", just remove all references if you want to delete
 - weak references

that would mean we always pass fat pointers, basically an owning type that needs to be ref counted 

we can never declare or hold a ref to a copyable/value type
 - it's ok because as long as we inc ref before entering a function, and we have no delete, we're guaranteed it stays alive
 - ideally we can do optimizations to reduce the inc/dec (like if we can see a whole function does multiple)

And we can safely take pointers to structs on the stack because we can't store it (it always copies)
 - so passing values is always by pointer unless it's primitive

What about the whole read/write issue, before thinking about threads, lets think about containers and iterators
,
Can we ever store pointers to members
 - fat pointers yes, because we could store a ref counted pointer to the root of the object and an offset
 - For value types, we always take by pointer, but maybe we can say like "ref value" to mean take a fat pointer
   - If we just have a value, we can't store it in a ref Value

For threading:
 - Maybe it's ok to pass or share an object
 - we can clone/serialize objects to pass over boundaries
 - but also any object with ref-count 1 can be transferred
 - sub objects can have different ref counts, do we need to traverse all child objects?
   - basically almost the same as serialization but only checking refs (can even have cycles and > 1 count but must be contained)
 - maybe non-const statics always have a mutex?
   - ehh, but we can't ensure it doesn't point at something else shared, hmm well actually we can because it would have to grab another static
 - someone could allocate an object and point at a static's member

really just JS transferrable model with workers, can't really think of anything better
 - maybe we can have some specialized containers that support threading, maybe memorycopyable values only (no refs) etc


do we support slices too?

And is there any issue with copy values having ref counted values
 - not really, but we may want to know if a value is MemoryCopyable vs Copyable (optimization)

we can have a ref, but not a ref to a ref, otherwise we would need a pointer that keeps multiple things alive
 - we can support pointer to member syntax so you can build whatever kind of fat pointers you want

moving a ref counted value does nothing, right, since the ref count stays the same, the pointer is still valid
 - can't move something with an internal ref
 - but we don't need to move anything really in this language

weak ref is kinda the way, but I don't like Weak<T>, I almost want T | null to just mean weak
 - but that's kinda weird, the user didn't sign up for that technically
 - but as a simple language I kinda love it, if you want it to be a weak reference, Player | null
 - now if all other references to player go away, this one will not keep player alive (it will go to null)
 - Player | Enemy | Targettable; // this would mean Player Enemy and Target are all weak references
 - type Targettable = Target | null;


function calls are syntax sugar
foo(1, 2, 3)
is actually
foo.call@(1, 2, 3)

call is a sort of keyword, because how does ( not expand again to a call
 - somehow we can enforce that it only expands once, but that's odd)
 - @() is a special function call syntax that does not expand
 - you can in fact write this syntax yourself, but generally it's hidden (same with `a.add(b)`)

a + b
a.add(b)

the only reason this matters is that all types should be compatible
 - a closure / function has a type, and that should be implementable by an interface

type Player implements fn(a: i32): void {
  fn call(a: i32) {

  }
}

let p = Player {};
p(5); // works!

use C# style generics, templates do not actually cause tree copying/expansion
 - we may choose as a final release step to allow templates to be instantiated and optimized individually
 - ideally, our language operates in this mode that in development everything is super fast to compile, like a scripting language
 - but in release, we run as many optimizations, inlining everything we can, eliding ref counts, really keep an eye on perf
 - I want it to be like 85% native perf, ideally beat C# with predictable and no GCs
 - as easy to use as TypeScript with sane containers, in Rust style
 - no invisible nulls, everything force checked, with match statements like Rust too
 - but super easy to read, and everything just makes sense and is easy to use
 - plus expand the language with ability to customize parse rules however we want
 - and compile time steps, evaluates wasm on the fly to get constants (plus WASI well defined way of running in compile mode)
 - it's gonna be a kick ass cool language
 - for each will be a feature you can include, whole parts of the language you can toss out
 - people will build crazy libraries on top, compiler is very much part of the runtime
 - all runs in browsers, all in wasm, completely safe because our language ensures safety, and because wasm sandbox

side note, maybe when you give something a type alias, type aliases cannot be assigned to each other without a cast
type A = i32;
type B = i32;

let a: A = 5;
let b: B = a; // error, a is of type A an alias for i32, but needs an explicit cast
basically compatable for all interfaces and things we can pull, but just that one case helps


the fundamental types we have are objects {}, arrays [], tuples? (), primitives i32, null?, string, etc, and functions/delegates

another type of object we can share in threading are fundamentally immutable types, like our string should be
 - that's how you could share string constants

how do we implement weak ref?

// great example, i32 should actually be a type/interface
// i32 can fundamentally declare members of a size;
// but this is a perfect example where we're saying Player implements everything an i32 is
// in our case, we could implement it all with just functions, or it could actually add the data member
type Player implements i32 {
  // technically because Player is a type name, it implicitly gets the member private value: $i32; without us having to
  // implement it manually
}

type i32 implements AddOp {
  fundamental i32; // special declaration that says this type 
  public fn add(rhs: i32): i32 {...};
}

I would love to see the actual implementation of i32 in the language
 - like how it can emit wasm instructions, everything, it would be so cool if the type could do all that


named types can be forced to implement members, but when it comes to i32, since it is a primitive it actually IS the value itself

i32 being a struct and having a private 32 bit sized member makes sense, but what's the type of that member?
 - where does the recursion end, like what if I try to say I implement $i32 (maybe that's just illegal... weird)
 - I don't like this, I'd rather just have some sort of keyword that's like this is a fundamental
 - I like that it's a member because it's always awkward to say you are a primitive
 - maybe we just reserve a keyword for members
   - it means the type must be one of the primitive types

type i32 implements AddOp {
  fundamental i32; // special declaration that says this type 
  public fn add(rhs: i32): i32 {...}
}

like somehow the "add" implementation here should somehow declare that it emits the wasm add op, I love this
 - I want to see the whole implementation of i32!!!

type i32 implements AddOp {
  fundamental i32; // special declaration that says this type 
  public fn add(rhs: i32): i32 {
    builder.emit("add", this, rhs); // something like this?
    // but this is odd, it runs at runtime or compile time?
  }
}

type i32 implements AddOp {
  fundamental i32; // special declaration that says this type 
  public fn add(rhs: i32): i32 {
    wasm(i32.add self rhs) // or somehow declare wasm inline
    // this way doesn't really let us generate wasm, this wasm is fixed (but at least works with params)
  }
}


maybe we can introduce some fundamental emit step for functions that's compile time

type i32 implements AddOp {
  fundamental i32; // special declaration that says this type 
  public fn add(rhs: i32): i32 emit {
    // when we do a compiletime function, we're saying we're going to emit the wasm
    // we get some fundamental inputs here, maybe we can declare but it's really just provided by the compiler
    // it's like a bytecode builder for a function
    // here, parameters are provided as named constant aliases for wasm, so I can use rhs but it's not an i32, it's like a WasmRef<i32>
    // or something
    builder.emit(Wasm.Add, this, rhs); // something like that
    // this will be run at compile time and the result will be checked to make sure its an i32
  }
}

THIS LANGUAGE IS SIIIIICK

basically the compiler uses compiletime evaluation wherever it can, types, functions, etc
 - I like that we can fundamentally use this with closures too
 fn(a: i32): void emit {}

emit could be like a compile time keyword, and it acts like a closure with refs to values

// Here we know the result of emit must be an i32, and it will do this all at compile time
let a: i32 = emit {
};

inline wasm works the same way, pretty much just short hand for emit

I do wonder if we should be using something more llvm like, with structs

lets just use the binaryen API for now, that's what we'll expose

emit is an expression, emit can be used in place of a function implementation

that means we can write fundamentally unsafe functions
 - maybe there's a "safe" version of wasm that uses no dereferences, or we do all the dereferences

basically wasm emit is a fundamental part of the language

we make binaryen part of our language too

we want to modify binaryen to add our ref counted / allocated types
 - so we can make optimizations to reduce inc/dec

sort of want to add structs in too, or at least just make them really easy to work with

really need to find a collaborator on this language

fundamentally the language should not get in your way with syntax, implicit everything where it makes sense

.t files

main.t

we register tasks for visting ast nodes
 - fundamentally running code at compile time
 - but some tasks can emit, during a pass

Maybe the way we emit is by attaching sub-trees to our nodes
 - it's not all one code builder, we build our little pieces and they get fundementally linked at the end

#BinOp = Lhs(Expression) + Rhs(Expression) => Lhs.add(Rhs) // automatically reparsed as a new tree

I love this whole language, it's so flexible and extensible
 - and we can easily bring in Binaryen C Api into WASM (and compile it all as WASM)

or compile it all in rust too

need to take a look at Mojo to see what they are doing that's different

I love that we can just bind binaryen and wasm stuff, as well as tower nodes to the compiler interface

dynamic indexing should definately be a part of the language

I never like how constants were passed around as almost their own language
 - like the fact that a template type could be a constant

like a.test could be a.index<"test">()

really we're just trying to at compile time find a member by the name of "test"

does that mean that some operators evaluate at compile time?

like a "compiletime" version of a function can be called when all constants are known at compile time

index should effectively be a templated function

but it's odd, it's type completely changes based on the string (because the member type changes)
 - this is a great opportunity to write some compile time code
 - I think other languages like TypeScript handle this with special syntax where you can index a type ["string"]
 - I want to do away with that stuff, it should just be more code, less language

if we want to write index within our own language (member access) how does it look?


type Object implements {
  fn index(value: string) {
  }
}

// Here we know we're evaluating an expression
let a = emit: i32 {
  // builder should be able to eval code in the language, as well as emit wasm instructions
} // means the result should be an i32

let a = emit {
  compiler.set_result_type(typeid(i32));
}

ahhh, so compiletime values are basically directly usable inside of emit code
 - if it's not compile time, then it's just an SSA address in wasm

fn test(compiler value: string) emit: {
  // If you leave off the type of emit, we can attempt to infer it from your compiled code like an inferred return
}

all compile values are visible in emit, I love that

Keep our generics simple, but template meta programming is just coding, way easier to understand

Anywhere that accepts a type we could also use "emit: Type {}"


I also like the idea that we can just leave off as much or as little as we want for an emit function
 - like we should allow any kind of parameter arguments, allow the users to test what was passed in
 - if they specify types then they are checked by the type system, but otherwise you can check them manually in compiler code


type Object implements {
  fn index(...) emit: {
    builder.arg[0] // says whether it's a reference or compile time
  }

  fn index(a:, ...) emit: {
    // a can be either a compile time or runtime value (can be used as a wasm value in both cases)
    builder.arg[0] // says whether it's a reference or compile time
  }

  fn index(compiletime a:, ...) emit: {
    // a is only a compile time value
    builder.arg[0] // says whether it's a reference or compile time
  }

  fn index(a: i32, ...) emit: {
    // a must satisfy i32
    builder.arg[0] // says whether it's a reference or compile time
  }
}

can an interface say that a particular function must be evaluated at compile time?

emit: Type {
}


the only thing I'm not in love with now is how our language operates at any level of performance if it's all interfaces
 - like i32, I'm really just saying I take anything that looks like an i32, add is virtual, all that

but maybe we're also talking about having an aggressive optimizer that inlines
 - maybe at runtime / development time everything is virtual, but at production time we optimize all output including inlines

or maybe there's a way to say we take concrete things
 - heck, even at development time we could do something where we generate two versions of every function
 - one with all parameters as interfaces, and one with them as exact types (if only type names are used)

maybe even as the development version, we still code gen if all types match exactly

I could imagine a syntax where we're saying like foo(test: @i32), where test must match exactly the i32 type, not a lookalike
 - in this case since we know they type is exactly i32 and not just "anything that looks like an i32"

we're basically saying all things are templates at that point
once we got to a call where we did foo(123) we would know that 123 is exactly an @i32, so we could call the exact version
 - it's bascically like there are points where we know things concretely (the types)

or if we did it rust style, separate data from interface
 - then functions could explicitly declare that they just take a struct instead of an interface
 - i32 would be the struct data

right now if I said

type Player = {
  health: i32;
};


// then this would be legal
let p: Player = {
  health: ObjThatImplementsI32Interface {}
};

But technically doing Player { health: 123 } is @Player (exact type)

Doing things the rust struct way is nice because we can easily just take the data itself
 - it's not an interface, it's just actual concrete data
 - and interfaces can be declared to work on that data (operators, etc) statically

what about the idea that there aren't value types, just automatic promotions from to heap objects
example:

let a = {
  count: 100,
  lives: 9
};

let p = Vec::new();

p.push(a); // we either need to copy a, move a, or share a reference to a

technically with our language only pointing at heap objects with offsets
every object should be movable, copyable, and heap allocatable

how do we do weak references?
 - technically adds overhead, but maybe it's small enough to not care

So the idea is for any allocated object, when we request a weak pointer we allocate a single object that has it's own
reference count (any attempt to get a weak ref to the same object returns the same weak ref count object)
 - weak ref is a heap object itself
the weak ref is lazily created, but takes an extra pointer of space on each object, so now objects look like
 - typeid (maybe, might be on the pointer), ref count, and weak ref object pointer

weak references only work on non copy types

what if it worked on all types, what if even stack types could have ref counts
 - it would work by the stack creating a single shared handle object (one pointer for it)
 - null it out when it's completed
 - could do it only if you get a ref to it, e.g. the stack doesn't generate one if it's never taken by ref

What if objects still had ref counts on the stack, and when they got unrolled/destructed if anyone still had a ref
it would exception

always a copy or move depending on what we can elide

let p = &Player {
  
};



if it's on the stack, then passing it to any function will elide whatever is possible
Maybe when we declare a type, we can declare if it's primarily by ref or value, so & can be implicit, maybe & just flips it lol
 - I kinda love this, it's awesome
 - could be one of those pieces of info that's only attached to TypeNames, since it's just syntax sugar And
   doesn't actually affect the type comparability, and also TypeNames are already unique types

How can we tell if a stack object needs a ref count and all that?
 - I don't want every i32 and random struct to be ref counted
 - only if we ever use a type on the stack as a ref, maybe & operator or ref (or maybe use ref and val)

let a = 32;
return a; // never passed as a ref, no need to allocate overhead


let a = 32;
foo(&a); // a passed by reference here, could all be implicit and no need for &?
return a;


fn foo(a: &i32) {
  let b: i32 = a; // implicit convert / copy
}

all types copied or moved as possible


let a = SomeBigValueType{};
v.push(a); // if that's the last reference to a, it's moved


Oh actually & is really cool if it just flips, because it is actually part of the type, not just a convenience

type Player = &{
};

Player {} // this will allocate because Player produces a reference

So when we say & types vs copyable types, what's the diff
 - Maybe this is where we differentiate the whole "exact type" thing we talked about before
 - value / copy types we don't do interfaces for, or basically we only take the exact object
 - or maybe we differentiate whether it's an interface or a concrete type by a new keyword, "like"
 - basically just dyn in rust but cuter I guess

fn foo(bar: like Player) {
}

I like the idea of "like" meaning any kind of interface
Or exact meaning only the type itself, copy types are exact only (must be a ref to be a like)

But I also like that code is basically always templated, if it looks like a duck...
And we use monomorphism / inlining
like and exact can be keywords you rarely see (like is implied)

Love it, allocating a like type produces an exact type
Player{...} always produces an exact player
 - returning a copy and then allocating it can move it



How does that work efficiently in rust, if a class is ::new() and returned by copy, and then put into a Box, we have to move it right? Or does it optimize to the point of allocating directly in the type itself (probably HIR/MIR and all) move is still cheaper, but not as cheap as allocating/constructing all in place
For now we won't worry about it, if rust can we can

When we get a ref to a temporary, we should have the option of ref on the stack or allocating

Hmm, in Rust there is no Copy because copy makes objects weird, except copyable types
 - can copy constructors be implemented?

Can copy be deleted or removed...

Maybe once you mark something as ref you can't unmark it, except in the crate it exists in maybe

So it's assumed all non ref types are copyable
 - maybe if you do the opposite, it can only be moved

I do like that copy types are exact types

We can also just have non-copyable be different, maybe ref types imply non-copyable but you can have a value type that's move only?

Mutex has data that is normally only modifiable when you lock and unlock
 - only get a ref to it when it's locked, ref count must be zero on unlock

All statics and global should be read only / immutable or mutexed

Actually when you unlock a mutex, all objects in the graph (mutex as the root) must be internally contained
 - graph walk? Hmm let's look more into thread systems here

Immutable objects really just mean no interfaces modify our own / self object
What does it mean to modify, do we introduce the const concept?

I don't mind having to mark mutable parameters as you won't need to do it often if we assume everything is mutable by default, or just self keyword is mutable by default

How does rust handle mutable / const iterators?
 - always hated this in c++

I do like the c# doesn't really have this, you just have to make read only interfaces if that's what you want
 - I like that, just easier
 - never really missed it in TypeScript

simple is the goal, less syntax
 - would be nice to know if an object is entirely immutable tho, not sure how to signal that

Match statements vs rust, it's interesting that their match is basically enum destructuring, and fundamentally no other way to do so
- basically have to store type ids in a pseudo union
- since it's typeid we should be able to use instanceof, but also match

Can we do value | ref type?
Well we should be able to | any 
Yeah, but non copyable infects so to speak

What about typescript isms like their control flow weirdness

Like instanceof fundamentally changes compile type

let a : string | Player;

if (a instanceof Player) {
// a is now player, a.attack() is legal
a = "foo"; // exception, is is locked as a Player (is this even legal too if a is now typed as Player...
}

I see, rust fixes this by fundamentally not allowing you to mutate while having a read only ref to 'a'

In C# instanceof is only pulling interfaces off objects which cannot be changed at runtime

But here | types are a bit different because they can be one or the other

My only thought is that maybe | types can't be assigned over (immutable once created)
Hmm kinda hate that but it could work

Or alternatively it's a little more runtime, type unions have an "instanceof" / match lock
- basically just a ref count of who has an instance of a particular type

When I say A | B (string | i32) just remember we need to specify interfaces or not (like string | like i32). What does like (string | i32) mean? Anything that looks like the common interface between string and i32

Can I promote a member or function parameter I don't own to be a like type? Or maybe an event sender for property changes?

Also for like types, when I declare a member am
I making a concrete type or like type. Might just have to do like as a keyword, and it's part of the type (anything like this).

type Foo = enum {
  A,
  B,
}

Is just syntax sugar for:

type Foo = u32 {
  static const compiled A = 0 as Foo,
  static const compiled B = 1 as Foo,
}

When we put statics on an interface is that part of the interface? Like does the type need to have the same statics to be considered an interface match? It sort of makes sense that a type could reimplement its own statics, but when you pull the interface with instanceof, how do you access overwritten statics, see that's kinda weird. 

Maybe we should treat statics like their own interface that the type implements? Using a Type name is basically like using a singleton instance

So we can have a static or shared keyword or whatever, but interfaces actually can define two interface, the static and the instance. Don't have to implement the static, but you can static({}). By default using any interface leaves off static members (not part of equality comparison)

I kinda also like the idea that static is a type expression keyword, so you can do {...} & static {...}
Using static on a member or fn is just syntax sugar for the above. static(...) extracts all statics

Hmm why do we like doing Player.staticmember When Player is a type. Could put those constants anywhere, but since it's associated with a Player it makes most sense to be on that type.

type Player = {
  instance_members: ...;
};

let Player = {
  compiletime readonly max_lives = 9;
}

Player has no actual size or any members
And you can now do Player.

Maybe we can do a shorthand

type X = Y static Z;

So static isn't part of type expressions, just shorthand for type names. Sort of makes sense, where is its storage? But can we just magically 'let' in the middle of a module to create the static type?

I like this too because now Player is both a type name and a value

Should start organizing this all into a spec, maybe hand it to gpt 4 lol!

Why a new language
 - memory safe, but also 90% "near" native performance without much thought
 - no garbage collection, deterministic performance
 - duck typing and interfaces / rust like traits
 - easy to build DSLs and custom syntax
 - wasm first approach
 - checked/ enforced semver with public interfaces
 - shading language? Need to really understand cuda / compute langs / mojo
 - typescript style interfaces and type expressions (A|B)
 - unique compile time features (visibility, compile, emit, etc)
 - crazy good inference
 - reflection

I also like the studio / easy gui interface solution
 - basically like you can make small prototype apps with property grid and some simple automatic Ui components - see anything in the app

Events and notifications for property changes
 - this is where we can now write super sick type wrappers that just replace all setters with a wrapper that notifies about property changes

So I guess we should start by making our own IR
And then lower that into wasm instead, can be very wasm based

So now what do we want tower nodes to be, if we have this new interface concept working
 - must support dynamic composition
 - or being forced to implement interfaces at least
 - dynamic seems better, it's not one giant bloated type
 - and we want to attach things like types
 - we can almost do bloated static style composition, just add T | null pointers to the tower node type for each T component type
 - I mean, it works surprisingly well actually and fast
 - We probably won't have that many and who cares about bloat
 - but if we wanted we could implement an operator to look up components by typeid at runtime (exact? Can it be efficient with all "like" interface queries?)
 - typeid.instance_of(typeid)?
 - can also maybe implement instanceof operator
I also love the component operators we had in Zilch, can do a lot of this with type expressions and implements + custom syntax (language extensions)
 - but that could also become standard
 - same with Zilch style event systems, all these should be types that are automatically generated and use implements cleverly to add members to other types

Has takes a compiletime typeid, any compile time typeid can be turned back into a type (for the return type too)

fn has(compiletime t: type_id): t {
  return this.has(runtime t) as t;
}

Or maybe shorthand

fn has(t: type): t {
  return this.has(runtime t) as t;
}

I almost wonder if typeid can be a typed expression to limit typeids to those that implement a specific interface, same with type()

Basically trying to get away from either complex templates or introducing template syntax

I Really love the idea that a templated type is somehow just a normal function call:

I want our templates to be:
let a : container(i32);

Because array is literally a function that takes a compiletime typeid (or type for short) and returns a compiletime typeid 
Like we said above that 'type Player' ... also has 'let Player' for statics.
So the idea here is while an array 

// leave off the return type so it can be inferred, but inferrence depends on the passed in type t, which is required to
// be a compiletime value, so it will all depend upon the instance when it is called and the t passed in
// since we return a typeid, and typeid is always a compiletime value, then the return type will be compiletime
// Because we're returning a compiletime typeid, this may be used as a type anywhere in the language
fn container(t: compiletime type) {
  parse("struct { val: T }").replace("T", t);

  return typeid({
    val: t
  });
}

// So now I can do
let a : container(i32) = {
  val: 123
};

// or shorter
let a = container(i32) {
  val: 123
};

// Can also do

type container(i32) implements {
}

// how do we do this...
type container(T) implements {

}

in my head this is almost a case for code gen
normally in languages like Rust we can say we're only implementing this for T where some-condition
and the compiler handles the details of who all that actually gets implemented for based on who has what and satisfies what interface
 - can we use like types here...

// does that mean anyone satisfies it, because it has no members (basically 'any' can be a keyword for `like {}`)
type container(like {}) implements {

}

type container(like Player) implements {

}


Lets try this experiment, evaluate container(like Player):

fn container(compiletime t: typeid(like Player)) {
  return typeid({
    val: t
  });
}

so this returns a type like this:
{
  val: like Player
}

Technically an 'exact' that accepts anything with a member val who looks like a Player

I guess my question becomes one about container(like Player) vs container(Player)
 - if I impl for container(like Player), does that impl also for container(Player)?
 - I assume so because I'm basically saying anyone who looks like this
 - or maybe I need to use like here... hmm!!!

type i32 implements {
  // whatever
}

// can we do this? anyone who looks like this implements this
type like i32 implements {
}

maybe thats the difference between how we implement interfaces

Technically here then, container(like Player) is an exact type, so we're saying no implement it for that type


Ok I'm still thinking that type names introduce exact unique types, they're not just aliases, and we can also attach interfaces to them specifically
 - or maybe when it's an exact you have to make sure to "capture it" in a type alias so that everyone refers to the same one... yikes
 - I like the idea that you're saying you're implementing for exactly this type (allocated) not just a lookalike, but that can be anyone who looks "exactly"
 - So type names introduce a uniqueness concept


Note that means that container(like Player) used in two different places technically runs and creates two different types, but they are equiavalent and therefore shared
 - typeid must share them automagically

type container(like Player) implements ... // means anyone who looks exactly like container(like Player) gets this

type any = like {}


We could introduce Template syntax but just for easy container cases

We kinda want to say that we only accept types that implement a specific component interface like CogComponent. C# generics uses concepts or traits or whatever it's called, rust has similar "where" clauses. 

type Cog implements {
  readonly Player { // getter shorthand with self
    return self.has(typeid(Player));
  }
}

Cog implements an instanceof operator or just its own .has, really whatever. I kinda like in a way this feature isn't a "built in operator". We can just make it our own operators.


Q: is it type names that implement interfaces, or exact types? Now that we have the exact type concept I wonder if type names should go back to being aliases, and exact types are unique.

alias A = {
  a: i32;
};

type A = like { // is this ok, should type names always be exact types? I guess it sort of works still
  a: i32;
};

type A = {
  a: i32;
};



Investigate MLIR, and maybe talk with River: https://llvm.org/devmtg/2020-09/slides/MLIR_Tutorial.pdf


Do we want slices?
Start with npm as our package manager
Tests built in like Rust
Destructors / Drop trait

Rust traits can have sub types:
  impl Add for Point {
      type Output = Point;
      ...
  }

In a really weird way, we can just output a compiletime typeid
 - I guess in reality all of our type expressions are really just syntax sugar for compiletime typeids

type Player implements Add {
  static Output: compiletime TypeId = typeid(Point);
}

Really need to organize this all into a spec to keep track of features

Topics to cover:

https://doc.rust-lang.org/reference/introduction.html
https://doc.rust-lang.org/stable/book/

Intro:
  Hello, World!
  Show off templates
  Show off bnf rules / foreach
  Implement a simple program

The base:
  BNF Grammar Rules
  AST
  Tasks
  Compile Time

The language:
 - Lexical
   - Keywords
   - Operators and Symbols
   - Upper camel names vs lower underscore names (enforced)
 - Compile Time vs Run Time
 - Variables
 - Expressions
   - Literals & Constants
   - Operators
 - Statements
 - Functions
   - Methods syntax sugar
 - Closures
 - Comments
 - Control Flow
 - Modules
   - Scope paths
 - Scope
   - Using statements
 - Name Lookup
 - Declarations
 - Types & Interfaces
   - ref, exact, like
   - Instantiating a type
   - implements (also discuss like types)
 - Type Expressions
   - Or types and memory representation
   - instanceof / is
 - Match statement / destructuring
 - Exception Handling
 - Memory (Representation + Ref counting)
 - Strings
 - Collections
   - Arrays
   - do we have tuples? or can our arrays support this like TS [string, i32]
 - Templates (or more importantly, how we don't have templates)
 - Standard library
   - WASI
   - IO (files, printing, etc)
   - Threading
   - Containers / Collections
   - Iterators / ranges
 - Inline WASM / IR


What if we also turned tokens into tower nodes
 - so the token stream is basically a big stream of tower nodes just in an array
 - they already have all the start/end info and all that
 - When we parse them, we can attach them to our parse tree, or turn them into real tree nodes
 - This also kinda makes sense for how complex our tokenizer can be
 - We can even parent things to the token, it will still sit in the stream (forest)

Little bit of brainstorm here on tower nodes, tokens, etc, we want replacements,
all that, but ultimately it should really just boil down to syntax sugar for running custom wasm / tower compiler API calls

how do we want the replacement and tree transform API stuff to work?
 - it shouldn't be too complex, just enough for simple stuff otherwise use WASM / user code

// Two separate references to expression
parse Expression = '@' Expression "," Expression => ...;

// N number of references to expression
parse Expression = '@' Expression ("," Expression)* => ...;

We basically just want regex features for capture

$0 means the whole thing, $1 means the first capture group, etc
 - parentheses are captures in regex, I always hated that
 - we should do named captures only

I never liked how regex replace worked when a group got matched multiple times, I think it just takes the first

As we parse sub-rules, the handlers for those sub-rules will run (either replacements, or they generate their own parse tree/AST)

The tokens become the parse tree becomes the AST

I think it's possible to compile time analyze a rule to see how many matches we expect
 - if it's a single, then it refers to the single node
 - if it's multiple, then it refers to a group (all placed under a single tower group node)
 - do we need indexing here? kinda don't think we want to go that far

named captures will capture non-terminals
 - Do rules always capture non-terminals?
 - If a rule has no logic, it always just outputs it's parse tree
 - Otherwise when we refer to a name like Expression, it just outputs whatever AST nodes that the handler for Expression output

I like everything I'm seeing here!

Ok, tokenization vs parsing, there are some things we can't really support in parsing like character ranges
 - string literals should actually create tokens in parse mode

This is a great example of why we would want the tokenizer to create AST nodes
 - the rule range should show up as a single token when we parse, but all the information about the parse should be contained in parse nodes
 - I can see where we don't want parse nodes just for invoking rules
 - maybe we can make like an inline operator or something

// Character Sets: [abc], [a-z], [^abc], [abc-], etc
token RuleRange = "[" "^"? (RuleRangeChar ("-" RuleRangeChar)?)+ "-"? "]";

// Match any character except ] or \, but allow escaping \] or \\
token RuleRangeChar = [^\]\\] | "\]" | "\\";

Part of our language should maybe automatically remove useless tokens from the input, if there's no variation on them?
 - e.g. they always have that token ( at the beginning...

dumb side note, what would a language look like where all objects are components / compositions
 - any object can have any number of components, children, 

Right now we separate tokens from parse rules
 - part of the reason is so we can define white space, and separate out words and symbols
 - the question is, should parse rules be able to use characters?
   - Or can parse rules only refer to token rules
 - I think trying to combine them sounds neat, but it's sort of confusing
 - When it comes to whitespace, I like that we can define the ignore rule
 - We can still have that though, since it's entirely a character rule...


So the idea is basically if there are spaces between token characters, then spaces are allowed
 - do we also allow spaces between any sub-rule? that seems annoying because now tokens can't define sub-rules
 - in pascal, white space does literally nothing (like it's removed)
 - but in C, whitespace acts as a token separator
 - int foo bar; is not the same as int foobar;

I could introduce something gross like token groups or something, but I think just having two passes is better

 - But the passes should be driven by the parser, the parser should reach the end of the token stream and request another token
   (which invokes the tokenizer, skips any ignored rules / whitespace until it gets a token, or EOF)

Also make sure the parser is iterative

Foo = 'for' '$'

always results in only "for$" being a valid token, not "for $"

The main issue is just that we don't know when we're defining a token that should have no whitespace
 - 'for' could be a single token, or 'for$' could be a single token

So basically we could do some sort of grouping operator that means "this is a whole token"
 - but I kinda hate that, we're adding new operators just to remove the token phase
 - it's also really weird, we need to identify which rules are contained in {}
   - the same rule could be used for parser and tokenizer, that's weird


Foo = {'for' '$'}


Ok, just separate them, it's better
 - but usually for a tokenizer, there isn't a single root token rule
 - like in GOLD parser builder, all tokens are just separate rules
 - So what's the root rule look like for tokens, maybe we just have a special one called root
 - but like... we have to add every token we want to add to it? That kinda sucks for tokenizing
 - and as far as the AST goes, we could always remove the root if there's only one node (that node can have N children tho)
   - I like that, it's kinda like captures
 - For parsing it makes a lot of sense since the parser must parse the whole file
 - But the tokenizer is basically just a splitter, so it makes no sense 
 - Keywords are odd, since they are technically identifiers
   - So how do we know when something is a keyword or an identifier?
   - exact match afterwards I suppose

We should probably add ways of doing tests in the language, specifically tests for the parser / tokenizer
 - Basically "we expect an error" or this should compile
 - maybe even can specify the kind of error expected

Do we have to add token rules to the root?
 - I'm not in love with the idea, but yeah we could
 - We could do something like any orphan token rule is auto applied to the root...
   - but if it's not orphaned, e.g. it's referenced somewhere, then it's not added to the root
 - makes writing tokens easier
 - I think if our langauge is going to use +=, we should too for the parser
 - Actually no, |= makes a lot more sense

But I could write:

token Foo = "foo";
token Root |= Foo; // This line is optional, orphaned tokens are automatically appended to the root

For named captures, how do we discard tokens we don't care about?
 - if it's not in the named capture, obviously you would have to pull it out yourself
 - could make up some sort of 'discard' which just means get rid of these, e.g. discard()
 - but now we're like into function calls in our BNF... I guess BNF just uses operators otherwise...

token Whitespace = [ \t\r\n\f]+ => discard;
 - This means when the token finishes parsing, the action is to discard it

token Whitespace = discard([ \t\r\n\f]+);

 - technically no action, but we're saying please discard all the nodes
 - in that regard, we need to think about how the token phase does parse trees

I think the basic idea is to do some minor cleanup of the tree nodes before they get to code
 - look at some example, binary operators, etc

Grammar rule examples that we should look at:
-----------------------------------------
```ts
// An identifier is any letter or underscore, followed by any number of letters, numbers, or underscores
token Identifier = [a-zA-Z_][a-zA-Z0-9_]*;

// A string literal is quoted, and allows escapes
token String = '"' ([^"\\] | "\\\"" | "\\\\") '"';

// Define a new rule that is either a transform or has a code handler
parse Rule = ("token" | "parse") Identifier ("=" | "|=") RuleAlternation ("=>" RuleHandler)? ";"

// Alternation: A | B | C parses any one and only one of A, B, or C
parse RuleAlternation = RuleConcatenation ("|" RuleConcatenation)*;

// Concatenation: A B parses A first and then B
parse RuleConcatenation = RuleUnaryOperators RuleUnaryOperators*;

// Unary Operators: A*, A+, A?
parse RuleUnaryOperators = RuleValue [*+?]*;

// Character Sets: [abc], [a-z], [^abc], [abc-], etc
token RuleRange = "[" "^"? (RuleRangeChar ("-" RuleRangeChar)?)+ "-"? "]";

// Match any character except ] or \, but allow escaping \] or \\
token RuleRangeChar = [^\]\\] | "\]" | "\\";

// A capture group captures all the AST nodes in a parse
parse RuleCapture = "$" Identifier "(" RuleAlternation ")"

// Values and grouped expressions
parse RuleValue = Identifier | String | RuleRange | RuleCapture | "(" RuleAlternation ")";

// Handle the rule in one of the following ways
parse RuleHandler =
  "replace" "(" UserCode ")"  | // Replace parsed rule with with parsed user code
  "wasm" "(" Wasm ")"         | // Run WASM code that has access to the Tower compiler API
  "run" "(" UserCode ")"      | // Run user code that has access to the Tower compiler API
  "discard"                   ; // Skip/ignore the parsed text, used to skip whitespace
```


RuleRange is problematic since '-' is a valid character

S = A+;
A = C;
A = C '-' C; // How do I say that this one has priority?
C = 'a' | '-';
https://stackoverflow.com/questions/21858092/conflict-resolution-in-lalr1-parser

Ok pretending we have that all fixed and there's some way to specify conflict resolution...

```ts
// Character Sets: [abc], [a-z], [^abc], [abc-], etc
token RuleCharacterClass = "[" $Not("^"?) (RuleCharacterClassRange | RuleCharacterClassChar)+ "]";

// Character ranges, such as "a-z"
token RuleCharacterClassRange = RuleCharacterClassChar "-" RuleCharacterClassChar;

// Match any character except ] or \, but allow escaping \] or \\ so we can write [a-z\]]
token RuleCharacterClassChar = [^\]\\] | "\]" | "\\";
```

What should the parse tree look like?
 - By default, we create child nodes for all terminals and non-terminals
 - none of them are named, just in order children
 - named capture groups and non-terminals can be added as named members

What's the ideal that I want here?

Well, in general I want to be able tp refer to a type of token by name, like "RuleCharacterClass"

so if I'm looking at a token stream with all it's input discarded

"foo" [a-zA-Z_] "bar"

would be:

String       RuleCharacterClass       String
            /    /        |    \
         Not RCCRange RCCRange  RCCChar
            /    \      /    \     _
      RCCChar RCCChar RCCChar RCCChar
         a       z       A       Z

And each of these nodes hold the start/end

[a-zA-Z_] makes sense as a single token because whitespace inside would change the meaning (like a string)

Should "Not" exist if it didn't capture anything?
 - tower nodes need to say which rule they were parsed by

Also, since Not is named, should it be a named member?

Should non-terminals be named members?

I guess it's OK, they're not a map, it's all in an array anyways
 - So maybe we don't name the node, we just name the members?
 - naming members is kind of a way of collapsing nodes, since the "Not" could just be a name and not a whole node

I guess since the node has a parent pointer, we can always do tower_node_get_parent_member_name

Hmm, ok so how do we know what a token is, if it doesn't have a completed rule name by itself?
 - I think we should only use member names for named captures (and maybe non-terminals)

But nodes need to know what rule captured them

Ok, lets just pretend for now that nodes have a rule name or rule pointer on them
 - as well as the start/end of where they parsed (or start/length)

So will all children end up being named?
 - Is there any case where a child ends up not named?
 - It really depends if we make non-terminals named members
 - Almost thinking not, since those nodes have their own name/rule which is the non-terminal
 - But it would be kinda nice if iteration worked like that

We can, I mean if we had the rule pointer, we would just take the rule's name as the member name (ref counted)

So now AFAIK, all nodes have an array of children in the order of parse (or whatever code re-arranged them)
 - and all children have a member name, as well as an accepted rule themselves


---------------------------------------------------------------
My idea:How to write a safe type container
Oh, and parameter types should be able to be listed in any order (called by name, or by position). Optionals must go to the end however for positional calling, but types can be inferred in any order (param 2 dependent upon 5, 6 dependent upon 1, etc)
This allows us to write templates like this:
fn foo(a: t, t: compiletime typeid) {}

Also another idea, what if objects or structure in our language could be like JSON kinda, but for component based design. The principles of one type per container (type map, type multi map)
There is a reason our serialization language was a little different. So then my next idea or questions is can an interface in Tower be used to specify the structure of our component trees? Much like JSON and JSONSchemas / avj
I wish TypeScript could validate, ya know?
So whats the idea now, components must be exact types (right?), type lookup must be O(1) for mapped types and mapped interfaces (interfaces must map explicitly, may only have one). Component addition can fail
Dependencies, how as a language do we also guarantee things, like always having a parent pointer
A type interface could require a specific component tree, or maybe it safisfies interfaces
Maybe they can also declare what kind of types can own them (do we merge owner statements in type expressions?)
Maybe we just know that type operations are O(1), and others are expensive (you can if any type implements an exact interface, like interface, etc)
Component based design language- has operator- parent type declaration- child type declaration- children can be named (json members)
Were basically saying what if everything in our language was a tower node? How can we do that and not be horribly non-native?
For starters, features on an object header should be based on what traits they use- we only get memory for children when we use the children statement
We can always walk children and components, and possibly index them?
Do we really even need like interfaces anymore?- its true duck typing, but in a way component based design with interfaces solves pretty much all that can still add it in the future, or maybe theres a really good reason it makes sense to keep in
We should be able to check if a sub tree matches an interface, thats what cast should do
So now maybe has is a statement on interfaces too.- has exact types should be fast, has like types should be slow walk (first in order to satisfy the condition)
Component interface paths?- we have indices for children- names for named members- has for components- dependency
Dependencies are always on siblings, and are
(That could be how arrays are implemented, just object with children templated on T)
So this is built in- or, built up from the ground level of tower
If all members were lower underscore, then we know when we do dot .UpperName it must be a component, so implicitly thats just a has operator
Must return Component | null though
If you do dependencies, then it doesnt need to be null (guaranteed and ref counted)
I like it better that you can just . off a potential nullable and get an exception, rather than being forced to handle it. Hmmmm
Construction and interface syntax should be similar.
{ has Player; // in the interface has Player {}; // in construction
 owner foo}

Maybe component types are only constructable as children of a parent (e.g it cant ever live alone)- you could potentially move them, but must always be to another living owner
When constructed under

------------------------------
I got some crazy fun ideas about how the compiler should work

if it's all component based trees

when you declare you can have components of a type (or maybe it's implicitly from owners being declared)
 - it also means you need to be able to enforce dependencies, replacements, etc
 - can dependencies just be pointers, I would love this
 - Order of releasing references, etc

So one question, does casting into an interface mean that the interface can hold some components?

like my weird situation:

let a = {
  test: 123,
  owner: ...
};


In TypeScript, you can cast away, the underlying interpretation may change

let a : {
  foo: string | number
} = {
  foo: "hello"
};

let b = a as { foo: number };

That's all legal, and unchecked

casting in most languages is either just on the type, or on a runtime where it's exact and simple
 - and more importantly can't change configuration

Here we would say something similar

has A & B, has A | B, what does that mean?
 - I do like this concept, we can have both, or one or the other
 - When we register a component with a type expression, we're saying it's these things
 - For A & B, we would walk the type expression and register the component as both
 - For A | B, we would error? You don't register A | B...
   - I guess it would just resolve what it actually is and register it as that
 - 

I do wonder what and types should do, do they just create a new exact type, or are operations on type expressions run over the expresions themselves?

When we make interfaces, we say has Type
 - We can also say has BaseType or another explictly implemented interface Type
 - has can come in any order, it's not specifying an order of any kind
 - it's simply just saying it has this
 - 

maybe we can only use named types?

I already had something similar with finding interfaces, where A | B instanceof would do something different

We need a way of pinning data
 - Or maybe when we cast, for things that can vary, we capture references?
 - 


has A;
has B;

would technically be different than 
has A & B;

that implies a single item satisfies both A & B.

has A & B;

however would satisfy

has A;
has B;

but

has A;
has B;

may not satisfy 

has A & B;



previously it was all interfaces on struct values, like typed enums basically

dependency also means we cannot be constructed unless the dependency is satisfied
 - is that just an exception to add then?

constructing a component with an owner means it's automatically added to the owner
 - Owners are implicit when constructing under a parent type

BoxCollider {
  owner cog,
}

owner also establishes dependencies
 - I guess it's just kinda weird, do I establish dependencies?

I'm not totally in love with component based design as part of the language
 - but not against it either, still thinking about it

it makes sense for my language to be based heavily around the tower parse tree as the general object form

also recognizing patterns and subtrees seems really important
 - https://stackoverflow.com/questions/14425568/interface-type-check-with-typescript

Ok, so either it needs to lock it, all members need to be functions that retrieve and throw if they fail, hmm

The allow modify and throw if they fail is the nicest towards writing any code
 - But it technically could be slow if you grab the same thing over and over, but who cares I guess

into {
  has RigidBody;
}

let a = b as {
  has RigidBody;
};

Or when we get an interface, it could be all functions, but maybe we capture everything as it's current type
 - kinda weird that the cast adds a bunch of ref counts, but I guess it makes sense
 - Then it doesn't matter if we modify, it's captured
 - So basically an interface captures, like a closure...
   - Hmm I don't like this, that would end up being confusing
   - If you replace a component suddenly the interface is storing it's own stuff and doesn't reflect the change??
 - Ok so interfaces are purely functional when casted to
 - That means they basically only store the root object that they were grabbed from
   - Then all other functions of the interface must traverse the hierarchy
   - kinda goes back to paths again...
   - yeah I like it, then they fail if it no longer exists
   - interfaces throw if they don't hold anymore, but at the time of retrieval they will match
 - and obviously, if it's just a type itself and you're casting it with as, that is also fine

We'll need to make sure that every form of type we have can be represented by an interface
 - TypeScript for example can represent tuples [string, number]
 - This is because JavaScript's arrays don't care which type they hold

I guess it's odd, does tower need arrays?
 - I can declare that I have children of a type
 - that can be implemented as arrays
 - but maybe I want something closer to real arrays?
   - I kind of never care about fixed arrays... do I?

How high level is tower?!? lol

maybe lets just say that components are an addition to tower
 - The final language of tower can support components and events
 - But ultimately it's sort of an addition to just basic interfaces version of tower
 - thinking of it as an addition, then no, it's not a replacement for arrays

I think the main question floating in my head was, can components be a replacement for arrays?
 - Obviously with TypeScript, we can kind of path down to any member or type
 - Since arrays support any types, it could be [BoxCollider, RigidBody, Model], etc.
 - As long as we strongly type the object (take a snapshot of all it's exact runtime types)
 - Pathing down is important for interfaces (structural types)

is everything a tower node? no, tower node is just easily built with our component constructs
 - tower nodes can have all sorts of components, and anyone can ask for them
 - I really love it, we can also get specific with the requirements

why are children a special construct? Why not just an array called children?
 - I guess it's because we want owner to also be special and the two are linked
 - and we can optimize our implementation of children/owners
 - we would need some way of marking that array as the children array
 - maybe children is always an array?

See this is what I'm trying to consolodate, is array it's own syntax?
 - Or is children the replacement for array?
 - Or does the children syntax require an array?


maybe owner and children are just reserved keywords, but used like members with :

is it children? or components?

{
  owner: Cog;
  components: Component[];
}


What if components isn't special, it just is any array with children
 - if the children have an owner, it's set to the parent automatically, but types must match
 - And there's an order to things


// ref wraps the entire type, note that ref ref is idempotent (ref ref = ref)
type Cog = ref EventHandler {
  owner: Cog | null;
  contains: Component;

  name: string;
  children: Cog[];
}

type Component = ref EventHandler {
  owner: Cog;
  dependency: RigidBody;
}

let c = Cog {
  name: "Player",
  owner: null,
  children: [Cog {
    name: "Gun",
    // don't need to set owner
    children: []
  }]
}

// All named types that implement the same interface Component appear on Cog
// any named types are enumerated and shown here

c.RigidBody.owner


EventHandler is a construct that uses owner, and maybe children too


Something we should think about, how can this whole component based design thing be just an include
 - the concept of type containers / has
 - how it's stored
 - keywords, has, etc
 - lifetime logic

everything should be able to be configured or included, like we extended the language big time

including also structure interfaces, like when we do a cast
 - normally for duck interfaces, they just look to make sure the type has all the correct functions/members
 - but now we're also asking to make sure it has the right components too
   - these components are runtime, not fixed at compile time
 - in some cases, like owner, we could probably just do it fundamentally as a getter property
   - that will match with no issues
 - but has is a different story, since owner is fixed at compile time
 - so has is kind of like saying hey your interface has a child type map and one of those types is X
 - Maybe we can do this all with macro style expansions in interfaces
   - e.g. has RigidBody, expands to something like components(): type_map(with: RigidBody)

 - and maybe typemap has a special behavior where we can cast a type map into one that contains components

let t: type_map(Component);
let z: type_map(Component, with: RigidBody) | null = t.with(RigidBody);
let b: type_map(with: RigidBody) | null = t;

b.  //nothing shows up except RigidBody

if (z) {
  // It's a type map that's guaranteed to have a rigid body... is there a better way of doing this?
  // maybe type maps implicitly satisfy any interface cast
}

Ok, well I like all these ideas anyways, I'm sure the full thing will come out in the future

-------------------------------------------------

Ok, next topic, what's the MINIMAL set of tower that we need to start building tower?
 - Technically I keep going back to this thought that the tower compiler can all be WASM
 - so ultimately for binding to other languages, they really just need a WASM interpreter

However, there is a still a minimal set of tower that we need to start building tower ourselves in tower
 - it's the tower bootstrap
 - grammar rules BNF, pointers, ast, wasm execution

basically it's if I was to run the tower compiler without including any of the "prelude", no-std if you will
 - what does that version of tower look like
 - before interfaces, before ref counting, before anything

it's very C like, even the concept of interfaces should be something we extend

well, what about just the token/parse rules + wasm + tower nodes?
 - I guess in order to properly support tower nodes, we need component based design
 - that's the ideal correct, to have everything work from component based design?

the basics of tower could start possibly as an interpreter
basically just directly translating actions to immediate wasm execution
kind of like syntax directed translation
there's no parse tree, just the parser directly runs code on reduce

so the beginning of tower is just a direct interpreter
 - it lets us start building tower nodes from within tower, instead of outside
 - and the concept of interfaces, all that
 - what can we do with just wasm, no syntax nodes

Normally when we do => we get parse nodes, tower nodes, but we don't have them yet?
 - we could maybe have some simplified wasm form that only passes like start/end pointers
 - easy enough to write in wasm, but tokenizing gets kinda weird
 - and I don't like the idea of rewriting rules constantly (like starting with a completely different language)
 - I like the idea that each piece is a subset of the next pieces
 - obviously we get rid of things like pointers for safety, but they're not gone, and they're fundamentally still there


we want to define tower nodes within our language, how the heck do we do this
 - can have some sort of compiler callbacks that tell the language to do everything (defined in wasm)
 - so maybe phase 1 tower nodes are very simple (tokens and parse tree, no interfaces...)
 - tower nodes evolve in type??

 - and then we ensure the language dumps everything, and implement phase 2 nodes

 - the main ideal here is to have as much as we can written within the language, and minimise the amount
   of hand rolled wasm needed
   - basically I don't want any algorithms written in wasm, only like primitives that it makes 100% sense and
     we'll always need it around, like how pointers work, or maybe structs, etc

 - or maybe we can make tower nodes and reserve exactly what's needed in terms of space
 - or maybe we can run an upgrader on them to upgrade them all to the latest definition... hmmm

the entire point of this is so that we can write the fundamental architecture of tower within tower
 - we really just need enough of the basics to code interfaces and components

I don't want to have to rewrite all the rules again as an AST form after the SDT form...
 - Ideally want to re-use the rules, but do something else with them
 - Maybe I need a way for rules to have a name, so we can reference them and redefine an action
 - I can do that, just give them all unique names

I'll figure out the best way to do all this as I go, I'm sure I can find a way to reuse it

With wasm linking, I can sort of rely on a top to bottom execution
 - Ok I like this, the compiler asks the wasm to define what a tower node looks like
 - the wasm callback passes in the start/ends, raw info we get from the parser

That means the parser is implemented externally

This kind of makes sense, there are two ways to bootstrap:
 - wasm interpreter + implement the exact parser algorithm / compiler callbacks into wasm
 - we also write the parser algorithm in wasm, first using any language, but then eventually using tower
 - so you also only need a wasm interpreter to run everything

But maybe we write all of tower in wasm and tower itself
 - Hmm, basically saying to write the parser in wasm first
 - so it's like, everything is the same, there is no just "run the wasm"

And if that's the case, the wasm that it starts with defines

so that means we don't ever end up rewriting the parse algorithms
 - I bet we can make them really small and easy to define in wasm

once the parser is running

we also need to get really clever about how it allocates, all that
 - I like that tower nodes are callbacks, because in the beginning there could be no allocator

maybe write an allocator in wasm

wasm is a subset of tower

it should basically be normal wasm + WASI, but with some extra compile functions that expose the wasm compiler / execution
 - expose self compiling wasm to wasm
 - should be able to replace existing definitions with new ones

wasm allocator
 - simplest possible form

the start will look like wasm + wasi file reads

parser implementation in wasm too, creates tower nodes
 - rules finish parsing, calls attached wasm to rule
 - passes in tower nodes
 - as rules are parsed, we add wasm
   - wasm can lookup existing functions, etc
   - can we support forward declarations in wasm?
     - we should be able to add declarations apart from definitions
 - C like language to start, everything must be seen (forward declarations)


Function = 'fn' id '(' paramlist ')' statements => wasm(
)

fn foo(a: i32, b: i32) {
}

is this approach bad?
 - don't we ultimately want to emit binaryen or something more sensical?
 - shouldn't we be writing this in C/C++, and maybe just bringing in WASM
 - compile it to wasm so we can just import it as part of our stack
 - replace it in the future with our own implementation maybe


if we do direct wasm, somehow we have to be able to return sub-expressions when code generating

so when we ask it to compile wasm, we need to get expressions

ultimately we do want this mostly all written in our language
 - I don't want to rely on binaryen to extend anything in our language

that means we'll basically be running debug wasm, but maybe the implementors can
pass to binaryen and do optimizing - like it's part of the wasm host
 - we can even pass in a flag that says if we want to optimize it or not

Maybe we don't even need to return a pointer, maybe we just build a string as we go
 - can we just concatenate wasm together?

I suppose we're actually saying that we also need a full wasm parser implemenation too
 - because we're not outputting direct wasm codes...
 - I suppose we could?

 - Would be kinda neat to write wasm binary output in wasm itself
   - not as easy as text, obviously
   - but certainly more direct
 - maybe it's not that hard...

so we output assembly that poops out assembly
 - I like this

if the beginning of our compiler is wasm text, doesn't that imply our interpreter should deal with text wasm?
 - but we can compile the wasm text to .wasm with wabt or binaryen...
 - at that point, what's the difference between using C or something else, we still have to interpret text
   - unless we're hand writing the binary...

browsers cant deal with text wasm, I could compile it

so even for browsers, I'd have to compile wat2wasm or binaryen...

then kinda who cares... I already will eventually rewrite this in tower anyways

ARRRRGHHH I DONT KNOW

maybe we only compile the foundation with wat2wasm or binaryen
 - we do it offline to get a wasm file
 - from there, it's all running in wasm itself
 - need some basic utilities to build wasm byte code, can be written in wasm

the browser

I don't mind running a tool offline to produce wasm
 - eventually this will be our code anyways, so lets not try to make it in wasm then

Or, build a wat2wasm parser myself using our parser logic
 - we could write the text parser in our own language
 - it kinda makes no sense, since we need our language first

alternatively, instead of wasm, we could make our own
 - either represented by the tree, or 

If we make our own, we'd want to lower it to WASM at some point anyways

I think the primary pain point is that text form isn't a good AST/IR
 - Returning values in expressions isn't straightforward

But maybe we can do it anyways...

should it be part of the platform, or should we just incorporate it?
 - lets just compile it to wasm, expose some specific C calls
 - wrap it in C and only expose our compiler API

Making it a requirement of the platform should be a non-starter, especially when we could do it all in WASM itself
 - the compiler we ship should contain the whole wasm

Ok, so it has to be part of our language, but we're going to basically use a wasm that we will scaffold
 - maybe we write it in wasm lol

And then either we output wasm (binary wasm)

One dumb question, do we want tower nodes to be our IR?
 - It would make sense in a way, however the IR doesn't need to be flexible
 - technically the IR should be something that's fixed, doesn't change
 - we know at the end of the day it has an exact structure that doesn't change
 - still could be done with tower nodes... hmm

the only advantage to tower nodes being our IR is that whatever low level language we invent is a "subset"
 - like, we can make it very C like with calls, ops, etc
 - everything is a call
 - but also just emitting wasm is easy too

 - I also like that we can just keep up with wasm, and people who know wasm can contribute
 - wasm has a lot of IR features
 - but binaryen api is just pointers, not safe to expose...

But suddenly now the version of binaryen that we integrate changes our language and what wasm we support
 - Still ok

Technically our C++ compiled wasm can host our allocator, and lots of other functions
 - we'll make it very explicit which are provided, generate a header based on binaryen exports
 - it could also implement our parser, until we rewrite it

Ok, so we're going with a C++ base then, built to wasm, with very explicit and curated exports

is the IR all just pointers
 - or do we somehow just emit wasm text
   - emitting wasm text seems unessesarily slow, but possibly nicer to deal with, and safer

at the end of the day, we really want our language to be able to output it's own optimization loops
 - should be able to easily extend optimization passes, etc
 - really build most any language you want in tower

so how do we start then...

I love the idea of writing a wasm parser and outputting wasm, but it has to be in a style that's getting ready to BE binaryen

Ok, what we need to do now is start to author the language

Lets just go with WASM text for now, we can always change this later
 - Like syntax directed translation

we write the whole parser in C++, including all the extension stuff
 - expose all the functions for creating tower nodes and callbacks
 - everything will be rewritten, but for now it all runs in WASM so it's good
 - we do rely on binaryen, but we state the intention to eventually remove it
 - we also specifically are built on top of wasm for it's mostly deteramistic characteristics and safety sandbox
   - but may someday opt for an entirely safe and 100% determansitic IR

it can just be pointers for now, and we can regard that as a generally not safe part of the language
 - but our future iteration will switch to a fully in language represented IR that's safe and ref counted
 - would need to be sort of generic, who says what tower nodes should look like if they don't include components??

maybe we could support a primitive lowering pass

ok so with binaryen, we also get text parsing

but you know what, we're making fucking tower, it's all built in itself
 - I want to one day be like, yeah bro look at it all
 - the baseline can still be wasm, in fact we can make that explicit

But I'd rather get to writing, the faster I get into writing tower code that's not just wasm, the better
 - that way I don't need to implement the parser yet, 

shoot, I just realized my compiler also needs to run wasm too, does binaryen simulate?
 - seems like it does have the ability to simulate
 - so we technically could ask the OS, or we could rely on binaryen...

at the end of the day, this can all be rewritten, keep it fast and loose

so we start by writing in C++, the tower API is the creation of tower nodes

WE JUST NEED TO GET STARTED, C++ IT IS!

it's more like a context has all these virtual functions about how to deal with tower nodes
 - and so our "passes" are just us dumping a context
 - but it's not like we want the context to stop existing
 - we just want to upgrade it, and ideally it should be compatible

lets think of the example here, we get our new version of interfaces all written within tower, structs as well
 - now it's time to "upgrade" the language to v2
 - do I need to specify all the rules again?
 - are we deleting everything before (pointers included?)
 - when do we discard tower nodes, or do we always keep them around?
   - 


tower_node_create(context: tower*) -> node*
tower_node_destroy(context: tower*, node: node*) // panic on double destroy, check free list header
tower_node_attach(context: tower*, child: node*, parent: node*) // automatically unlink and relink
tower_node_attach_member(context: tower*, child: node*, parent: node*, member_name: string) // automatically unlink and relink, any member of the same name is removed
tower_node_get_parent(context: tower*, child: node*) -> node* // or null if it's the root
tower_node_get_child_count(context: tower*, parent: node*) -> u32
tower_node_get_child(context: tower*, parent: node*, index: u32) -> node* // or null if not found, panic if out of range
tower_node_get_child_member(context: tower*, parent: node*, member_name: string) -> node* // or null if not found
tower_node_get_kind(context: tower*, node: node*) -> string
tower_node_get_parent_member_name(context: tower*, child: node*) -> string // returns empty string if not named


lets just try doing this in wasm, why not

Ok screw that we're doing C++ to get started faster
 - along with binaryen

we'll build out the wasm
our end goal is to have a parser that can parse the very basic BNF rules, as well as support

one of the first things we'll want to implement is structs and types
 - and determining how pointers to types work (members, etc)

ah fudge fine, the whole reason of tower is to build from the ground up so lets start with wasm
 - it'll force us to use super optimized structs

Now that I can add on to the root parse rules, what's the first thing to do?

all I can really do is execute wasm, so now I need to start building my tree
 - tower nodes are implicitly created during parse
 - but we can also create them by hand in wasm

do we need the tower node executor framework setup yet?
 - can dependency inputs and outputs just be strings for tasks, and we determine what those strings are later (paths)
 - for now we can do the terrible dependency graph, that just checks if every task is ready to run
 - maybe we allow tower to hook into and redefine this execution model

since we will be reducing leafs first, those tasks will be the first registered
 - so as long as we basically keep things in order, most tasks will complete in their first iteration

maybe when tasks fail, they are automatically placed on the back
 - technically when we finish a sweep, only failed tasks will be left
 - we keep iterating over the lists until nothing changes

lets not get clever until we get stats, we'll know better what to do

- I think this task model where we check our dependencies first is golden
  - does the check need to be separated from the task, e.g. two different functions?
  - In the future we'd want one to be immutable

We still need to know if a tower node is alive, and kill it when the associated tasks fail
 - otherwise those tasks would either keep the node alive, or just continuously fail to run (stuck)

lets worry about that when we get there, assume nodes don't delete
 - and if any task gets abandoned, we'll know about it

So a task has a condition, an action, and a failure action that produces an error

When a task completes, it's removed (do we ever expose tasks?)
 - can you hold on to them, cancel them? etc
 - tasks can self cancel, or complete
 - not sure if they technically need a state for that or not

basically if the function returns continue, it continues, if it returns complete, it stops

tasks may need to be closures at some point, may need to keep their own data

I think this is kinda everything, right?

is => wasm() actually a task, or is it immediate?
 - does the parser just keep going?
 - when do tasks get executed, after parsing completes?

I like that the tokenizer is lazy, but is anyone driving the parser, or is it just driving until it consumes the rest of the input?
 - does it ever pause to execute code, and if so why?
 - code can add parse rules and affect globals
 - maybe that's reason that the parse rules should run the wasm on parsing immediately, not a scheduled task
 - we can make adding parse rules during scheduled tasks like an error or something
 - but we can add them while directly parsing

So our tokenizer and lexer need to be able to rebuild
 - I love the idea of lazily building a DFA from an NFA (only building it for rules we go down)
 - but not required for now

so the bare minimum C++ is this:
 - parser/tokenizer implementation
 - callbacks for creation and linking of tower nodes
 - basic language and tokens for BNF rules (subset of tower)
 - parser rules should just be specified as tower nodes if we register them (or maybe reparsed as strings...)
 - tower API for emitting and compiling wasm (can just be text for now)
 - run and parse wasm text format
 - parse wasm after the end of a rule, and in the main context at any time
 - some defined way that different wasms modules run and link together
   - maybe don't need to specify an entire module
 - tasks and dependencies, cancel/complete
 - allocator

With that we should be able to write everything we need

We could do direct syntax translation as we walk and parse rules
 - but now we have dependency tasks, so why do that when we can do proper codegen
 - I would want to do a basic type pass, even for our early language (again, subset...)
 - maybe we specify the binary format of tower nodes long before
   - so we don't replace them or change them in any way
 - this would mean the concept of type ids needs to be figured out
 - our initial C like language would need to decorate tower nodes like usual
 - if we did component based design, that means attaching components

I don't like that our object format changes (header, etc)
 - what if we add weak refs
 - we should make every feature we add to the header an extension
 - I want to be able to include type safe object systems
 - as long as we can extend remote types, who cares what we do
   - since at any point, if say we couldn't get a weak ref because they didn't declare they supported it
   - make them support it!

Maybe the same approach I take for hot compiling will work for updating tower nodes
 - we would need to reallocate
 - move all "features" from one to another
 - can figure this all out later

my main question is now, without just doing basic syntax directed translation, how do we do types?
 - ideally I want to attach types like components
 - we can reserve a type id that's literally for the type id structure
 - type is basically just an interface though that gets the actual typeid
 - we can just start with a tower node for now for primitive types
 - we also want a type id to just be tower node itself

 - we can also attach this as a named member, kind of like 'pre interfaces'
 - or, all types of types (pointers, primitives, interfaces, etc) all implement a single type interface
   - and we can add that to the tower node, mapped by the type interface
 - yeah, that would also work, both are OK
 - whatever we do, ideally I want it to be a subset (we don't change how we represent types on tower AST nodes)
 - attaching a 'type' interface makes sense
 - like adding a component would register every interface

you know, maybe we just support "queryable" interfaces?
 - when we add a component to a composition, we don't want to have to register every single type we want as fast lookup

can we pre-compute component configurations?
 - kind of want a map at this point, constant time lookup
 - instead of having a map on every object, we can share maps of similar component layouts
 - basically lazily pre-compute interface lookups
   - the map just tells us the index, or memory offset, something like that

ok, but either way for getting started we can just map the single base type interface
 - as if it was a component that declared "interface Type"

C++, binaryen, compiled to wasm with no-std
 - possibly just use the wasi SDK for this
 - ideally minimize wasi calls, we shouldn't import them if we don't need them
 - may need like wasmtime or something, not just binaryen
   - if we want to execute WASI functions and not have to get them all working in binaryen
 - browsers would only need binaryen
 - really, the wasmtime part should be the "host" platform, not part of our wasm
   - unless we really want to have an entirely emulated version of wasi
 
checkout binaryen as a third_party sub-repo
 - create scaffolding cpp project with cmake
 - add binaryen to our cmake project
 - get WASI sdk docker container building our scaffolding (no-std, or minimize it ideally)

copy welp code over just to get things working quickly (can rename and all)
 - get parser running with tests and samples
 - show that we can execute our wasm


-----
for minimal rebuild in the future, I really need to dive in to see how it would work as I haven't done it
before and don't know the pitfalls (I can only imagine). So the idea would be that we can basically save state
at any point in the parse, including the states of tasks and their wasms, etc.
 - almost like docker container layers, we 
 - basically if we see a change in source, it rebuilds from there down
 - but that's sort of a very basic minimal rebuild, how can it know just to ignore whole subtrees in the parse?

note that we must call _initialize on the scaffolding, we may want that in the future too (maybe we can rename this)

tower_memory_allocate
tower_memory_free

-----------

what does our parser definition look like?
 - do we really care if it's tower nodes or not, ultimately it's just going to come down to a BNF API

If we used tower nodes, how would it work
 - we'd really just be building the parser from a tower node configuration
 - and ultimately, it would only work on specific BNF type nodes
 - I think it's like trying to create an IR for no reason

ultimately we have productions, non terminals, terminals, and grammar symbols
 - character sets are probably important

what would the tower API look like?

I think it should be tower nodes, a non terminal node with a bunch of children

in order for a node to "be something" 

one thing I like about using tower nodes, it simplifies our API
 - otherwise we have to be able to create and delete productions, add and remove grammar symbols, etc

tower_production* tower_create_production(non_terminal);
tower_production_remove_element(tower_production*, index)

push, pop, insert, clear, etc

basically array like functions

We pretty much need all those for tower node children

so if tower nodes need to be component based, is that insane?
 - ultimately parse rules are pretty small
 - it's not like we're making a ton of them

Ok, so in order to support components we just need the type map structure
 - again, this can all change


TowerNode* non_terminal_type = tower_node_create(context);

TowerNode* non_terminal_node = tower_node_create(context);

struct NonTerminal {
  string name;
  owner TowerNode;
};

NonTerminal* component;

tower_node_map_component(non_terminal_node);

type ids need to be perminantly kept around...

Ok, we'll start off with component based design, doesn't matter what the implmementation is
 - We can make it match the future implementation of components
 - and we can static assert that all the member positions match so everything lines up
 - but also, we can totally do the thing where we swap out all the pointers on the tower table

are captures really just mostly syntax sugar for generated capturing rules?
 - like captures ultimately just make a new rule right...


maybe we also want all tower node components themselves to also be tower nodes

What would this look like in tower...

can you add more than one of the same component?
 - if not, then it's not really a substitute for children
 - we can allow it, but it only ever maps the first one (kinda like, it's only useful if it's useful to the component)

// Adding an owner line implicitly adds a children: TowerNode?
type TowerNode = {
  owner: TowerNode;
};

// So now tower node can have an owner and children
// When we do this with a single chain, we should mimic C++ inheritance (with no v-table)
// Basically structs packed one after another so that they can be memory compatable
type CharacterRange = TowerNode {
  charStart: u32 = '\0';
  charEnd: u32 = '\0';
};

random thought, what if the ability to get interfaces off a type dynamically via pointer was a feature
 - like a vtable feature, you can add it to your type
 - normally in class/struct in C++ with a virtual function gets one
 - I guess we can still mark functions as virtual or not, that tells us if we get a vtable...
   - but I kinda hated that, like typeid doesn't work cause no virtual functions... gross
 - or all of our allocations have vtables, at the end of the day tower should be easy
   - value types don't have them, reference types do, something like that

new problem, should components be able to not have owners?

type TowerNode = {
  owner: TowerNode | null;
  component: TowerNode;
};


We're only able to extend exact types (or named types), basically types with a fixed single type definition
 - they're basically like a reference to an extensible type

so effectively owner can work with any extensible type or null
 - is that just a built in thing for owner...
 - that's probably a reason owner shouldn't just automatically extend children for components
 - maybe for any non-extensible type other than null, we require children
   - basically we always require the type to declare that it has children that support our type
   - but if the type does not, and it's extensible, extend it instead!
   - kinda like that...

dumb side note, if we can get a good dependency system going, the tasks could possibly be kept alive for monitoring if types change
 - e.g. if a type is extended, then it's like a minimal rebuild only for things that are directly dependent upon that type
 - we just need a really solid dependency system (also it shouldn't run all the time, ideally only when it needs to)

-----------------
ok, so how should tower node allocation work

at first I wanted to just allocate it on the heap
 - but it gets odd if we do 
   type CharacterRange = TowerNode {...}
 - because how do we extend a type that is only a pointer now (unless tower node allocates children)

originally, tower nodes were not supposed to be extended so it didn't matter,
but I'm grappling hard with what I want the tower node format to be

since TowerNode is a named type or exact type, whatever we settle on, then those are hard requirements.

whatever it is, it must match structural interfaces

I need to be able to match an interface as it will be very common, say for example I want to easily
check that this tower node represents an expression between two integers

// IF A VALUE IS USED IN A TYPE, THEN IT IS ASSUMED TO BE THE TYPE OF THAT VALUE (THE TYPE OF A CONSTANT IS A CONSTANT TYPE)
// In typescript: 'a' refers to a value, but is being used as a type here. Did you mean 'typeof a'?
// you could only ever use a value within a type if the value was in scope of the type
// but ideally it could be part of it, and we would actually check for that specific value

let rootExpression: TowerNode = ...;
let binaryIntegerOp = rootExpression as TowerNode {
  has: BinaryOperator;

  left: TowerNode {
    has: typeid(i32)
  },

  right: TowerNode {
  },
};

let binaryIntegerOp = rootExpression as TowerNode {
  [0]: TowerNode {
    
  },
  [1]: TowerNode {
  },
} | null;


----------------
I almost think that keeping tower nodes as individuals themselves (not extensible) is better
 - yes the api is larger, however it forces a sort of nice structure where tower nodes are just the skeleton
 - and components are just components, not children
 - we can still allow components to have sub-components, at the end of the day you can declare anything in tower
   - so maybe I shouldn't try and make it generic, it's just kinda weird

I actually don't really like the idea of single chain inheritance either for tower nodes
like I feel people are just going to start getting odd with tree structures


Ok now one last thought here, lets revisit our token stream:

String       RuleCharacterClass       String
            /    /        |    \
         Not RCCRange RCCRange  RCCChar
            /    \      /    \     _
      RCCChar RCCChar RCCChar RCCChar
         a       z       A       Z

This is just the raw parse tree
 - We could do it where these were all tower nodes, but what does the raw parse tree mean in component form

our nodes should be pretty generic as just the parse tree, since we don't even know what components to attach
Parse trees haven non-terminals as parents and tokens as leafs

ah, so it's just a component that's like NonTerminal, and Terminal
so if I just read the roots of each token in the stream, they should all have a single NonTerminal component


basically our token stream is [TowerNode{ has: NonTerminal }];
 - So now because of that, every element in the array I can go tokens[0].NonTerminal // and that's a component!
 - AWESOME!
 - This was a great example of the parse tree as it comes in raw (RuleCharacterClass), but then we transform it
   - we compact the RCC ranges into 'CharacterRange' classes as a single component
   - we extract the two RCCChar values (should do a neat interface style cast in the future here)

WAIT, IN THE FUTURE GRAMMAR RULES CAN DEFINE REALLY SLICK TOWER TYPES!!!
 - since we know the structure, we can actually apply TYPES TO THE PARSE RULES

Holy cow that's awesome, that means when we do 


"elixir implements the language constructs such as defining modules, functions, other macros etc in the form of macros"
 - phew, almost thought it was going to be a nice looking language... lol



token RuleCharacterClass = "[" $Not("^"?) (RuleCharacterClassRange | RuleCharacterClassChar)+ "]" => fn(node) {
  
  type NonTerminalNode = TowerNode {
    has: NonTerminal;
  };
  type TerminalNode = TowerNode {
    has: Terminal;
  };

  // The type of node here would be
  node as TowerNode {
    children: [
      TowerNode {
        has: NonTerminal {
          name: "Not"
        }
      },
      
      TowerNode {
        has: NonTerminal {
          name: "Not"
        }
      },

    ],
    RuleCharacterClassRange: 
  };


  // We can detect everything, like if a rule is nullable (locally)
};

// OK LET ME COME BACK TO THIS, MY BRAIN HURTS

how about before we do ANY type syntax, we just start with compile time type ids, so we can start immediately generating
types


it's like we really want to be able to say children also has a child that looks like this
 - possibly by index, but 


Ok one more question, how are we using member names, I guess for the parse tree are we even?

what if types are always just functions that return compiletime typeids, and functions with no arguments are called implicitly
 - then I can really just use that compiletime typeid anywhere

let Type = fn(): compiletime typeid {
  ...
}


let b: Type ...;

I'm basically just arguing that the phrase type

```
type X = ...;
```
is just syntax sugar for
```
let X: compiletime typeid = ...;
```
OOOOOOOOOOOOOOOO OK

except I don't want X. to give me typeid members
 - unless typeid implements MEMBER ACCESS OPERATOR :O!
 - no that's not good lol
 - I think it's more that X is the statics

```
let typeid_X: compiletime typeid = ...;
let X = typeid_X.statics;
```

Trying to think of how to make statics feel correct when associated with a type
 - I've always felt weird about this concept
 - like ultimately, accessing a static through a type is using it just like a namespace
 - that's probably why C++ did :: instead of .
 - but . feels so goooooood

it's almost going to be so common to specify has... I wonder if has can just become a shorthand

let t: has RigidBody = owner;
// same as
let t: { has RigidBody } = owner;

------------


------------
when it comes to destructing a type, we need to do it when the ref count reaches zero
 - at compile time, the compiler should look to see if the type is virtual or not
 - if not, it calls the destruction logic directly on the memory before releasing it
   decrement reference
   placement destructor on memory
   free memory
 - if it's virtual, it needs to consult the vtable or typeid to see what virutal destruction needs to occur
   - for tower, this also involes destructing extension members, etc
 - does that really mean every path needs to generate an if == 0 and inline the destruction logic?
   - or, every type can have it's own dec-ref logic that simplifies the whole process
 - then it's up to the optimizer if it wants to inline that function or not, sweet

------------

maybe we don't make any of the tower functions automatically add references
-------------------------

I think our idea should basically be this:
 - we define the initial version of tower nodes in C++
 - right now, lets not worry about the memory layout or implementation too much
 - in the future when we fully settle on how the object layout works, we'll make a matching type definition in tower
 - we can even do the whole replacement thing where we replace all the tower functions with ones implemented in language


---------------
ok so this is a case to consider, normally we wont' allow construction of a component without an owner

in Rust, we would require the owner member to be filled out
 - but also, we'd probably mark it as private, because we require construction logic to be run for the component
 - including adding itself to the owner, dependencies, etc


---------------
next problem, for component based design, reference counts are kind of strange
 - since the owner holds a refernece to the child, and the child holds a reference to the owner
 - it's a unique pointer situation, except we do want others to be able to store references

that's why we did handles for Zilch, Components were OWNED entirely by Cogs

we could do something odd like the stack thing we were going to do before
(any ref count non zero on stack destruction is an error)

So in this case, if the component has a reference count that's not 1, it's an error

is that... ok?

I mean in most cases we don't store components anywhere
 - dependencies... actually that's kinda cool, it's implicit!
 - as long as the other component stores a reference, we can't remove the dependency
 - do the components also store a reference to their owner too (cyclic?)

Maybe components don't increment owner reference count
 - but then how is it safe?
 - when an owner reaches ref count 0, it will always detach all components whose reference counts must be 1

e.g. it's illegal to let all references to an owner die, but keep a reference to the component around

kinda weird to throw during destruction...

I guess we can also have the component store a "weak reference" to the owner
 - and throw if it's ever non-null
 - and call some sort of detach logic, effectively akin to a destructor but like, you've been removed from the component...

ayah....

we pretty much want components to die exactly when their owner dies
 - we honestly want the references to kind of just... be the same?

 - like a reference to a Component really just increments the owner reference...
 - whoa... that's interesting...
   - other than just using pointers under the hood, I wonder what would be a way to declare this in language generically
   - like, my object is fundamentally linked to the lifetime of another object
     - my reference is their reference

- maybe independent of components, we can "own" child objects, and their references are references to the parent
- therefore the must exist within the context of the parent only, even if dynamically

Somewhat interestingly, since it's reference counted, we know all references are gone once we release the last one

Technically components only need a parent pointer to be SAFE, owners don't need to point at components
 - Once the reference count reaches 0 on the owner, we also know that no components are still referenced
 - HOWEVER, we would have no way of calling component destructors or freeing component memory if we had no pointers from the owner
 - so the owner must hold direct pointers to components in some fashion and guarantee that they are freed and destroyed too
   - fascinating, can we make this concept generic...
 - 

it's really just unique ownership but with reference counts
 - and the idea of redirecting the ref count


So what are the fundamnetals here, its safe IF:
1. The child has no reference count and refers to the parent (child must have parent access)


Ok this is really cool, one thing though that also breaks down is ownership trees of tower nodes
tower nodes point at children, children point at parents, etc (another cylcic reference)

we almost just want roots to have reference counts here...
 - like, I don't want children to keep parents alive so parent should be weak

hmm...

what happens if children did not increment their parent
 - it would be OK if when the parent died, it removed the child's parent pointer (nulled it out)

hmm actually I forgot about this, parents are already nullable!
 - so when the parent dies, it can null out the child's owner pointer

cool, so basically now we have this ownership concept too that involves weak parents
 - but rather than doing a whole crazy thing for weak parent pointers
   (like python allocating another extra object with it's own ref count)
   we just declare owner: TowerNode | null; and somehow this means it's weak?

it works, maybe we can formalize these ownership safe models

as long as a type removes all references to itself... hmmm!

ok but now dependencies get weird
 - need a separate count for them to hold them
 - weird wierd

I want tower to also be an exploration of alternate, less restrictive forms of memory safety than Rust / borrow checker
 - I love what Rust is doing, but I would sacrifice 10% of my speed to have the language feel like C# (but not GC fuck GC)
 - unless there's a way to do GC objects, but have it be less intrusive (manually called or something)

---------------

so right now the model is this:
 - lets start thinking of object's as just having hidden intrinsic features that you add in
 - so like reference counting should be one
 - when you attach that interface to an object (maybe implements?) you also somehow affect the storage of that object
   - that includes what kind of memory restrictions you need, like can it go on the stack?
   - you're basically defining all the parts of a type, fundamentally what we can do with it in memory
 - Originally I was thinking of this as 'ref' or & on the type, but now I'm thinking of this a little differently
 - like how can an interface modify a type behavior? defining a handle

I do like the idea that Tower can have many different kinds of objects for different scenarios
 - with a really good default of ref counting
 - handles that go null when the object is deleted
 - handles that always return a valid object and throw on access
 - unique handles that only grant temporary access or are moved/nullable
 - raw pointers
 - but the point is, these features are added to the object piece by piece, along with the type definitions

--------------------

so now rather than having a "TowerHeader", we need to pick
what's appropriate for each, like TowerComponent has no reference count
 - hmm, that does make things a little harder to be generic
 - like, it's nice if we know the first 4 bytes of every object is a reference count
   - otherwise dynamic features like interfaces need...
     well actually they just need to consult a virtual object anyways to say "what happens when I make a new handle"

maybe in tower we can have this handle concept, almost like a built in operator
 - basically you can return another type that the compiler will use as a fill in for storing a handle to your object
 - the fill in object is always given a pointer to the original object
 - it's like we have the "make an initial handle" and then "copy/move a handle"

-------------

ok lets go back to ref counting, it's now tower node specific

ok for example, we don't want people to inherit from tower node
 - so tower node is not virtual
 - and we would require virtual destructors when implementing an interface
 - also tower node is sealed so we can't add features to it to force it to allow inheritance / extension

hmmm

 rust fundamentally does not have inheritance

check out rust deref tho:
struct StructA;
`````
impl StructA {
    fn name(&self) -> &'static str {
        "Anna"
    }
}

struct StructB {
    a: StructA, 
    c: i32,
    // other fields...
}

impl std::ops::Deref for StructB {
    type Target = StructA;
    fn deref(&self) -> &Self::Target {
        &self.a
    }
}

fn main() {
    let b = StructB { a: StructA, c: 123 };
    println!("{}", b.name());
    println!("{}", b.c);
}

`````

I think the issue is we're still trying to have it all. In rust, structs are just data types
the fundamental question is, why do we need virtual destructors, isn't that a relic of C++ inheritance
 - but rust DOES have destructors
 - how do they work in memory??
 - because I can create a Box<Thing> and thing has a vec, which must be dropped?
 - hmm, but it's never virtual
 - so I think rust kind of gets away with it because the type that's dropped is never virtual
   - or even if it is, it's only got one single destructor to every call
     - well, it technically has some built in ones since it needs to destruct members

so a question would be, in rust can I hold a dyn interface to keep a reference counted object alive
 - and then have it destruct through the dyn...

------------------------
https://users.rust-lang.org/t/v-tables-differences-between-rust-and-c/92445

I would draw it like this:

&dyn Foo (Trait Object)
+--------+----------+
| Data * | Vtable * |
+--------+----------+
    |        |
    |        v
    |     +---------------+--------+----------+-------+
    |     | drop_in_place | size   | method_1 | ...   |
    |     +---------------+--------+----------+-------+
    v
+---------+---------+--------+
| field_1 | field_2 | ...    |
+---------+---------+--------+
And the C++ version:

Foo* (Pointer to derived object)
    |
    v
+--------+---------+---------+--------+
| Vtable*| field_1 | field_2 | ...    |
+--------+---------+---------+--------+
    |
    v
+----------+-----+
| method_1 | ... |
+----------+-----+

------------
interestingly, Rc in rust is always on the heap, it's like Box
 - there is no RC where it's not allocated

hmmmmmmmmmmmmmmmmm, I don't hate rust's way of doing things (no inheritance, no including types within yourself)

what would components be?
 - a type component that wraps one single type, no inheritance

still has a virtual destructor

Anything dyn is a fat pointer, so it has to be a vector of fat pointers to be polymorphic

HUMMMMMMMMMMMMMMMMMMMMMMMMM

how does an Rc work with dyn objects
 - I see, so they just specialized Rc and Box to work different for dyn object than pointers

ok so now lets really think about 

I started brainstorming what a purely component based language would look like, difinitively
 - we make a statement with tower, the cool parse rules, the awesome compiletime syntax, how templates and generative types work
 - and then we wow the world with a new(ish) language paradigm: Component Based Design

My declaration is that the only kind of pointer or tree traversal should be with compositions and components
 - the language has no other way to represent anything

all structs are pods

We do need some sort of way of doing mixins however, and I still like A & B for this
but what if we went simple, all types must be named, like you can't do {} & {}, has to be C = A & B
 - I don't really know where I'm going with this, let me just think a bit


or like, what if we stated that all heap allocated structs are always colocated
 - even member by member colocated? we can do that weird thing in arrays where the first member of struct[0]
   is grouped with the first member of struct[1], and so on, like each member of a struct is an array
 - this can be a hidden detail
 - so inheritance and interfaces is really just a single pointer to the super


I guess the idea here is there are only owners, components, and pod data types/structs, that is the only data types

that would basically imply that interfaces are tied to data, like a component is an interface
 - 


One thing I want to think about is GameSession, Space, etc, how do we solve those?

in Zero, Space was a Cog, but it was kinda awkward, there were space components that had a dependency on their owner being a Space

Space being a Cog meant it could have normal Components too (like ZilchComponents)

owner Space

what are all the types we want
 - owners (cannot be components themselves, lets just put our foot down, it's too awkward)
 - components
 - pod data types
 - ref counted pod data types?

we're also saying there's no dynamic cast or interface stuff, there's only asking an owner for a specific interface
 - ok yeah lets be weird with it, lets say that 100% for sure there are no other interfaces other than 'base' component interfaces
 - I'm just thinking of a world where there are no Rust style traits or anything, it's just very simple
 - like maybe BoxCollider actually just has a dependency upon Collider, but Collider is never constructed by itself...
   - the only advantage I can think of is that you can swap from a BoxCollider to a SphereCollider

Lets also imagine a world where the owner tree/hierarchy concept is extremely lightweight
 - then we may refer to objects just by their components, but really we're saying 

like when I allocate an object and point to it, I'm really always pointing to an owner that has a child

and maybe we can make optimizations for owner declarations that specify they always have specific components (allocated together)

 - this was the true component based language I thought about a while back, lets try it as a thought experiment

should derived components just be sub-components?
 WHOA

so like... Collider is basically a component that declares that it ALWAYS has at least one sub-component

Collider = {
  has: ... hmm wat
}

I really do like sub-components for some cases, 

the whole structural interface matching part of tower is where I feel like it starts to diverge from being native perf
 - but maybe we can make this extremely fast, and practical/useful

the main problem we're trying to solve with Collider is that while it may also have data (we don't know or care) it
does have an actual interface, with un-implemented methods
 - and now the question becomes who gets to implement those?

moreover, one part of Zero that I think is paramount is events
 - basically we're declaring that owners should always have parents and children, regardless


when would I want to use a heap allocated value that wasn't a composition?
 - lets try and think of how we can make it so you would never know the difference...
 - I'm basically saying what if there was no struct/interface keyword, just components, no pods

we're basically saying that any interface on any object should be swappable

a: RigidBody

is the same as 

a: { has: RigidBody }

we only pass owners...

I think at the end of the day we're still going to want pod structs, containers, etc


all of this brainstorming is great, but I think the main blocker I have right now is that I don't know the data
layout of tower nodes and tower components

because Rust gave me a serious pause with thinking about how vtables in C++ work vs fat pointers in rust

wait, one cool thing about components, they don't need to store their RTTI type this way
 - if it's always only stored by the owner, kinda cool


So the weird thought experiment is that components could basically require
that an interface gets implemented, like a pure virtual function with inheritance
 - it means that the component isn't valid without being added along with a component that implements that interface
 - this sounds exactly like a dependency, EXCEPT
   - A dependency is offering an interface
   - This is a dependency that is asking someone to implement it's interface
     - it's odd because it's still going to call it in the same way, gah it's so similar
   - it's really like BoxCollider is saying it implements Collider
   - BoxCollider cannot be constructed without a Collider
   - Collider cannot be constructed without a type that implements Collider interface
   - That means it's cylic
   - Normally in C++ the base can "access/do work on" the derived members through virtual functions
   - and the derived class can just directly modify the base because it IS a base
   - but here we're saying in order to communicate, we need pointers on each to each
   - but obviously, in the case of collider, it's like an opaque interface

really is just a dependency upon an interface... right?

Hmm, it's odd then, is it...

------
interface ColliderPureVirtuals {
  fn get_mass(): f32;
}

component Collider {
  dependency ColliderPureVirtuals; // except these methods should also appear ON Collider too
}

component BoxCollider implements ColliderPureVirtuals {
  dependency Collider;
}

------
and then maybe we have a shorthand for the same thing above
------
component Collider {
  fn get_mass(): f32; // because this is not implemented, it means this component implicitly
                      // has a dependency upon any component that implements our interface
}

component BoxCollider implements Collider {
  // implicit dependency Collider;
}
-----
and in the same way, we could have done
------
component ColliderPureVirtuals {
  fn get_mass(): f32;
}

component Collider {
  dependency ColliderPureVirtuals; // except these methods should also appear ON Collider too
}

component BoxCollider implements ColliderPureVirtuals {
  dependency Collider; // collider methods could appear on BoxCollider too, correct?
}
------

component BoxCollider {
  implements Collider {
    ...?
  }
}

so then what if we just don't have interfaces, just components with functions only

I guess the kinda cool thing is since we can always separate out interfaces into separated components


lets just imagine tower not being so low level anymore, except for when we're starting to build it
 - but just imagine it wasn't built in all these layers yet, just direct to component support
 - we'll break it apart where it makes sense, but for right now just picture a purely component based language

so basically interfaces are really just questions answered by compositions

components can implement multiple interfaces
 - remember, when we say implements, we have a dependency upon it too,
   and we offer an implementation for their unimplemented parts
 - lets examine diamond of death (bad example but yeah):

component Stringify {
  fn toString(): string;
}

component Damageable implements Stringify {
  int damageTaken = 0;

  fn Stringify.toString() {
    print("Damageable");
  }

  fn takeDamage();
}

component Breakable implements Stringify {
  fn shatter();

  fn Stringify.toString() {
    print("Breakable");
  }
}

component Player implements Damageable, Breakable {
  dependency RigidBody;

  fn Damageable.takeDamage() {
    ...
  }

  fn Breakable.shatter() {
    ...
  }
}


so if I look at it, Player implements Damageable and Breakable, which both implement toString

when we add a Player component to an owner, we must also add Damageable and Breakable
 - both of them also have a dependency upon Stringify
 - however Stringify has a reverse dependency upon whoever implements its interface
 - in this case, Stringify will lookup and get the first component that implements it

component based design doesn't make much sense when you start duplicating components, unless you use that functionality for meaning

so that's it, thats component based design in a nutshell

ok so what's the object layout look like

oh and owners are basically just basic ass data types, always ref counted, deletable, etc
 - treat them like cogs, but you can't abandon them either

cogs had members like name, so maybe we allow owners to declare info like that
 - but I almost rather that be a component, go all in
 - because the second I allow owners to have data, I need to suddenly have syntax for owners implementing interfaces, etc

I'd almost rather that all compositions are equal in binary memory,
but the type system differentiates them by which components they are allowed to have

 - We would make spaces just by creating a component, and that component can hold a new root tree

yeah, if you want to "Tag" a composition as a specific type, use a tag component, it's light weight
 - and the component can even have data
 - e.g. this is a Cog, the composition is never a Cog
 - compositions are just magical creatures with a fixed set of functionality and API
 - we just define exactly how they work
 - so we're saying all the components just have the same "dependency Cog;" line

some components implement a specialized tree of pointers that point down the heirarchy
 - lets worry about hyper optimizations later

having all compositions be equal except differentiated by type only is really fascinating

do I even need to declare compositions or give them a name?
 - lets just imagine for a second that compositions are completely generic
 - you can't declare them, they have no component types
 - then what makes up different compositions? maybe header components
   - just basic components that provide some shared features like object names, etc
 - so we fundamentally don't have compositions that can only take a specific type of component
   - all compositions take all components
   - components don't have an owner type, it's generic and always the same
   - but we can be a CogComponent by just having dependency Cog
 - we can call those like Header components
 - Space might have a dependency on Cog, and TimeSpace has a dependency on Space, etc

- ok this hierarchy along with safe handles describes all heap objects

so compositions have children, parents, all that jazz and are ref counted
 - basically they have a single form and definition, there is only one composition
 - but you distinguish types by their pieces
   - it's almost like if it looks like a duck, but more like if it has a mouth and some eyes
 - we make sure compositions support all the best types of handles (weak, strong, throwing, deletable, unique, etc)
 - we also make sure the memory layout is kick ass for game engines, cache, etc

now the last little hole is how we handle struct types, really just pod types
 - I think we just support pod types? it doesn't really make sense to have component based design on the stack

maybe my "frozen" objects concept... basically the idea of freezing a known composition definition
 - but I think structs are just better for this
 - the freezing advantage would have to be like it's minimizing the object
   - it would have to look exactly like components in memory, but it's statically composed together
 - eh... I don't feel it plays well

 - I think just pod structs is fine
 - initialized entirely by name
 - maybe not pod, we can have vector<T>... fn vector(t: compiled Type): compiled Type {}

 - how do we handle containers and all that?? full component based design would scream and say EVERYTHING POLYMORPHIC!!

 no arrays, only compositions
 - no containers, ONLY COMPOSITIONS!!!
 - bleh

well, maybe it's not "frozen" per se but I do think the idea of declaring an interface with "has" means it must have that
 - which means that as a type, we can reserve memory for it and all of it's dependencies
 - (the ones we know about that cannot be dynamic)

so I guess my weird point about frozen things is that they could also remove features (sealed)

is trying to throw component based design at everything too much of a hammer?
 - I'm trying to make it ok so that there are no arrays or anything
 - like I really want compositions to be your arrays too
 - ah see that's where defining a composition starts to make sense
   - I want to be able to say what my children type can be
{
  [{has RigidBody}]
}


that's saying a composition whose children all have a RigidBody component
 - can I do better?
 - casting is just comparing interfaces, or doing runtime casts

compositions are tuples too

{
  [0]: { dependency RigidBody };

  [
    { has }
  ]
}



// is this what they look like?
{
  RigidBody,
  [
    {RigidBody}... // array syntax
  ]
}


when we say the name RigidBody, it is also usable in places as {RigidBody} (any composition that has a RigidBody)

[RigidBody]

so RigidBody is shorthand for {RigidBody}
 - but RigidBody in parameters actually does take a RigidBody pointer
 - and declaring it as a member actually does hold a strong ref count to the composition that holds the RigidBody

- but what we're saying is that technically the two are one to one convertable (when it delcares that it has RigidBody)
  - so we implicit convert and treat them as interchangable
  - well we may end up needing to always pass the composition anyways to keep it alive


[RigidBody...]

is actually just

[{RigidBody}...]

which is actually just

{ // composition
  [ // children
    {RigidBody}... // composition of RigidBody
  ]
}


component RigidBody {
}

let a: [RigidBody];

a.add(RigidBody { // shorthand to construct a new composition with that component
});


composition as [RigidBody...] // this cast would check that all children have a RigidBody




that's really just saying `a` is a composition whose children have RigidBody components
 - may have a lot of other components

so for this composition, 


who cares, people still use JavaScript to date...
 - and ultimately these are at least structs and assembly level implemented

WOW I LIKE THIS LANGUAGE KINDA!
if I stop thinking of it as being bad.. I mean look at JS arrays, they're arrays of variants
 - so much allocation and random overhead, GC, prototypes, etc

this is not rust!

we can also do tuples too, [A, B, C]

can we do compositions on the stack?
 - or are we just going to be happy enough about temporary component allocation

technically if we do structs, it won't make any sense with using compositions as arrays
 - since we can't do [Vec3...] if Vec3 was a struct (it has no parent, etc)
 - that's kinda gross though, we can't even have just an array of vectors?
 - But I mean, we can they just have parent pointers... who cares JS is worse!!
 - so everything is a component, it's higher level
 - again, just look at an array of vec3 in python or JS... this is leaps better

 - also does it always have to be heap allocateed?
   - hmm, but maybe that's ok, especially if we're using a fast typed allocator for cache
 - lightning fast amortized O(1) allocation and freeing
 - yeah ok, whatever!
 - basically we're saying it doesn't matter if we're heap allocating or not
 - ideally we want an array of Vec3 to be colocated together
   - but they may not be contiguous, while they are all allocated together with all other Vec3s
   - and the likelyhood is that if you allocate a bunch in a row, they will sit nearby each other
     (they first occupy free spaces, so there's always that issue first)
   - but otherwise they're mostly together, I can live with that

yeah, it's kinda crazy, everything is structs...

and then really serious code gen for templates and stuff
 - maybe we can define new tricks with reflection type + compiletime generics
 - like whatever we need for Rust style enums/match, or TypeScript type expressions + typeof/instanceof

it's not a native perf language, but it's pretty damn fast
and it's crazy extensible

everything is a component

hmm, what if I just want
  position: Vec3

value types again?

component Vec3 {
  // implicit owner, dependency count, etc
  x: f32;
  y: f32;
  z: f32;
};

component Foo {
  position: Vec3; // hmmm?? does that create a whole composition?
}

I guess this is the C# part where it's still nice to have proper value types

but virtuals, interfaces, etc only work with components

hmm, well dang I really wanted [] to be syntax for just compositions / tower nodes

well fundamentally we really just need a few kinds of data structures:
 - hash maps, arrays, lists

maybe at the end of the day, a component based language isn't one that forces everything to be component based
 - after all, C is part of C++, you can always go back to structs

I guess that's a part of rust I really like, they only have structs, pure data
 - and then interfaces implemented for those structs
 - it's very simple and beautiful
 - although they do have tuples, arrays, and a complex generics syntax

so then if I take a step further back and ask myself, with a language that has component based design built in
can it be broken down into pieces or features, instead of having one type of composition?
 - The only reason to ask this is that it could be interesting and maybe break the difference between struct/component
 - part of me kind of likes the idea tha a struct can be a component with the right members
 - hmm, but maybe composition is still fixed?
 - I love the idea that component is just syntax sugar
 - that's one thing in my language, I never mind syntax sugar because it's shorthand!
   - it's not a "new way" of doing things, just shorthand for something larger

maybe compositions can store structs that don't have owners?

I guess we haven't figured out component layout anyways yet, kind of want to talk with Chris...

I mean, the wacky ass way this langugae would work is that everything is a component, including like Vec3
 - component just means it has owner and like next/prev pointer or something


Lets take a look back at this problem:

component Foo {
  position: Vec3; // hmmm?? does that create a whole composition?
}

yes, yes it does!

is there any way to do value like semantics though?
 - value really just means it clones every time we move the handle
 - I mean... yeah we can, that sounds horribly slow but whatever!

what are f32, i32? I feel like those have to be structs...

how do I do [i32]?

I mean even in JS that's a variant, so at best it's an array of variants
 - basically just saying a composition doesn't sound that crazy
 - can compositions be stack allocated and copied?

 - technically the pointers to a composition

And even though we're allocating it, we're doing so with a lightning fast allocator (amortized)
 - but cache coherency becomes the issue

I think if I go all in on component based design, I don't know if I'll love it as a language
 - part of the reason I love TypeScript so much is how flexible it is
 - even just native TypeScript + Rust style traits would be a big win in a langugae

let me just think more about component based design languages

we could also do the equivalent of "boxing", where any value type that gets used in an array or whatever gets boxed
 - C# does pretty much that anyways with allocating objects

basically even if we have value types, an array of value types would be

struct Vec3 {
  x: f32;
  y: f32;
  z: f32;
}

Hmm wait maybe thats the idea, maybe we only have structs, but any struct can be used as a component?
 - it would implictly wrap the struct
 - hmm but what are dependencies, unimplemented interfaces, owner pointer that the user can access...
   - that should be the things you get when you declare you are a component...

so maybe we still have structs and components, but structs can be components (just no owner, etc)

[Vec3...] // is just a composition whose children (not components) all "has Vec3"

same as

{[{Vec3}...]}

lets make it illegal to add more than one component of the same type, I like this design
 - if you want children, use children, not components, components are a map functionality

I don't mind where this is going for a high level language, but for a native language, eh

components listen for events on their composition

events can propegate in many ways

events are just a composition themselves

we can drop the has keyword too

cog.send({
  RigidBody {
  },
}, Propegate.Up);

hummm, not quite the language I wanted to work on...

I mean if it just had the type expressions and other native "handle" features, I think I would not mind

hmm, well now that we have an idea for our composition structure, can we start structuring tower nodes?

component based design is really just a piece of tower
 - the main reason it's so important right now is I'm trying to find a way to represent it in language

what I'm overwhelmed by is implementing Rust style trait generics, C# where clauses, etc
 - type constraints
 - A where clause should just be a compiletime function

lets plan to have components, but lets still just focus 

lets stop worrying about making tower nodes this perfect layout
we can always change it later

and ultimately, even if we dont match in memory we can always just wrap the tower node API
with a structured / componentized version of the tree...

For tower's foundation, we need both a generic parse tree (to hand to wasm)
 - as well as a tree structure for specifying productions/rules
 - we also have no types


instead of ever exposing how tower nodes actually work, lets just give them all the facilities we would ever need
to wrap a tower node with our own type

 - Tower node's can actually take pointers to our types, but lets not assume anything
   about memory or how components work in the future
 - this is just tower nodes, their own thing, yes they are extenisible
 - we don't try and assume an rtti layout or anything, just give it what's needed
   - like for a destructor, tower nodes just have to take in a destructor

so then our goal is really the easiest extensible structure in wasm
 we can make it match later!

lets just gooooooo

ok so one last thing to remember here, our parent pointers are weak pointers
 - child compositions don't increment parent refs


--------
random other dumb question, can we make a language with safe fat pointers?
 - basically like C, but you can use pointers freely because we know they'll throw
 - even the concept that at runtime everything is a refcell...

can we also enforce single writer multiple readers?
 - we just need to know which chunks of memory are considered "together" and mutate with each other
 - this would be data you'd normally wrap in a mutex

Basically our version of this data always has a header
 - for multithreaded, maybe we can do mutex or atomics
 - basically it's like wrapping a thing in a block, but if another bigger block contains it, it removes the inner wrap

basically the idea for pointers is that we can always point to the head of an object
 - we will guarantee that these allocations are done on some aligned boundary


we don't want think about sizes or sizeofs, because ultimately we could have a void* or u8* that just points anywhere
 - how can pointers to anywhere be made safe?

I think to answer that, we have to think about what memory is even ok to point at
 - generally when thinking about a safe language, we can say that a
   pointer can never be initialized to point at an unallocated location (stack or heap)
 - And also that if a memory is freed, the pointer should become invalid or nullable
 - and that even if a new memory is allocated in it's place and the pointer coincidentally points there
   it should still be considered invalidated/nulled

the most granular way of breaking this up would most likely be stack frames and malloc allocations, alloca's
 - since the stack is reserved usually by call frames (unless things like alloca exist)
 - so the idea would be that at the header of every one of these is an intrusive list of all pointers

 - I would love to do handle style, but with pointers that gets difficult
   - and I think fundamentally a well behaved program should not have outstanding pointers
   - so a well behaved program will have O(1) deletion (only one reference left to it)
   - though it makes the pointers "fat", it only does so by next, prev, and then the pointer itself
   - the intrusive list also acts as a reference count, we can either auto-delete or mark it as an error that it wasn't deleted
     - you get to pick your behavior, some of them are lowerable to release mode variants

 - ah but it's a bit more, because technically we also want to point anywhere inside of a memory block, so we also need an offset
   - we need to also constrain any memory moves to be within the block, as well as reads

I think most of our work will be in reducing these pointers and the cost of moving them around
 - reads are more likely, so read cost is not inhibited

 - we do know that pointers have a fundamental width (the size of the type)
 - meaning that no matter what, all reads from a pointer (except ones like memcpy)
   will have a compiletime size associated with how much they can read
 - we do NOT want reads to have to check if they are within range every time
   - ideally we make an assertion, we do NOT allow you to ever even point, without reads, to invalid memory
   - that is because it is far more common to read from a pointer than modify a pointer
   - so we want to do the bulk of our safety constraints on pointer modification, not on read which is more likely
 - so because of that, it is possible to ensure valid pointers
   - casting from a u8* to a u32* will incur a check, if it is within range of the memory it's pointing at

 - so that means the pointer does not need to be any fatter, just next, prev, and ptr
   - because we know the size we'll read, upon construction of the pointer (or cast to pointer, etc)
     we do the checks to ensure that pointer never points outside the memory

Moreover, a pointer can only point within it's allocation, not to other allocations
 - that would be considered a different pointer and constructed from another location

this all ultimately means that you cannot construct a pointer from a constant (such as foo: i32* = 0x1234 as i32*;)
 - I think it's probably a security benefit to not ever allow that, we can ensure only some things can be pointed at
 - at some point in our language, we could offer a way to point at 

this can be the default style for allocations, it's almost all C++ like but safe with an overhead
 - but then we can introduce other handle primitive types, which can ease pointer restrictions but still be memory safe
 - reference counting, etc
 - and ultimately, we can always compile in a way that lowers some primitives to actual pointers (almost like "release mode")
   - technically "unsafe" but great for non restrictive environments and sandboxes like wasm

ok next issue, if we can point anywhere in the class and at anything, then we can point to internals of pointers (next, prev)
 - we could mess with those, which is unsafe

I think then it just matters what we're pointing at, and if we're allowed to point at that thing with that kind of pointer type
 - is it reading on boundaries?
 - is it a type that makes sense?

maybe we can come up with a very fast table lookup for if the pointer is viable or illegal
 - and I could see how we could enforce rules, even with structs, on certain layouts
 - I wonder if there's a fast way to generate all these, maybe they cache on the first occurance?
   - if it ends up being so big, then have a way we can declare and pre-cache (this memory here will be casted to X)
 - but that also means when we allocate memory, we need to give it an explicit layout
 - stack memory is also weird, doing it by stack frames makes no sense as it will change with optimizations

We also need to think about arrays!
 - does it matter how the memory was allocated then?

I guess we're saying that the way pointers work should be up to the struct itself

but in C, memory is just memory, the interpretation does not matter
 - our language is basically taking a step and saying that there is a physical layout
 - in rust, is it just considered unsafe to re-interpret memory?

I think we can declare that every stack allocation and every malloc has an
interpretation of memory that cannot be changed without deallocating
 - it's kind of like it's type, but really just says what memory exists where
 - it's only needed if we get a pointer to that type (which is always the case with malloc)

so that also means an array of u8's can be re-interpreted, but not as pointers!
 - I love that... 

imagine there was a pointer inside a struct to a Thing
 - and then we reinterpret the struct itself to a new struct, which now has a pointer to a Foo
 - we would check that the pointer to Thing is compatible with the pointer to Foo


some memory should be opaque, and never pointed at
 - for example, if we did rust style enums, the tag value is not technically exposed (unless we did expose it...)
   - but that memory shoudln't ever be pointed at or modified, it's unknown!

it's all simply about binary compatability

what about arrays or uninitialized values?
 - we also have to guarantee that all memory is initialized, not really important for pod types, just pointers
 - but, how do we say allocate an array of memory and keep track of what's allocated and what's not?
 - well at least we know we only really care about the vector/array case,
   which can use a moving index to let us know the capacity vs size
 - I don't love that this would be a special case, but otherwise we'd need like some duplicate memory to keep track



Well, so far I really like this C style version of Tower
 - we make it so some primitives are effectively invisible to the eye (maybe, we'll see)
 - I kinda like the approach where a type can say what it wants, but then it's overridable on the pointer level

------------------
lets think about the reading from array while modifying it case
 - if the iterator was pointing into a vector, and then the vector resized, it's now pointing at invalid memory
 - I think the primary issue here is that it could have invalidated the pointer, but you don't know until it lazily happens
   - our code is technically still safe, but has a sleeper bug in it
 - can operations know if they might affect memory (the allocation) and therefore check if any strong references exist and bail?
   - we really want push to throw while we have an iterator already to it
   - and the thought is that maybe push can declare it affects memory somehow
     - or maybe this can be inferred somehow


I'm not sure about threading though, or about data modification guarantees
 - I guess the nice thing is that, at least manually right now,
   we can invalidate any references at any point to a piece of memory
 - so even if a hash table operation may have added something to the hash map which may cause it to rehash
   - a pointer to anywhere inside there could be invalidated on every add, even if it didn't break it
 - again would just be cool to mark functions as "affecting memory"


so for threading, in a way we can guarantee that nobody else can get references to a piece of memory
 - if there is only one reference
 - but really, we would need to scan the tree to see that the only references held are all internal
   - e.g. if the tree/graph is an island
 - this is basically like a lightweight version of serialization of state for workers
   - but instead of serializing, we're just guaranteeing nobody else is reading it
 - I do like the pure sandboxable worker tasks that you can basically farm out (all copied in out)
   - if something needs to be lockable, maybe it must be it's own memory object
 - like an object doesn't need to have one ref count if it's always a mutexed object

 - thread safe, sandboxy threads too (launch threads with only dependencies needed, pulls out the wasm)
   - I love the idea we can send this over a network and create a networked wasm protocol for massive parallelism
   - the compiler/runtime should support this, pure tasks! (copy in out!)

note that if a type is wrapped in a mutex, because of the intrusive linked
list we can't technically even move around or modify pointers to it
unless there's some sort of magical atomic intrusive list that you can add/remove links from different threads?
 - how could something be accessible by different threads
 - instead of intrusive list, it could be atomically reference counted
 - basically just changes the kind of pointers we're allowed to use

side note, two threads should't be able to point at the same thing and have it null out, that should be an error
 - only one reference left to an object when we delete it for threading purposes...

---------------------

We really do want memory to be contiguous
 - maybe for arrays, in memory we separately contain inlist headers per element
 - it means array allocations are actually larger (and must support resizing)

maybe when we declare that a memory has a structure, it must all be initialized simultaneously to a layout that is binary compatable
 - that means memory can even be uninitialized (which also destroys all intrusive linked lists)

note that if we don't care about getting references to the elements in the array (e.g. copy is fine)
then we don't need that extra data
 - it's not that the array is immutable, it's just that we only ever copy in and out

hmm, technically as a feature memory would be relocatable
 - obviously if you supported relocatable memory, then you can't lower the pointers into a release mode style
 - but it would allow us to re-allocate arrays and keep pointers in-tact
   - so you can iterate through it while pushing to it
 - basically we can allow re-allocate, and if it fails it can even move all the pointers and re-allocate
   - the re-allocation would require a binary compatable definition if we were to re-allocate

--------------------

last odd side note, instead of the memory storing anything about it's layout, can that be stored by the pointer?
 - like rust style fat pointers, obviously this will make our pointer even fatter
 - we could certainly do this, so then the memory itself never needs to know it's own layout
   - but what gets odd about that, we already need an intrusive linked list
   - we'll think about it, may still need it for rust style interfaces

in our language, we're saying anything you can get a pointer to anything
 - it's almost like it's changing the type of the value we got the reference to
   - it's wrapping it in a structure that has an intrusive list
 - we can infer this for local variables
   - we can also get a reference to any child member of a struct, as it will always get the reference from the parent with an offset

our pointers look like this:
  next
  prev
  pointer // always points to exactly what we want it to, reads are FAST
  inverse_offset // offset in reverse from the pointer to the header that holds type info, etc


with these odd but safe pointers/shared contstructs, we can basically make our language look very nice
 - especially with compiletime constructs

 - then we might consider just adding rust/dyno traits


if we start off considering memory as uninitialized, and then any initializing operation on that memory
marks the memory as being initialized

I am thinking what it might look like to support a dynamic layout in memory
 - maybe we can support it but it's slow?
 - my dumb thought was we obviously need some data that tells us what the layout is
   - what if that layout could be dynamically constructed
   - but that kind sounds slow...
 - that's really if we just wanted to allow anyone to use placement construction anywhere

fundamentally we could have different kinds of memory for optimization:
 - an N layout, always instantly initialized (needs no flags)
 - an N layout, uninitialized but initializes in contiguous array order, needs a number to say how many initialized

memory needs an intrusive list next/prev (does it technically need a prev?)
 - in a weird way, pointers could all point to themselves, and we only need a pointer from the memory block to the first
   pointer in the list (any pointer really, they're circular)

so a memory block that can have references looks like:

head_pointer
layout_pointer
initialized_count

hmm wait, do we need to have the head_pointer per array index?
 - since we want to keep track of references to each individual array item
   - but we want a pointer to be able to move anywhere in the memory block right...
   - simple, even if we keep a head pointer per array index, if the pointer moves within the array
     we just relink it to the correct position!

- for the initialized count, the only check we do is that the placement construction only happens
  on the end (or placement destruct / back to uninitialized)
- it's actually the act of placement construction that validates it's in the right place for that type of memory
  - same with placement destruct (again, only adds O(1) overhead...)
I like this concept that the overhead is always fixed, possibly always constant (at least we can argue it's constant)

I had this idea that for uninitialized memory where you "placement new" anywhere inside it
 - my dumb idea is that we just update the layout pointer with a new layout pointer
 - but this is kinda... like almost just maintaining a separate map again

or the memory maintains a sorted list of layouts, still need up to N of them...
 - the layout would also need to store the location, now it needs a layout pointer AND index...
 - it could always resort to allocating if it needs more than like 1/4 it's space, that wouldn't be terrible
 - keep in mind we also need a head pointer per too!

I'm OK if this memory layout ends up taking up 2x memory, but not 4x that's crazy
 - 2x would only work if we character by character specified primitive types
 - if we could do everything in 4 bytes aligned chunks, and somehow only needed 1 byte to represent...
 - for example, we could have special bytes for all the primitives, and then all byte values above
   went to a table (like 8 bit color pallette tables)
 - that table could store the layout pointer, only needed for pointers
 - technically, our memory could have more than 256 types of pointers in it
   - so we could also have a 2 byte table version, etc
   - we always pick the lowest memory size we can get away with
 - I think in practice the sorted linear search is going to be the best...
   - it's worst case scenario is the same as the indexed table


there might be something clever I can do, because while I know u8 is a problem technically any struct of size 1 (u8)
 - can only be composed of a single u8 (struct foo { test: u8; })
 - a pointer at minimum is 4 bytes, and if we have our fat pointers they are much larger
 - so maybe we can do everything by 4 byte layout minimums
 - but we have specialized layouts that represent the only ways to get to 4 bytes
   - 



I think the ultimate problem is that imagine we had two types, u8 and i8
 - maybe we could boil them down to the same type, but just imagine different
 - if we initialized an uninitialized block of N bytes of memory byte by byte, we would need N layout pointers
 - think if we did u8, i8, u8, i8...
 - I guess if you really need that type of memory, go for it

is this a DFA recognizing task??
 - the idea of representing memory in small words/characters
 - because every time we need to reinterpret a pointer inside a pointer, we need to do the same thing again
 - it's almost like a grammar... I think? pointers are like grammar rules / non terminals, they can be recursed
 - the job of checking if two layouts match is really fascinating
   - I'm sure we can come up with this algorithm
 - it's kinda crazy but I think it almost is, like in one form we can have the memory boiled down to primitives
   - and then run a dfa/grammar to check if a layout matches those primitives, sort of, interesting...

an even dumb thought too
 - we could even support storing function pointers, and cast those too... craycray!
 - it would ensure it's all correctly memory compatable, safe, and initialized

---------------
ok well that's the whole idea, safe memory!
 - we can invalidate any references to memory
 - we can ensure we only ever point to initialized memory
 - pointer reads and writes are as fast as in C
 - pointer manipulation however is checked (we make every attempt to make this fast)
 - allocations are also fast

 - have many other different safe memory models to choose from
   - reference counting (allows weak, strong, deletion/handle)
     - pointer is only 4 bytes
   - unique pointer / moved
     - pointer is only 4 bytes
   - child/parent pointer
     - a relationship between two allocated objects that says the lifetime of the child is tied to the lifetime of the parent
     - but these relationships must be kept, the child may not exist without the parent
     - children can also have children, so long as their removal keeps the child attached to the parent
     - somehow declare and enforce this relationship...
 - That gives us a ton of options to memory manage and control our pointer sizes
   - and the language is maintained as entirely safe
 - this does mean that any time we pass in a pointer to a function, the pointer itself must be copied (added to a list)
   - even calling on self.something()
     - if self is a normal safe pointer, a new one must be copied/added to the intrusive list
     - if self is a shared pointer, we can pass the pointer in directly as 4 bytes, but must inc-ref and dec ref as we leave
 - that's fine, it's a fixed and quite small overhead
   - inlining will be our hero here

If you use this special kind of pointer, in a release mode it can be treated as if it's just a normal pointer IF:
 - It's the normal intrusive memory model safe pointers
 - The pointer cannot be invalidated / is set to error/panic on memory deletion
 - This also means that any attempt to manually invalidate memory, such as a container add, would immediately error/panic
 - I say panic because this type of error should not be catchable, we're basically using this as a debug mode only assert
Same thing with reinterpret casts, if you do one that panics on failure, that can be release mode
 - release will not check if the type layout matches, nor will it panic

Work with JD to come up with other memory safe data structures
 - mainly just thinking about that crazy intrusive list structure
 - really want to be able to write an engine in this, and then go release mode if we need it

 - when we specify an object, we should specify all the traits of it's lifetime
   - is it deletable
   - does it have a reference count?
   - weak references?
   - etc

---------------
dumb thought, we were already planning that `a + b` is just `a.add(b)` approximately
 - why not the same with type expressions?

like isn't a type expression really just a binary call

A | B is really just A.union(B)

it's just taking in compiletime types...
hmm!

---------------

for tower nodes, right now we have it where we call alloc and then construct in place
 - this means the user is also responsible for deleting component memory
 - but that kinda sucks, I'd rather have it where we allocate a tower node with extra space leftover

-------------
note, one interesting place that might make sense for a borrow checker is unique pointer and similar...
 - we could potentially also use the borrow checker to know when we can use just a raw pointer,
   or if we need to pass the full fat pointer
 - an interesting idea, lets think on that...

-----------

double initialize of the same region also needs to be illegal
 - I think we just need a concept of an initializer that covers an area of memory, and sets up the layout pointer
 - it's illegal to call an initializer over any area of memory already initialized
 - the single object and N array cases are very easy to check
 - the general area memory case is more difficult
 - technically we could support initializers overwriting pod data as it doesn't matter...
   - that can be an option

we can also support memcpy
 - it will copy initializer data / layout maps and ensure it's not trouncing anything
 - we can support overwriting pod data too (u8s to u8s...)
 - again, we can support a version of this that just panics, but in release continues (debug panic)

I love the fact that you can always code in a style that is exactly akin to C,
 - and ideally as fast if you stick to exact C primitives
 - as long as you compile down to a release mode setting
 - we can do the same with alloc and free
 - just always look for these opportunities, it will make the language attractive

and ultimately you can also just use pointers too

so we'll have the concept of "uninitialized" pointers, pointers that are allowed to point at uninitialized memory
 - maybe that kind of pointer (like a void*) can point at anything
 - we can also use it to uninitialize memory
 - note that they don't have a size, other than a single byte for convenience (and because that's the smallest)
 - that means they aren't checked when you point them around
   - I do like that, it's basically just an integer
 - Hmm, not exactly, tbh it never makes sense to allow them to point at dead memory, just any live memory
   - they just don't care about layout/memory format or initialized/uninitialized
   - they are specifically tied to a block/region of memory
     - so they still act the same as our fat pointers
     - but you can use them to initialize memory, with placement new, etc
 - as much as we could support initializing a pointer from an arbitrary int
   - I think it defeats one of our security guarnatees, that a pointer from one object can't point into another
   - and pointers only point to valid places in memory
 - (unless running in unsafe release mode)
 - so these void*s still need to point at valid memory, but we can move them anywhere around inside
   - initialization of a type, e.g. Player {}, should require one of these pointers
   - almost like placement new, allocate(memory) Player { }
     - maybe this is also how we indicate we want a heap object...
     - I wonder if I can find a tasteful way to make the memory layout a type in the language
       - and you specify which type of memory block you're allocating
       - basically allowing the user to extend memory block types
       - but we do want them to be very fast in execution
   - we'll think on that syntax, something that I never liked about C++ was that you kinda had to pass that down
     - e.g. if you wanted to do placement new, you had to pass down the buffer for every function that wrapped the placement new
     - I guess it's not that bad, but Rust solves this by moving everything
     - Things are moved into the memory, but theoretically their optimizer and aggresive inlining/templates
       will cause it to actually just initialize the memory in the correct place

What comes out of the allocation should be the Player*

so basically memory blocks have this virtual behavior (but ideally we want it almost unvirtual for perf...)
 - check if a given uninitialized pointer manipulation is legal within the memory block
 - check if an initialized pointer manipulation is legal within the memory block, and pointing at a matching layout
 - whether the memory supports deleting or not
 - on deletion of the memory, ensuring that the memory has been fully uninitialized (or that it's pod...)
 - also responsible for constructing the initial pointer handle
   - this is probably how we should implement reference counting / shared ptr
 - on initialization of a type from an uninitialized pointer
   - validate that the size of the memory's uninitialized space will fit the type
   - if our memory block is an array block (contiguous initializations, only need a count)
     - the first initialization at 0 determines the type
     - because technically the memory can be unallocated
     - if we deallocate all the values, the the layout type will vanish (the next 0 will set the type)
   - store which area is now initialized to what layout

 - I also wonder if uninitialized pointers should only point at uninitialized areas
 - Technically, if you need to UN-initialize something... use an initialized pointer!
   - how do we ensure that the last pointer you delete dissapears...
     - this is where the compiler keeping track of moves is kinda nice, we can "move" the pointer out
     - we could still do this in tower, like after deleting
     - How does rust know when a value has been moved out of a heap type, is there a runtime component that knows it's gone?

-----------------
fn test(b: Vec<i32>) {
    println!("woooo {:?}", b);
}

#[derive(Debug)]
struct Foo {
  z: Vec<i32>,  
}

fn main() {
    let f = Box::new(Foo {
      z: vec![1, 2, 3],
    });
    
    if rand::random() {
        test(f.z);
    }
    println!("Hello, world! {:?}", f);
}
-----------------
error[E0382]: borrow of partially moved value: `f`
  --> src/main.rs:18:36
   |
16 |         test(f.z);
   |              --- value partially moved here
17 |     }
18 |     println!("Hello, world! {:?}", f);
   |                                    ^ value borrowed here after partial move
   |
   = note: partial move occurs because `f.z` has type `Vec<i32>`, which does not implement the `Copy` trait
-----------------

Basically as far as I understand, because they completely keep track of borrows, they know if anyone has borrowed the parent
 - while a child has been potentially moved out (illegal)

For us, we're not going to keep track of any of that
 - at best we could do so inside a single function, but this is a full fledged feature of Rust

we can make it so you can only delete Player* | null, and it nulls after...
 - Actually I do like the idea that it always must be initialized, so either you declare it as nullable
   or you fill it with something else
 - how do you delete a value inside an |?
   - maybe that's just something delete implicitly works with
   - Player* | Enemy* | i32 | null; // how do we delete this?
   - I guess you can cast it first?
   - or delete just works on any union, and will invoke delete on any pointer values stored within
   - but then you have to set the union value afterwards

let a: Player* | Enemy* | i32 | null = ...;

delete a; // since null is present, it will implicitly pick null

let b: Player* | Enemy* | i32 = ...;

delete b; // error, b will be left uninitialized and must be set to a value

delete b (123); // eh... we need to come up with a syntax that's like the allocate syntax kinda

--------

let mem: void* = malloc(sizeof(Player));
let p: Player* = new (mem) Player{};



delete p; // error...


Or deleting p could set the leftover single pointer to null
 - obviously if you have Player* | null, the type will change to null and will force you to check it everywhere
 - at the end of the day, nulling out P would result in a panic if we ever read/wrote to it
   - and we shouldn't be able to perform any pointer manipulation on a null pointer, other than assigning to a new value

- this is one of those 'release unsafe' things, where the Player* doesn't need a check on every access, we just read
  - our language may need to handle traps
  - in wasm, null pointer dereference isn't invalid
  - maybe we make it -1? that way we can actually trap on out of bounds access and see it's "null"
  - I like these panic behaivors

- we can also introduce "unsafe" pointer | null checks
  - basically you can say we assume this pointer is not null in some cases, and the check will vanish
  - it's when we "know" a value is not null
  - oh, better yet you can just do '.' off any pointer, but that's implicitly a panic if it's null
  - basically we're saying that in order to do . off a | null type, we can 

 - as a side note, we could introduce uninitialized typed pointers, basically a pointer that will eventually become typed
   - it would just be something that users could use for type safety
   - Player*, Player uninitialized*... something like that
   - The reason for this would be that you aren't allowed to do pointer manipulation if it goes beyond the size of
     uninitialized memory
   - so technically it acts a little different than just a pure uninitialized memory pointer
   - I wonder what Player* | Player uninitialized* would be...
     - basically just a tagged union of the two identical structured fat pointers
     - still have to check the type...
     - I don't know why this thought is interesting to me...

So a safety guarantee in Tower is that, provided you're not using the raw pointers (we'll have some sort of true unsafe)
 - you can basically provide true access control:
   - if you can't reach an allocation in code, you can't mess with it
   - we can then define "visibility" or access, for compile time features
 - so then the cool thing is unless someone finds a way to exploit one our types (a bug)
   - then people can't mess with memory that isn't theirs
 - then the pointers are all checked and safe
 - and you can ultimately work in this "debug panic" mode that sheds all checks in release (C mode)
   - maybe we just require code coverage tests, lol!
   - I guess that could be a mode, for areas that are covered by code coverage, we allow release optimizations...
   - too far and too fancy I think (basically just PGO...)
--------
when we do array.push(123), we're implicitly passing array as Self
 - at some point, internally, push is going to check if it has memory capacity
   (side note, can we just get this from the memory object, kind of like C++ delete on arrays)?
 - some function is going to eventually operator on memory, either resizing it (deleting and mallocing)
   or it has enough capacity and we don't touch it
 - ultimately, our compiler CAN track that a particular function will attempt to resize

is this something we should push onto the user... basically that memory references can be invalidated...
 - like in this case, `array` obviously owns the memory pointer
   - in fact, it's probably just a unique ptr, but we can get our intrusive fat pointers to it
 - when the array dies, the memory itself will be destructed

we really just want to make sure this case is always detected

can we just avoid having references if pointers always automatically dereference
 - hmm, well then we can't have any methods on the pointers themselves...
   - or we do that Deref thing rust does, where it hides it, sure why not

let a: array(i32) = [1, 2, 3];

i32* foo = a[1];

foo = 123; // am I assigning a pointer here, or a number to the pointer value?
*foo = 123; // should we still have to do this

// no comparison between pointers and i32, so it must be the value of the pointer
if foo > 100 {
}

I guess as long as we're careful with pointer types, we can kind of perform the above
 - but it would be weird if 123 was a value with implicit conversions to either...

that's why references exist in C++
 - we could always introduce references, but I kind of don't want to

I like the idea that you can just use pointers interchangably, and address-of can be implicit too

Self pointers technically can be ignored on cleanup, as they will be released
 - but that's a rare case anyways (maybe sentinel node in a data structure)

one thing, I do like the idea that we can just allocate "memory blocks"
 - but technically the array-like memory block needs to know how many elements it can store
 - because that's how many inlist head pointer's it needs to reserve (one for each element that is initialized)
 - that kind of also tells us we need to know the actual size of the element (because if we know how many bytes)
 - I know initially my concept was that memory blocks could pre-declare the layout
   - maybe that's just required for array like memory blocks
 - The arbitrary memory block will use some reserved avilable space to represent an in memory data structure
   - but if it runs out of space, it will expand memory (realloc first, but otherwise point to a new block of memory)
   - it could technically realloc in place..., that's one of those odd features we can support

one memory type could be copy in out only, no references
 - the only thing we need to make it safe is to ensure that it can't be indexed out of range
 - so we still need a count of how many things initialized
   - normally we do placement new on a type with an uninitialized memory pointer
   - but in this case, we can't get that uninitialized memory pointer
   - we really just need to ask the memory block for the next place to allocate
   - or in this case, maybe we do support an uninitialized memory pointer
     - because technically we still have pointers to the base, there's no issue allowing an uninitialized pointer
       to point anywhere in there
   - but the difference is that when you initialize a type there, you cannot get a pointer to it back
     - e.g. new (that_kind_of_memory) Player() results in a Player, not Player* (or maybe void)
 - we could also allow this type of memory if the pointer is actually just an offset
   - but now the cost is not zero to read/write

I'm really just trying to make the use case for pod data... I don't want a u8 array to be 5x the size in memory
 - the only reason I need an intrusive list header pointer PER array elment is because I want to make sure to null out
   exactly the pointers that need to be (or error) upon uninitialization of the end element(s)
 - the reason for that is because we never want to allow pointers to point at uninitialized data
 - this is so that we know the memory read/write can't corrupt (dereferencing a pointer made from random uninitialized data)
 - we really care a lot less for anything but pod data
   - at most, there are maybe some weirdness with float format and word boundaries
   - but other than that, it's really just pod data is pod data
 - so if the struct is pod, an optimization is that we kind of don't care about tracking initialized values
   - but we do want a guarantee of at least one time initialization, this can be memset, initializing all elements
   - and only pod data can be put in that memory
   - we can point anywhere in it, and there's no invalidation because it's always valid

Hmm, interesting concept
 - if the memory is "always valid" memory, meaning it's initialization is tied with allocation
 - and uninitialization is tied with deallocation
 - then it means the data structure doesn't need to track anything
   - in this case, it's like we're making a fixed array of u8's all initialized to 0
 - fascinating, I love this! this is probably how we do fixed arrays too
 - we can still have an array wrapper that wraps this type of memory and pushes/pops
   - but it means we can get references to anywhere in the array, even when we pop (it's just pod data...)
   - kinda don't like that, but you know maybe we just don't do arrays like that
   - I mean it's fast, we don't really touch the pointers
   - Yeah, actually, it's a good thing for really perf heavy code
   - and I'm sure knowing if a struct is pod (whether it has pointers/memcpy able) is a good optimization feature
 - pod array, cool


we can also have static array types (initialized to a fixed N, all is initialized at once)

it's really just when we have pointers I think

what about structs that have A | B types, are those pod?
 - I think the requirement is that we can't be allowed to put anything in an invalid state
 - well actually, no that doesn't really matter, you can absolutely mangle your own stuff
 - but you can't mess up other people's memory lol (NO GASLIGHTING)
 - We can make sure the format of A | B things is very well defined with tags, and what those tag values will be
 - So you can always predict what the memory layout will be
   - but technically, you can set that tag to a value that isn't valid
   - upon trying to read it or do anything with it, our code will panic
   - I guess one bummer is that we could say the compiler could make optimizations for that
   - I'll see when I get there
 - I know the idea is that any opaque thing we don't want to give the user access to gets it's own layout marker
 - it's basically just that thing that if we muck data, we might muck an algorithm, but that's sort of up to us

Tower, a programming language without gaslighting
 - You can mess up your own memory, but you can't mess up anyone else's!

is const and transitive const more meaningful here?
 - it still really doesn't mean that it's const
 - but does it mean we can have different interfaces that can return const pointers
 - const is just saying whether the pointer is mutable through that pointer
 - in Rust, what makes it so novel is that you can't mutate something while someone else has a borrowed read
 - so it'll never be something where you're looking at something with a "const pointer" and then it suddenly changes

maybe const is a bad name, mut/mutable kind of makes it a little better, because then I have a Player* and Player mut*
 - in my head, it doesn't imply that Player* won't change, just that *I* won't change it

ok I like that, as much as I want it to be C like I think it's better to be explicit about mutability
 - does it ever make sense to keep track of mutable pointers vs read only pointers differently?
 - some sort of enforcement of multi-reader OR single-writer...
   - for threading, we could have as many readers as we want to the same memory
   - it would have to be like a magical intrusive list that's thread safe to add to
 - hmm, just something to think about!

let player: pointer_to mutable Player = ...;

with C style syntax it would be

let player: Player mut* = ...; // pointer to a mutable Player

Yeah, I really like this...


when we get a mutable pointer to a type, in C we could obviously just re-interpret cast it
 - but I think it might be something you want control over
 - almost like returning a safe reference (basically just a pointer, but you can't do "unsafe" stuff to it)

basically our root level security guarantee is:
 - wasm sandbox
 - secure memory access (only get pointers to objects that have granted you access)
   - cannot move the pointers to other objects in memory
 - secure type or struct level layout enforcement

so basically even though we offer "raw" memory access, we can return restrictable types that don't allow
the user to circumvent type enforcement
 - is that what references basically are, just a way to preserve type safety (no pointer arithmetic, only reads/writes)
 - must be an error/panic to destroy any memory that's still being referenced
 - you can usually get pointers from references, but not in this case, it needs to basically be guarded

---------------------------

side note, it's almost like pointers need to say what kind of memory they could point at
 - changes the pointer layout... maybe can do A|B to mean the pointer needs to support either... heh...

-------------

ok, so the tree and components are ready for prime time
 - now we need to just write the parser part
 - the parser needs to produce tower nodes to represent the parse tree
 - and we need to be able to consume tower trees with parser components for building parse rules
 - random idea, we can support sub-rules if we encounter a Production within the rule definition
   - We can use the Production as a NonTerminal (maybe even have them inherit so the layout is the same)
   - But since we know its a Production, we special case it and recursively walk it's children
 - This is how we can define sub-rules, we can even have a parsed format for this too
   - so that we can just define our rules as replacements
   - hmm, doesn't exactly work, we want sub rules that we don't have to reference
   - we really want to do this all in replace
 - and in this case, we're only replacing B*
 - it means we need like scope blocks or whatever


For Kleene closure
  parse E = AB*C;
into
  parse E = AZC;
  parse Z = BZ;
  parse Z = epsilon;

I need a syntax to specify inline rules
 - does it ever make sense for this syntax to be shared anywhere else
 - or like, treated as a scope, is it code?

  parse E = A <parse Z = BZ; parse Z = epsilon;> C;

We (almost) generically want to say add this stuff to the outer scope:
  parse Z = BZ;
  parse Z = epsilon;
- and then result in just Z...

what if the "replace" rule was also just wasm?

parse E = A <parse Z = BZ; parse Z = ;> C;

Expression = Expression '*' => replace(<parse Z = $Expression Z; parse Z = ;>)

This problem of whole statements inside expressions is not a new one
 - I sort of like the idea that we define a syntax for this
 - But I don't like making random gross new syntax
 - () is always used for grouping, and valid in every expression syntax
 - {} is what we use for scope normally
 - in regex, {N, M} usually means, N to M of the previous
 - lets imagine we didn't do that operator

parse E = A {parse Z = BZ; parse Z = ; Z} C;
parse Expression = Expression '*' => replace({parse Z' = $Expression Z'; parse Z' = ; Z'})


Because we make the rule under a scope, the rules are not actually referencable?

----------------
Rename non-terminal, since when we're in the parser we can reference token rules by name, but that's
technically a terminal, not a non-terminal

How about just CharacterRange, CharacterSequence (for 'aaa'), Reference, Production

---------------
one of the earlier primitives we need to introduce for tower is error reporting
 - basically just the error stack like we had in zero for attaching context information (and eventually a real stack)
 - can be a very simple push/pop api, with user_data and a callback
 - or a simple string version too (maybe a string format/interpolant version?)

-------------
eventually when we add callbacks, we will want those to be part of the Rule component I think
 - just a simple function pointer callback that can be set on the component

For the virtual stream, we want to have it return tokens
 - but we also want to actually consume the TowerNode out of it
 - A gross way of doing this would be to make every character into a tower node, and consume it that way
 - e.g. the parser only consumes tower nodes, and the root of the node can have the value on it
 - It's sort of nice because it's generic, we really just consume components
 - but that's SO heavy, in a way we should be able to do all this through interfaces
 - I do like however that we can run our parser on anything, as long as it has a numeric value

Maybe we should have the concept of an INLINE rule?
 - meaning all calls to that rule act as if they are inlined, would be nice for token speed possibly
 - token inline A = 'a';
 - obviously, inline rules can't be recursive

we can just have an implementation for utf8 characters and for tower nodes
 - Hmm, well, we're already going to be creating tower nodes for parsing of characters
 - remember the CharacterRange and String cases
 - but tbh, for most of it, we really don't want to create parse nodes
 - like for identifier, we just want one single node, not like 20 when we have aaaaaaaaaaaaaaaaaaaa
 - I think just having the interface return two things, a numeric value for the token
 - And then also a tower node if we have One
   - we can attach that

--------------
When it comes to the parser reading from the tokenizer, I think we need a way to resolve references
 - or do we really process token and parse rules together?

Hmm, I think the primary idea is that I actually think I should create two separate parsers with separate state
 - I think we just enforce the range rule via the parser, not by actually saying tokenizer or whatever
 - keep the parser generic
 - basically saying that we should separate out the token and parse rules into different parent nodes
 - I mean, that's pretty easy from the perspective of our parser running wasm
   - when a token/parse rule completes, we look at the keyword and add it to a child tower node keyed on token/parse

So does the parser even know about parse phases?
 - nope!
 - I like that, nice and generic!

------------
so... when we have sub-rules, I know we said when they are in thier own scope they can resolve by themselves (e.g. Z')
 - But now if we're saying we separate token and parse rules, then can sub-rules of a parser have token rules?
 - Since sub-rules are almost entirely intended for regex operators, then we can just keep them to being the same type only
 - not exactly what I want in the future, but it works...
 - Either that, or the parser needs a way to "weed out" which rules are for it
   - YUCKERS!
 - We can just generate names I guess

-----------
Note the parser isn't just a parser, we need to separate it into the table portion
 - technically as a good API, we should be able to run the table with as many simultaneously active parsers as we want
   (all from the same parse table of course)
 - So parse tables really need to be their own thing
 - We can just call it that, however it's implemented internally

And then we have the actual parser, but I don't want to just call it parser
 - what can we call it... it keeps track of shifts/reduces and the grammar symbol stack
 - runner... lol
 - parser_table
 - parser_parser...
 - parser_runner, I hate it but it's the only thing I can come up with

for the parse table, we don't actually need to hold on to any data related to the construction of the parse table
 - we just need to resolve symbols on construction and then we're done with the resolve function

-----------------

ok dumb discussion on if we should just have streams return tower nodes with a match component on them
 - the "rule" value would be the read in utf8 value, which all works
   - side note maybe we rename that rule, that's like eh, it's more like just 'value'
   - and then we can mention, cannonically for the tokenizer it's a character value, for the parser it's a token's rule index
 - it's really just allocating a tower node and a match component, two allocations per
   - that's probably heavy for an embedded system
 - but also, we can always put these into pooled allocators for fast allocation and locality
   - plus those work almost like a stack if you allocate one temporarily and release it
 - and even though we're making one per character, we're also releasing it almost immediately
   - is that a sign that we're also being stupid here... using a whole heap object
   - not to mention the parser has to look up the component (or we return the Match* component instead?)
 - I also don't want to give any intention that these character nodes are going to be held on to
   - so in reality, I don't want to keep them, because who is going to decide to get rid of them?
   - I don't want to have to toss them every single time
   - it's too much of a waste to keep them per character, so that right there tells me no
 - Ok, so maybe I just need to output to a match (and we can make that one on the stack...)

----------------------
ok another issue, in the parser phase we said that any String literals / String components
would actually indicate a token, basically we can use them as keywords
these would need to register with the tokenizer
 - or, maybe we have a "keyword recognizer" that goes in between, or is even part of the parser
 - we could try to do this by bytes in a string, but another way we could do it is by code points,
   basically a u32 string that is the result of the decoding
 - one issue, I don't really want to keep those around or allocate space for them
   - it's ok if we have a temporary small buffer with reserved space and every time we complete a token
     we run a runtime check over that token
   - maybe that's something we can pass in to the 

I think it's fine if the parser holds on to these strings and gives them ids
 - how do we give them ids that are distinguished from the tokenizer?
 - maybe the recognizer can be registered with a "keyword callback"
   - at the end of recognizing and completing a rule, we invoke the keyword callback with the shifted inputs
     - keep track of the code points on a stack (only if keyword recognition is registered)
 - it invokes the callback, passing the last index of the rules defined (so you know any index above that is free)
 - perform recognition, and if we find it (can just do simple hash map for now)
   - add the last token rule index plus the keyword index to make a whole new id
 - SO EFFICIENT lol
 - I mean not as efficient as just having precedence between keywords and identifiers...
   - hmmm... actually? I mean to be fair maybe that makes a lot of extra nodes...
   - I'll have to read on that again, I remember the book just recommends hash lookup or whatever
   - but really could just be a DFA lol
 - whatevs, I like the keyword callback solution
---------------

next task, implement the recognizer stream
parser_stream_recognizer_create
 - I think this will make sure the recognizer interface is going to work properly
 - after the recognizer stream and maybe some tests, down to get to the algorithm
 - parser_table_create will become the main focus
 - also should extract the unicode read function, and make it length based instead
   - and maybe the char* stream has a strlen stored so we know not to read past
 - then we can test that unicode function by itself too
 - should also put debug markers for all this stuff, maybe have a debug mode
----------------------------

odd idea, is it a weird concept to "try something" and rewind?
 - kind of comes back to the determinism thing
 - it reminds me of that idea that I had for everything being reversable
 - but back to try and rewind, it was the idea of save-stating wasm and running it (modifies memory, etc)
 - I think it was just some dumb thing that popped into my head about like seeing if something is viable
   - like run an algorithm with determansitic inputs and pick the first that succeeds

----------------------------
should we make it an error to overwrite a member of the same name?
 - I mean not really, that's kind of just JS style...
 - I guess my thought would be, is it uncommon to overwrite a member in our AST
 - it is sort of annoying having to know beforehand
   - I guess that's where we should pass the flag, is it an error or overwrite

technically a few modes
 - overwrite (whether there or not, always write)
 - only write if it's there
   - exception/fail
   - return some indicator that it didn't add
   - return some specifiable default (also supports the above if you pass nullptr)
   - return the object that is currently there in place
 - only write if it's not there
   - same as the above options for errors


since components aren't moved between objects (yet)
 - and they are always constructed with an owner
 - then we sort of have another option
 - do we overwrite, or return the one that's there?
   - can kind of do that with tower nodes too, but it sucks because you need a whole object

I think we can kind of say we really just want has_or_add behavior
 - and ultimately you can implement everything with just get_component/has
 - you can check if it's there beforehand and throw an error
 - if I ever end up supporting remove, you could remove it and re-add it
 - so yeah I think for now it's just has or add behavior

probably should do the same for components then

so this really only affects object parenting, otherwise components will just get the one that exists


---------------------------------
random ideas again
given a wasm, prove X
LLM combined
say something like "prove that this can never return null"
 - I wonder if this is where I can use the llm, for the compiler
 - it's like you ask it to determansitcially compile a function that has X inputs and Y outputs
 - we can test the function the llm generates
 - must be a pure function, or maybe this is where memory is reversable lol
 - no, must be pure, I think it would be pretty bad to affect things

// generates a function in the current context, with the includes and grammar rules we have access too
// ideally we can run within a sandbox to generate this pure function, something to think about!
// basically with tower we always want to be able to eval
// we should make a feature in tower where we can specify local test blocks for a function
// especially useful for pure functions
// basically we can detect the tests that are for a specific function and evaluate those tests
// and for the llm, they must be true otherwise it needs to keep trying
let f = llm(a: i32, b: array(i32)): i32 {
  pick the first value in b that's higher than a
}
test {
  assert(this(5, [1,9,3]) == 9);
  assert(this(-100, [0,1,2]) == 0);
  assert(this(100, [0,1,2]) == 100);
}

ok that's neat and all, but I guess what I was weirdly wondering is if there's any way to prove with an llm
 - it's really just like, imagine we ask an llm to do something
 - and then we can verify if it did the thing
 - if not, we provide feedback, try again
 - this is a process we can repeat until it solves something
 - and we can verify it's solved, at least for what we asked it

but I have some bug in my head telling me:
 - it's like imagine asking it to prove that every element:
 - the ask is basically a predicate, it's like

prove that after sort(a)

for (let i = 1; i < a.length; ++i) {
  assert(a[i] > a[i - 1]);
}

and then what does a formal proof look like
 - and now that we have this code concept, what can we do with that?
 - as a human it's easier for me to infer something...

but is there like a really cool llm proving way to ask a compiler to do something

like now can I go in the type system:

a[1] > a[0] (and now it's always true?)

because what I attached to it was a code truth...

ah this is so bonkers and weird, what am I doing, I'm out of my league lol
lets think about the proof for a second
I mainly am just thinking, is there any sound language or data structure that can "prove" something is true

obviously we could run all possible values of "a", but that's also impossible and unrealistic
 - that would effectively prove that for every input, this function will produce the desired result
 - and an exhaustive proof like that is valid
 - so then the next question becomes, can we start to cut corners
 - rather than us providing test inputs, can we get some formal guarnatees here
 - what would the first step we could take to proving this
 - just a basic step, start with all default arguments
   - ideally for a pure function we can build any structure, just imagine it's all pod right now
   - copy in copy out, pure
   - or even imagine that you have to hand it a memory block, like everything is serialized
 - so we generate defaults, call it for the first time, run the asserty code on it
 - verify that it doesn't fail/assert
 - if we have a single counter example, we've failed the proof
 - the llm needs to generate examples based on what it sees in the code
 - complete code coverage
 - we want the llm to be able to see every choice that was made while running
   - so we can examine branches
   - indices too
   - base case and recursive case
 - another question to solve would be like, are these two exactly equivalent?
 - because the idea would be we could separate out things into base cases and verify that the base+recursion
   is equivalent, and then move onto proving that one alone

ok next dumb question, what if a type was defined by how it was used instead

just like, if a type is defined by a proof rather than it's definition?
 - the weird thought came to me because technically a type system offers a form of proof or guarantee
 - like a function call that always returns an object and never null, and takes some inputs is like
   saying that no matter what for any input, we will always have an output
 - provided no exceptions, aborts, etc (again, talking pure here)
 - so in that way, it's a form of proof

so what I'm talking about is a weird proof based language

the definition of sort is like:

// this is a pure function, all copy in/out
proven sort(a: array(i32)): array(i32) {
  // prove that we have the same amount of elements
  prove(return.length = a.length);
  // prove that we have exactly the same elements from before
  for (let i = 0; i < return.length; ++i) {
    a.contains(return[i]);
  }
  for (let i = 1; i < return.length; ++i) {
    prove(return[i] > return[i - 1]);
  }
}

or like, for saying a struct needs members

let a;
prove(a.lives = 9);

things don't declare what they do, they declare what's true after they're done
and it's like part of the type system, now it's true for that thing
 - and we just allow an AI to determine the middle parts

I really just like the idea that a proof could be code

--------------------------------

going back to the parser I had this keywording idea
but I kinda forgot that the parser might just need symbols too like "{" "}"
 - it's not just kewyording, it's literally adding new rules
 - I wonder if when the parse encounters a string, we should like find and add
   a token by the same name, and then replace the parser node with just a reference...

I guess it's odd since I'm doing a lot of tree manipulation, and the parser table builder
really just expects one format, so I'm just prepping the tree to be in that format
 - but a lot of me is wondering whether I should be prepping the tree
 - or if the tree should just be as is, and I hand the table builder some sort of
   predicate/stream for walking the tree
 - like an adapter for a component tree... ehhhh
 - I guess that's where the custom magic can come in for the marrying the tokenizer and parser

is that taking it too far?
 - I mean... parse tree adaptors / transformers should be a thing

virtual children... huh...

-------------------------
but that said, altering the tree is supposed to be something we do
 - that's like a big part of tower, so lets not be afraid of it
 - there's no need to preserve the original parse tree as long as we can preserve
   the information in the transformations, and make errors appear like a normal language would
 (replacements need to be traced!)

A String component should probably actually take a Stream, and we give it a utf8 stream
 - can do the same thing in the parser
 - but when we complete the String rule, if we're under a parse rule, then we swap ourselves with a reference 
   and re-attach ourselves to the token grammar with a new name

-----------------
another side note, I also love that instead of a bound type like in zilch, it's really just a component tree
 - so now when we say any compiletime evaluated function that returns a "type", we really just mean anything that
   fits the definintion of a type as per a component tree (maybe has components that implement an interface)
 - so if we want to build a "bound type" we really just output a type that probably uses the tree as it's interface
   - like there's an implementation of type that reads the type info entirely from the tower node tree
   - so now we just attached named members, etc to an object
   - and suddenly we can have types
 - I love this way of building a type, it's so natural
 - we still need to understand dependencies though

-------------------------
if we want to do has or add, then component create should not have to pass in the size
 - ideally it would be part of the type...
 - A size component at least, right?
 - I mean, that's not a bad change...
 - it's like the beginning of us defining what a type is
 - but then how far do we take it, construction? etc?
 - hmm
 - I guess for right now, if you do has or add for a type you guarantee your call matches the size

-----------------
In the tokenizer's case, it runs the recognizer until a single token is completed
 - in the parser's case, it will usually return the entire root node
 - but maybe it returns any root node, as we will execute them

the next thing we need to figure out is how recognizer steps work
 - needs to return more than just a pointer
 - in the case of tokenizers, we return any node that would be attached to the root

we can run the recognizer manually step by step
but really we just want to run it until something changes
 - that may means we either get a tower node (for tokenizers)
 - or we executed some callback (like wasm) and tasks/dependencies may have changed

as a side note, the dependency check is just about like an interface cast in tower for components
 - we just say something like, don't run until this cast is true
   - we can even use constants, etc
 - it is really just arbitrary code that we execute to see if we run the dependency
 - it's not my favorite, but if it's organized well it might be ok
 - I think that way to start is ok
 - tasks can hold on to references to tower nodes, like a closure
 - then they run some condition against that node (may traverse it, etc)
   - the condition is just code, and ideally we want to say it's pure
 - the user could really just write a condition before, but I think we want to make
   it more obvious how your task should work (readonly/pure condition first)
 - so a task takes an array of nodes to hold on to, correct?
   - a condition to check upon any change

in the future, to optimize our conditions can have triggers
 - things that must be true, but we can listen on easily too
 - like we might say this node must have a "left" child
   - we can add callbacks/event listeners for adding children
   - and when a child is added, we check the name
   - if it matches, we mark the trigger to true and move it to an active list
   - then the full condition gets checked
   - it is left in that list until the trigger (or any of the triggers) become false
     - for example if the "left" member is removed, then the trigger is removed
 - we attempt to optimize as many conditions as possible with triggers
   - but ultimately there may be conditions that are just easier to write without triggers
 - the important idea is that triggers are true/false storage too
 - that means that triggers also have to be true for the condition to even run (aka part of the condition)
 - we can chain triggers
 - types of triggers
   - trigger on a child by index
   - trigger on a child 

------------
ok so even without triggers, we can just blindly run conditions in a row
 - but we do still hold on to the tower node(s)
 - maybe that's how we start doing triggers, or conditions?
 - we can make it so that we can add conditions...
 - each condition may optionally take a node, this is how we refer to more than one
 - does that always work? I think so
 - or we just separately keep track of an array of tower nodes
 - and then register whatever condition(s) we want

the recognizer needs to return:
 - to continue the parse, but no tower node
 - to continue the parse, with a tower node
 - to end the parse, no tower node
 - maybe those are just mutually exclusive
 - we just return a value that is whether we keep going

----------------
unless we have a 'do events' style loop for tasks
 - we're going to have a problem with tokens

 - otherwise the streams need a way to also return 'continue'
 - basically that they did some work, but it's not ready yet (they did a unit of work)

hmm, do we do this?
 - at least we know the inner tokenizer recognizer doesn't return until it's done
 - I guess we just bubble the 'running' boolean out of streams too

- I dont want to deal with re-entrant problems in the recognizer or table
  - calling step within step...
------------------
you use english to specify what you want
 - then it uses inputs, available APIs, etc to write a script with no side effects
 - no matter what, we always run the asserts
 - technically slower, but even if we come up with a proper algorithm it may not work in all cases
 - so this techniuqe is self correcting

ai add(a: i32, b: i32): i32 {
  // comments, everything is eaten!
  do whatever we want here, it's just a suggestion
} test {
  assert(self(1, 1) == 2);
}

once it finds a viable version of the function, it will compile it and cache it
 - as you run more, a better version may be discovered
 - eventually when we compile to release mode, we remove the checks and the llm

could the function be not pure if we did the whole wasm backup thing...
 - I think the issue with not pure, it's more just that llm writing in isolation is baaad


why this could be considered part of the language:
 - first, it uses the current grammar/context to write the code (we need to write the token selection algorithm)
 - in order to generate code, we can't rely on pre-training for everything
 - so when we ask AI to gen a function, we must walk all parameter types and return and format them
   as text to give to the llm (all the context)
 - whatever the llm generates, we must run and test
 - upon any compile errors or hitting any asserts/exceptions, we must give the llm feedback and try again
 - it relies heavily on our compilation and sandboxing techniques too
 - we can pass all sorts of ai options into the llm (model, heat, seed, retries, prompt framework, etc)
 - also need to create an agent for function / class / type selection
 - also ensure it doesn't get stuck in cycles, and do as much as we can to help the convergence to be faster
 - we can also record previous implementations and ideally ensure we don't do the same one

struct Sphere {
  radius: f32;
  pos: vec3;
}

struct Aabb {
  center: vec3;
  extentds: vec3;
}

struct Collision {
  point: vec3;
  normal: vec3;
  depth: f32;
}

ai(opts) collide(a: Sphere, b: Aabb): Collision | null {
  compute the collision between the sphere and AABB
}


we can have it generate non-pure functions if the generator runs entirely in a sandbox
 - e.g., we can pass in tower nodes, etc, and have it correctly generate them

I also like the idea we can sort of do the same thing for structs... maybe?
 - I guess it's odd here, because this is a hidden case
 - We're basically saying, hey we know nothing about the implementation, but we know exactly how the type works
   (parameters, and return)
 - does it ever make sense to do the same for structs? I guess not really
 - because ultimately that's changing my usage
   - we're really just allowing AI to use anything

Maybe this is the idea where we can create a network of hidden structures
 - Basically, if all it ever returns is a Thing*, we never know the implementation
 - We're just declaring how we want to work with it, and what it should produce
 - but it may opt to change the internals of the struct based on the functions

I think they should in general all be solved as a group
 - otherwise we run into the same weird re-compiling scenario
 - like it's easy to exchange a function, but structs have actual allocations in memory
 - well it's the same problem we already have, if we modify a struct need to recompile all the affected places
   - and re-build anything in memory, or require a shutdown

for now that just means we need to solve it all together

based on the performance of the demo I just tried, LLMs still have a ways to go
---------------------------------
ok back to the language then, what's next:
 - we need to start creating then table and the recognizer now

-----------
I could use a hash set, but I want to preserve order in some way
 - insertion order would be ideal
 - but we can also just do sorted order for now
 - a quick insertion order way would be a vec + unordered_set
 - is that worse than just doing a set, we just want determanism really

One problem right now is that we use tower node indices as grammar symbols
 - but for String, it's actually a bunch of symbols

I think that means we should translate our structure into a simpler internal form
we can parse String into it's respective codepoint values
 - or alternatively, do the "reference" lookup for String
 - that could either handle breaking string into individual codepoints
 - we should reserve a bit on uint32_t (check unicode for one we can use)
   - the bit will indicate if it's a codepoint or a rule reference
 - basically an array of arrays, very fast

I think at the end of the day, String should not be something we try and directly consume
 - it always has to be processed, in both cases

---------------------
When we point 

Right now the rules come in any order

We need a two way data structure, one that is indexed by productions
 - as well as one that we can refer to the rule non-terminal (meaning all the productions for the same A)
 - so I think the easiest way to do this is just to put all the rules in sorted order

String serves a dual purpose
 - in tokenization, it's a literal array of concatenated characters
 - this saves us on allocations of tons of nodes (Range nodes)
 - it's also just really convenient for anyone writing a parser to just pass a utf8 string
   - or even ascii string (cause that's utf8)

In the parser, it's really just a new rule added to tokenization (like a keyword)
 - so what's the reason we don't just require the parse rules to handle this
 - at the end of the day this is a specific detail of our parser language

So maybe what we do is just populate the String component from a Stream (or something like that)
 - I think the primary danger here is that you can use it really
   stupidly if you accidentally place that component into the parser
 - But then again, that's a concern everywhere, like using the wrong values in Range

Ok so we just don't use String components in parsers!
 - That means that when we finish parsing a new rule, and we see it's a "parse" token
 - we then need to walk the tree and replace any String
 - or, alternatively we place a task on the node and the dependency waits until it's connected to the node
   that says whether we're a "parse" or "token"
 - we're bottom up, so we know we have children before we're even connected to the root (backwards yo!)

So then what the String component really should be is just a bunch of uint32_t's in a row
 - we can have convenience functions to populate them from a stream, or just set their value

--------------------------
I think we should probably stop using std::set and implement our own basic order preserving set
I think as long as we keep it to only functions we use, only for LRItem
Basically just a vector + unordered_map, and we're not really worried about removing elements ever
 - is that a little heavy?
 - I mean it's probably fastest

------------------

since we're using ranges, it's getting a little bit confusing
 - specifically with goto
 - I get the feeling that goto needs to return how much of the range it consumed?
   - like an upper bound that cut the range?
   - I don't know it's weird :/
 - because they should not be considered the same
 - it's almost like what we need to do is a query first to see what sub-ranges would make it through
 - I think the thing I really need to do is construct an example grammar and walk through it by hand
 - run closures and gotos myself to understand what the final result should look like
 - can simply just do [a-b] as a range, and also see what it's like if we do:
   X = a
   X = b

I bet that will tell us the exact thing we need to know


Side note, I love the idea that I had in welp for comparing two sorted sets
 - that kinda changes my idea for unordered_map/array (sort of)
 - althought we don't actually need to sort the until we compare them

-----------------
ok so another side idea that keeps popping into my head is that struct layouts are kind of like grammars
 - and that technically any qualified sequence that matches could be considered
 - and pointer types and such could be described via this grammar (if they had unique ids)
 - the token stream is like asking what is the token value for
   - pointers would almost be like, instead of diving in and shifting those values, we just shift on a non-terminal/rule

so a simple example might be that you're trying to cast a void pointer into a pointer of a type
 - and we need ot see if the memory is laid out correctly
 - we're basically going to do a broadphase

lets just think about this now because it's fun, my brain is too fried to get into tree sitter

so there are kind of two conflicting constructs
one is that we think of things as struct values, (floats, ints, pointers, u8s, etc)
 - all these values can have different sizes too (f32, i64, etc)
 - in general it would make sense to constrain data to be correct and align

- so I think some of the primary operations that we must solve is whether a pointer can point at a spot
  - which is basically also like 'reinterpret_cast'

my claim is that we can do a full sweep in linear time with the number of bytes
 - we do say that pointer manipulation is expensive in this language

ok, so linear number of bytes should be my minimal requirement, it can't take any more than that
 - but there may be some ways to validate it quicker than linear

we have two questions to answer:
1. What is the current memory layout at position N, length L
2. Is that memory layout compatable with X

Answering #1 is actually not trivial
 - if you asked it right at the head of the memory allocation for an array allocation type
   then it would easily know it's of type A (whatever the array type was of the first element pushed)
 - in that case, it's like A is a non-terminal (it's a layout that's already been fully described with children)
   it's kind of like accepting an existing parse tree
  
Lets take the trivial case first where #1 is answered, and lets focus on #2
 - so the case of the memory having exactly A at the base position 0 and then asking for A would complete instantly
 - but now imagine we asked for if the memory at A was compatable with memory B
 - if we imagine these all as parse rules that describe what kind of memory is valid

struct healer {
  health: f32;
};

struct enemy {
  lives: i32;
  wizard: healer* | null;
};

struct player {
  life_count: i32;
  health: f32*;
};

let e = enemy{
  lives: 9,
  wizard: null;
};

let enemy_ptr = &e;

// this cast is going to be checked
let player_ptr = enemy_ptr as player*;


that's basically saying hey take this current initialized enemy_ptr with layout enemy


so the question is how do I quickly know, or even just know, that B and A are compatable

since we know about the cast from enemy to player, we can generate the check at compile time

ok, well the first question to even answer period is if they are compatable at compile time
 - we actually don't even care yet if the runtime is somehow compatable
 - I think, right? maybe we do, but the thing we need to answer first is just knowing two layouts A, B

 - the compile time check would save a runtime check I think
   - because we know we had a memory compatable value
 - but obviously it has to do runtime checks if the types are bigger
   - or if you're doing a void* casted to anything

I think the effective idea would be to linearize the layout, but still use symbols
kind of like, if we normalied it
 - then it might make those checks a lot easier
 - ok yeah so first thing we do is 

Hmm, wait, so in this case if this was a grammar, what we're
actually saying is that safe casts are actually ambiguous rules
e.g. two structs have different definitions but can both fit the same memory, ambiguous!
 - but here we're doing it as if we put both A | B into a grammar, and then we expected ambiguity
 - well actually that's one way to see if they are compatable, if they are ambiguous
   - if that's easy to detect that specific case, then yeah that's a good way to check
   - we can also test struct compat offline, or do it at runtime and cache it between known types

Ok so that's the simple case of A as B

but now lets imagine memory was more complicated, it was an A with a C right after it
 - This is going to happen all the time when you point into the guts of a struct
 - I imagine solving part #1 mentioned above will involve querying a tree with a range
   - if any part of the range cuts a type that must be whole, its an error (like a pointer)
 - so #1 may return back AC (it could return back every single byte, but lets imagine A and C are huge)

but lets say we wanted to answer these queries in as minimal as possible time


side note, I also don't want users to just be able to grab a pointer to the internals of a container
and just start jacking with it, so we may need to answer privacy/security questions here too


it's like a little more complicated than just pod data

we just want to prevent corruption and non-determanism / undefined behavior
 - beyond that, who cares!


I think it never makes sense to just do the interface cast with the data thats "there right now"
 - because that's always going to be out of date if anything changes
 - but maybe we can do it where it either returns a full copy of the data
 - or it returns an interface where everything is checked

struct player {
  target: enemy | object;
}

struct foo {
  target: enemy;
}

let p = player {...};

&p as foo*; // this would not work currently because (enemy | object) has a header

but in typescript it works, because everything looks the same in memory (bloated!)

so now I'm wondering, can we do something with layouts instead then
 - like every decision point needs to know...
 - it needs to be in memory, but technically not 


A = 

side note, maybe we could support an interface definition that still lists all the types it could be but selects one of them

struct player {
  target: enemy | object;
}

struct foo {
  target: @enemy | object;
}

ok, that's just something we do to make it so we can acquire direct pointers to things
 - hmm, but there's the problem again, we're pointing at it with a specific type of pointer

hmm, the whole | thing hits one dilemma, if we're claiming that we know how memory is initialized
 - but maybe we just know it could be one or the other
 - how do we ever safely point at target?
 - I think this kind of needs to be a feature of "memory"


 - any type that can change it's initialization kind requires an intrusive list too
   - because how can we safely point at the sub-type


we're really just saying there are multiple type configurations

type configurations are interesting, they're basically a way of saying this type can occupy tons of forms
 - and we know which one based on some runtime information

for a flat data structure that only has one form
 - the advantage is that we know we only need one intrusively linked list head
 - otherwise we effectively need a pointer per byte
 - we're basically just defining it as "change points" in memory
   - any place that can be pointed at and changed
 - kind of just maintaining a broadphase of what memory is initialized where

hmmm, well if I'm thinking of this more like C, and this is a replacement for unions
 - then this wouldn't add any byte for what kind of data it holds
 - it would be exactly like a union
 - however, our memory checker system would independently verify that you're pointing to it correctly
 - unions use members to distinguish the type from each other

union enemy_or_object {
  enemy emember;
  object omember;
}

struct player {
  target: enemy_or_object;
}

struct foo {
  target: enemy;
}

let p = player { target: enemy{} }

let f: foo* = &p as foo*; technically this could work (foo might have less bytes than player, but can still point at it)

but the problem would be if we then did

p.target = object{}; // note that this changes layout in memory and invalidates the pointers

and now we changed it, what becomes of f

f must be nulled, or it's an error
 - pretty much the same as deleting memory


so weirdly in this case...
 - the f pointer is affected by ANY change to any fixed part of a definition of an object

it's because the intrusive list is for a range of memory, like a lock almost

we're also saying that pointer manipulation as well as initialization in memory can be spensive!

lets take a step back, I think the array case optimization is really cool, but I want to think
of a fast but generic structure that we can tolerate the performance of

we're claiming that because [un]initialization can happen on any memory, we may need to invalidate
pointers pointing to that region

this entire structure could be stored within the memory block itself
 - and the worst case scenario could be that it allocates more to keep track (every byte has a pointer)

but again, the cool thing is that we know we only need the intrusive lists / layouts at decision points
 - so if I wanted a mutable array of bytes that I can point anywhere into, I allocate a fixed byte[]
 - now that memory is all accounted for, it's flat with one possibly layout structure
 - and we can do anything within that region
 - can we uninitialize a sesction of bytes?
   - not without invalidating pointers, but yes we could
 - it's something like, every allocation allocates 150% of its size, the extra 50% (with some minimum amount)
   is used for the broadphase
 - and we need to be able to query that broadphase for basically like grammar tokens
   - I want it to be super fast to check if it's the layout matches

hmmm, technically pointer invalidation should only happen if it doesn't match the new layout

so we almost need a broadphase for the pointers pointing instead

like, we need to be able to query the broadphase for initiailization data when a pointer is starting to point in an area

but then we also need to query the broadphase of pointers when we change any initialization
 - which pointers cover which regions
 - and now that we know, do they now match the new layout after initialization

it's almost like we'd want to use the tree sitter kind of approach here
 - we changed memory, layout, which pointers are affected, how is it reparsed
 - but the only difference here, we know what we're changing it to exactly (as in like an existing parse rule)

A = aaaa
B = bbbb
C = aaaabbbb

AB as C

That is the primary problem, we actually know what we have is AB (simpler than going byte by byte)


We could test if AB | C is ambiguous
 - but that doesn't exactly tell us that it's wholly ambiguous

of course another idea is that we could cache it, so like do a linear scan
 - and now we know (given flat layouts) that AB is C
 - If A has multiple definitions, it's like we pick one specific production
   - but it's not just one specific production, it's like a specific parse tree

Or maybe we really just have to maintain the tree like tree sitter
 - and when we say that we modify memory, all interpretations of that same memory must be re-parsed

 - and then maybe we just make some specific optimizations, like if it's the same layout type, it's instant

we're basically saying that a bunch of people are pointing at the same memory, with different structural definitions
that may be compatable

f1f2f3f4i1i2

wait, some kind of crazy issues, if we have to traverse safety through pointers, then we
also need to walk the pointers and register "fixed ranges" of memory holds



struct player {
  target: enemy* | object*;
}

struct foo {
  target: enemy*;
}

let p = player { target: ...an enemy pointer }

let f: foo* = &p as foo*;

// whatever, I'm just giving an example
uninitialize(p.target);

I mean, technically foo is holding a pointer to enemy so it's all good
this would throw on the internal pointer inside, pointers are kinda nice like that

problem: in this case foo* is putting a restriction on what type target can be
 - that means we want from player having this sort of "empty hole" in the middle of it leaving it "undecided"
   - I should not say undecided, it's very specifically up to the user
   - and it specifically should always be initialized to one or the other, as in the type declares it will initialize it
   - so it's not like we can just change that memory tho, it's not a hold, it's still accounted for
   - I guess the thing is we guarantee that with the initialization of player, we fully initialize the area
   - like when we say we're initializing memory, we pass a region that we're initializing (I don't think it can have holes)
     - or that would be two different calls, lets think on that for right now
   - 

struct player {
  target: enemy | object;
}




again, the argument should be that you shouldn't be holding pointers to things that are changing

side note, can pointers actually not be fat?
 - e.g. can we get away without storing the links on the pointer itself
 - the pointer always knows what memory it's pointing to
 - yeah but this avoids allocation, if we did it that style, we need reserve space in the memory...

------
should probably just look directly into what tree sitter is doing
 - I really do like the idea that it's fast enough to act on a keystroke
 - although we'll be doing a lot more than tree sitter would be doing just by itself
 - so probably not as realistic as an expectation, but you know if we profile it a lot lol

still, I can see a game engine with tower embedded, lots of parse rules added and restrictions made to keep it simple
 - custom parse rules to describe game features, components, etc

embedded and running a wasm sandbox with exposed calls to the game engine
 - reflection library for C/C++/Rust

editor has a code window and as you type, the minimal rebuild occurs live
 - the compiler / runtime is sort of meant to be run live together
 - yes you can just run the compiler until it spits out a wasm

with that level of minimal rebuild, we could better support object patching
 - we could possibly even rebuild the stack
 - can even support serialization, updates possibly

can we just use tree-sitter?
 - might as well just take a look at the api
---------------------

I think for the reinterpretation of memory, a pattern matcher is the best idea
 - can just basically be a dfa
 - I think my only remaining question is if I have to traverse pointers, and if there's like a graph issue here



struct C2 {
  c: i32;
}

struct B2 {
  b: i32;
  c: C2*;
}

struct A2 {
  b: B2*;
}

struct C1 {
  c: i32;
}

struct B1 {
  b: i32;
  c: C1*;
}

struct A1 {
  b: B1*;
}

let foo1 = A1{...};

let foo2: A2* = &foo1 as A2*;

// In this trivial case, the definitions of A1 and A2 should have been reduced to the same


struct C2 {
  c: i32*;
}

struct B2 {
  b: i32;
  c: C2*;
}

struct A2 {
  b: B2*;
}

struct C1 {
  c: i32;
}

struct B1 {
  b: i32;
  c: C1*;
}

struct A1 {
  b: B1*;
}

let foo1 = A1{...};

// Whatever we decide here, we're basically saying we have an unknown pointer
let v: void* = &foo1;

// From an unknown pointer there is no way to do a compile check, so at runtime we must look at the memory
// pointer to by v and see if we're allowed to even point there with A2* (is it initialized?)
let foo2: A2* = v as A2*;

In this case, A1 cannot be reduced to B1
 - so we must perform a recursive check
 - And technically if that void* came from anywhere, we have no way to compiletime deduce any type

One thing we could compute is if two types were overlayable, even if not exactly the same
 - That's sort of N^2 though, for every type check if it's compatable with every other type

But I think we could argue "amortized" time here, since it will cache it once it's done
 - I could see how you could setup a worse case scenario here though
 - and you could argue that even if it's N^2 in the number of types, it's fixed

But I think that it does have to be a recursive approach
 - note that when we recurse, if we find the same type again, it's all clear

Right, if we do anything to invalidate the memory, it invalidates the pointer, so all good
 - or the pointers revalidate, and then invalidate if they don't match (or panic, error, etc)

so... because we're storing a pointer over a region, doesn't that already mean we need more than just an intrusive pointer
 - or maybe that's what we do, we walk the pointers to see what region they hold...

can we ensure that intrusive pointers are sorted... does that help?
 - we can't randomly index into them so kinda no

question, if we've already reduced types down to their shared equivalents, how could A1 ever be compatable with B1

struct A1 {
  a1: i32;
  a2: B*;
  a3: i32;
}

struct A2 {
  a2: B*;
  a3: i32;
}

locking is basically all about decision points, which also have pointers
 - but the issue potentially comes from interpreting two consecutive values as one
 - e.g.

let v = malloc(sizeof(B*) + sizeof(i32));
B* (v) {};
i32 (v + sizeof(B*)) {};

let a2 = v as A2*; // legal, but now there are two decision points
 - so if we attempt to point over multiple decision points, what happens
 - well technically now both memory needs to be responsible for invalidating the pointer
 - almost saying that the pointer needs to appear in two intrusive lists

so it's really all about these decision points
 - and yeah the pointer may need to be in both, that's... PROBLEMATIC!
 - gotta really

when a pointer could exist in a single decision point only, it made sense

another memory model that could help here is actually the multi-reader single writer of rust
 - because we can ensure that we have zero readers pointing to any part of memory
 - and the writer pointer can be typed, but also untyped so it can point at uninitialized data
   - hmm, that does beg the question now, do we need to know if it's typed?
   - I guess that's what the locking region does
 - but we only have one writer in this case, so it's easy to store the region

as far as a pointer spanning two regions / decision points
 - I think this is where we treat this more like a general data structure
 - We -can- just iterate over all pointers and invalidate any pointing at that region
 - we would need to be able to know the pointer's runtime layout
 - so now we need to have a layout pointer as well at runtime?
   - starting to sound pretty heavy


what we need is a broadphase


I really love the array case of structs because without any decision points, it's so easy
 - but the second we add in unions (a | b) it gets complicated
 - technically even with decision points / unions we can still just scan the pointers to see if they are
   - pointing in any re-initialized area (like changing a to b)

maybe what we want is a general broad phase that can work for almost anything
 - and we just allocate an amount of memory reserved (it will re-allocate if it needs more)
 - we can do some sort of run-length encoding structure so that arrays can be efficient

and since the pointer no longer needs the information stored on it, we can store it all in memory
 - I kind of like that too, especially if we put it at the end of memory
 - then we can ensure that this feature can be entirely turned off

but technically the multi-reader scenario would still benefit from an intrusively linked list (no allocations)
 - but in that case, we don't even need the broadphase or whatever

so now pointer sizes can be different
 - and it's not exactly tied to the type either, it's tied to the kind of memory

if we took a max of all pointer sizes like zilch
 - but then the pointer needs to know what kind of pointer it is
 - now all calls virtual, ehhhh

but I suppose that's also something we appreciate, that no matter what we can stuff everything into a "handle"
 - but like, wouldn't we rather that be a generic feature

 - I guess the neat thing about pointers or handles is that getting a pointer directly to read/write is always easy

some handles will require calling something for read/write and dereference

unless I can think of a generic structure

ptr(Player)

shared_ptr(Player)... eh.




I mean, we could go the C# route and somehow make the pointers invisible

I mean, * could also just mean handle for us
 - but again, the issue is that I suddenly can point to the same memory from different types of handles

handle(Player)

 - this actually seems alright

But I do think the original idea was to have pointers just be pointers

it's just that I did like this about zilch, it was nice
 - that memory tracking was decoupled from the actual thing itself

but yeah now suddenly handles need to be virtual

I think if not, then we need some kind of way of denoting what kind of pointer something is
 - and ideally since we can just define new safe types of memory along with handles
 - then we need a syntax for that
 - we could have it so just Player* always means one kind of memory, like the default kind
   - and otherwise you have to say the kind of pointer
   - but see that's the same issue thing though, I don't want to have to define the type of pointer everywhere
   - I guess, is it really that bad... hmm

honestly if we're allowing child libraries to modify their parents (any library can modify anything and cause a recompile)
 - then maybe one approach we can take is looking at all the ways a pointer gets used
 - I'm not super duper in love with this, but it's kind of a neat idea
   - or maybe this is just something we do when building the 'release' mode version of it
 - the idea is basically a type that might need to handle a variety of different situations
   - but if those situations never occur in code, then the structure can change to something lighter
 - I don't think there's really anything like that in most languages
 - it's easy to keep track of every point where something is referenced or used
   - and how it's used too
 - this could even be a first class feature of tower, like a morphing type

so we're almost just saying a handle is like

Handle = Pointer | SharedPointer | MultiReaderSingleWriterPointer ...

And then boiling it down lighter later

at least when it comes to the idea of the memory runtime, I think we can handle it all
 - we can have the concept of just the fat pointers themselves, minimally sized as needed
 - and then we also support the concept of the handle
 - a single size object that fits all pointer types generically and uses a virtualism behavior to determine which
 - moreover, the handle is extensible (anyone can implement a handle using unsafe code)

MultiReaderSingleWriterPointer is actually just a pointer I think
 - since only one can change initialization

Well shoot, maybe we just go with that concept instead?
 - it's a lot simpler, but yeah attempting to push into a vector while someone holds a pointer will throw
 - but then the single writer pointer needs to basically be moved (like the old one nulled out or something)

maybe we just make a library of determanistic memory safe routines
 - and yeah we can define a virtual interface for handle behaviors

hmmmmm

one thought I had was if the last pointer is deleted, we can kind of think of that memory that the pointer occupies
as being "uninitialized"
 - kind of like when we move Something
 - C++ has what is called "non-destructive" move, the destructor is still called on the moved object
 - Rust has a destructive move, the object that is moved can no longer be referenced and is destroyed

I think it's not particularly easy for us to keep track of that
 - rust's compiler and borrow checker has specific tracking and restrictions
 - I was thinking we can do something that lines up with what our language typically does,
   keep track of the uninitialized area of memory, but now we're kinda back to adding a decision point
   (and does that mean every pointer, or specific types of pointers, always add decision points?)
 - Player**
 - I think we just have to support non-destructive move
 - In our case, the pointer is nulled or 0xFFFFFFFed to indicate that it's been deleted (can have a special value)
   - can still assign over it, etc, just can't read/write to it or do any pointer manipulation / casting
   - all those operations will throw
 - 

side note, if we allow uninitialized holes of memory, it might mean that member access gets weird
 - Ideally, member access should be no worse than offsetting a pointer
 - and I think that's something we should always know is quick and safe (since it's like nested)
 - even in the `target: enemy | object` case, we still knew it was initialized
 - although technically the empty space at the end should not be pointed at (if enemy or object was smaller)
 - yeah so, the whole memory is initialized

so then what's this data structure look like ultimately
 - the most common cases should be optimally accounted for

can we have a type that declares an uninitialized region of memory?
 - and basically we know we're going to be allocating in here
 - again, this is a hole
 - but you know what, the hole actually isn't a problem
 - like, if you do a member access to a hole, or even a (enemy | object) it's the same issue
   - whatever you access it as has to be checked
 - so yeah seems like we should be able to declare an uninitialized region of memory
 - and then that region can be initialized to anything it wants
 - side note though, gaps from (enemy | object) are not "uninitialized", they are blocked/reserved!

what is a data layout?
 - pod
 - pointers & system primitive types
 - blocked/reserved (for gap's in data with unions)
 - also unions themselves probably appear there
 - uninitialized - specifically not set to anything

the memory management system should just be told about the existance of pointers
 - and manage everything internally
 - we should only have just raw pointers... (32bit)

maybe that's what we can do to make it all work out, it's always just pointers like C
 - I feel like we still need a way to know where the head of memory is (or where the reserved space is)
 - so pointers may have to be at least two 'pointers'

is there some clever way to know where the head of memory is
 - has to be in constant time

a dual pointer is not a bad way of doing it, especially since that would be dropped in release mode
 - I was just really hoping for a clever way to use only a single pointer, be very C compat but still safe!

I think there's no way to do that period without extra data on the pointer
 - because if we want the pointer to be able to point at whatever it wants

lets think for a second, because maybe we want that

pointer goes to move, we look up what block it's within by asking the memory manager
 - that operation could be say O(log N) (binary search)
 - then we need to find the reserved space for our data structures
 - to keep it in the realm of safe, this would be an operation we do on the memory manager itself
   - basically telling it we're going to do something to this pointer

memory manager comes back at us with whether that's illegal or not, or the new value on success
same with creating a new pointer from an existing pointer
or reinterpreting a pointer

we should ask the memory manager for the memory block
 - and then ask the memory block

when we finish this library, we should publish it
 - with all algorithms detailed, memory layouts and structures detailed (sizes, pointer sizes, etc)
 - and complexities of each of the kind of memory block types
 - as well as limitations

I like this little memory management system, it's safe
 - and at the end of the day, it's very removable

also should think about slices too

because ultimately a slice is just a range (larger locked area)
 - then it shouldn't be too hard to add
 - and moreover, moving any of the pointers inward should be trivial
 - may still need to check layout
 - but that's a point, if we have an array of things, and we advance one whole pointer forward
 - whatever that operation is should be fast (constant time ideally)

------------------
I think the one thing that's really bumming me out is that whole initialized uninitialized thing
 - because it takes a broadphase to invalidate pointers
 - I had a way better argument for "constant time" before this
 - the main issue is pointers, we bascically really just need to know that the pointer is 

in an interesting way, it solves the whole "lock this in this format" issue

I think there's another really odd issue with pointers

struct B1 {
  x: i32 | void*;
}

struct A1 {
  b: B1*;
}

struct B2 {
  z: i32;
}

struct A2 {
  b: B2*;
}

let a1 = A1{... new B1{x: 123}};
let a2 = &a1 as A2*; // now we're looking an A1 through the lense of A2

but that technically means that B1 is being pointed at in 2 ways
 - hmm, does that mean that literally the same exact pointer has two ways it can be invalidated?
 - see I think that makes no sense...

a1.b vs a2.b

a1.b.x = void*; // this would remove the x as i32, and thus invalidate B2

but that makes no sense, since those two pointers are the 

This is because B2 is more specific than the original definition

I think casting something into a definition that's more specific than the original is problematic

I really want you to be able to do almost anything C can do, but that's possibly difficult

maybe that's the restriction, you can't force a pointer to have more than one view of the same thing
 - unless you make a new pointer

ok rule:
 - you cannot force the same pointer to have two different layouts

------------------
I want to go back to another thought, the main thing was that pointers (and internal pointers)
are the only thing we need to avoid pointing at or changing

without change points, we only need intrusive list links at the head

that's really the only important part is that we ensure pointers only point at valid spots

but I think the parts that threw a wrench in that plan were being able to point over multiple initializations
 - as well as being able to point at more specific versions of a type
 - and change points

if we only had vanilla structs and we only allowed them to point at a whole allocated object, not over multiple

but we really just have lots of interpretations of the same memory that need to be checked anytime we change anything

I really want to know if there is some way to abuse quantum instructions to create safe memory

When I move a pointer or reinterpret memory, am I always going to end up doing that linear scan?
 - No, in the event that I point at the exact same layout (constant time)
 - So can I just argue that if you point at exactly the right type, maybe we have a head inlist so to speak
 - but we always have to query anyways...

If memory was literally doubled, or if every byte basically got it's own value
 - but then we have to set all those bytes, but then again that's the same size as what you'd have to initialize anyways

--------

at the end of the day, what's the justification for all this
 - I like coding in C style with components
 - not as steep of a learning curve, very familiar to C family
 - and I like having perfect memory safety guaranteed
 - plus the fact that I can always compile this all out if you use AssumePanic (something that says we assume this is true)
 - you can teach tower as if you're learning C with raw pointers, but it's checked
   (the safety system is effectively removable unless you use it's side-effect features)

I basically need a very compelling argument for why it's all "constant"* time overhead
 - I will make a real argument about amortized, and about choosing the right memory kind
 - as well as arguments about how things generally *should* be used, and why those cases lead to constant time
 - as well as how they could be used esoterically or for the advantage that the safety check also provides (nulling pointers, etc)
 - maybe show some real world C programs and how I transformed them with correct memory choices
   - starts off with the worst case scenarios
 - I do kinda love that I could compile full C programs, especially using the grammar from tree-sitter

that would be a sell right there in a way, hey compile your C programs in tower and get instant memory safety
but it doesn't quite work like that, C doesn't have the concept of "initializing memory"
 - in C it's perfectly legal to just allocate something, immediately interpret it as a struct pointer even tho it's all garbo
 - and then start setting values on it (initializing it yourself) while leaving some values completely uninitiialized

Can our memory model work for C?
 - the main issue would then be following pointers
 - we can't know if the pointer following is going to be 

the only way that would work is if we checked during the re-interpretation of a pointer that it's pointers were safe
 - well, that's an idea I guess...

so one note in tower is that it checks things like double initialize and stuff for you, or accessing uninitialized data
 - C doesn't really care about that at all, there's no "placement new"
 - so for C it's really just "is it initialized yes/no" and is the pointer legal

because basically in C, the memory itself has no layout (unlike tower)
 - interpreting it is what gives it a layout, temporarily, through pointers
 - so since we can't guarantee that pointers stay safe until interpreted, we'd have to check upon interpretation
   - well, it still kinda fits one part of our memory model that a pointer holds a specific interpretation
 - but C also does all kinds of weird stuff (like probably storing bits in a pointer, etc)
   - things we just can't deal with basically

just a side note, memcpy needs to copy all the initialization information, and also handle duplication of pointers, etc
 - it's no longer a trivial pod copy
 - I imagine now if you tried to write that using byte by byte logic in tower, you would hit an error when you tried to copy
   the bytes of a pointer (because all the pod bytes would allow reinterpretation)
 - but the pointer bytes would throw going WAT
 - which, kinda cool, but also an odd limitation for those who know C
 - so now that I think about this, yeah this memory model does not exactly work for C


let me think about C style for a second, if all we care about is whether it's been initialized
 - and then upon interpretation, we have to check
 - but we only need to check the things that are accessed, like if I access a pointer on a struct
 - and then once we have a live pointer
 - and then any time we interpret some memory as a pointer,
   I need to check if that pointer is into valid memory and the size matches
 - is there anything more than that? hmm, is this my simplified hail mary?
 - hmm but there are no object safety guarantees (e.g. you can point anywhere)
 - but it really would just be validating that a pointer is within a region of memory upon use
   - hmm that ultra sucks
   - I want to say I could just resolve it on interpretation
   - but the problem is anyone else could have a pointer to that pointer and manipulate it
   - so on every write, it's invalidated and I have to check it again
   - tower's memory model keeps track of layout/structure, so you can't just write over a pointer
   - the tower memory model makes sure you don't just trounce your memory with memset too!
 - I mean, for C style we could have a fast cache of memory regions, maybe even sorted

maybe one of the operations in the tower memory model can be "pointer accessed"
 - for tower's memory model, we never care about that because pointers are always guaranteed to point correctly
 - but for C style pointers, we need that

I do like the idea that different memory models just care about different parts of how things were accessed
 - I would note that the C style pointers would not be allowed to point within the tower safe memory
   - when we do the pointer resolve, it would result in an error
 - but they can point anywhere within each other
 - and use a runtime cache for fast validation
 - I kinda like this, because even in tower we could utilize this for really fast memory manipulation
   - but you can't put any pointers in there to any place that tower can't track with proper calls
   - we need to ensure all writes

Big Note!: It's only legal to initialize tower pointers in memory locations where you can guarantee all calls to:
 - initialize, move, etc are respected
 - and nobody trounces memory randomly
 - so even the stack is a place that tower has to be respectful of
 - although it should be it's own kind of memory allocation


This can be one of the types of memory blocks we publish, but when the compiler uses the pointer it needs to report it
 - it only just purely guarantees that the memory is safe to access and will report access violations
 - anyone can resolve a pointer, no guarantees of it's correctness or if access was allowed

so tower has a slightly stricter requirement for memory safety, but not that restrictive
 - we can also in the future make weird memory types that leave the bits open on pointers, etc just to fit weird uses)
 - C++ motto is don't pay for what you don't use
 - I think we can mimic that in a way with the AssumePanic, at the end of the day we always have that to lean on

when it comes to memory safety in tower, the point is that we can tell at debug time all the places we screwed up
 - the most common cases should be O(1)

---------------------
thread safety:
 - each memory block will define how it can be used by multiple threads and under what circumstances
 - any globals strickly require mutual exclusion
 - like the multiple reader / single writer can be used by any thread (the values are atomic)
 - so we always know there's only one way to talk to the block
 - most memory blocks can only be accessed by a single thread
 - does the multi-reader one need another type of pointer, one that we can't read from but can control it's lifetime
   - a lifetime only pointer... hmm
   - we can acquire a read or a write pointer from it
   - ehhh I dunno let me think

--------------------

you know, when we point at a memory location if it isn't an exact layout match
 - we can require that the operation be constant time (e.g. a special kind of cast, fast-cast)
 - and then we can have a verifying cast that will verify anything (still usable with AssumePanic)
 - that way it's not that unpredictable

what does the data structure look like, well in my simple opinion it's really just a sorted list
 - I'm not sure but I think we keep a separate one for pointers
 - do we need separate read/write ones too?
 - the whole issue with threads is that it's only ok for one person to write


The reason it's OK for the same thread to have multiple readers/writers to the same memory
is because you can kind of think of it as like whole functions are atomic (all their changes to memory)
 - it's as if you froze memory and said this is my region
 - everything you do is "transactional"

---------------
imagine a weird language where you could define transactions
 - they were fully simulated in almost like a copy or a reversable part of the environment, a fork
 - when they completed, the result and all changes to memory were recorded

 - The simulation could take as long as it wants, but the final application of changes to memory and the result
   can be must faster to apply
 - if another task runs and changes memory in the same way, we have to stop and re-run
 - need to worry about starvation I suppose, it shouldn't be based on how long it takes things to run
   - and then the transaction is basically "atomic compare exchange"
   - e.g. if everything matches the original state (all the reads are still the same)
 - I suppose the starvation system could involve locking after a number of retries

the second we start a transaction, what we really want to do is effectively freeze/copy all memory
 - but we only actually need to freeze the memory that we're going to read from
 - but we don't know what we're going to read from until we actually read
 - copy on write memory... COPY ON WRITE MEMORY!
   - we have a mix of instrumentation between OS features like CoW plus knowing all the loads/stores in wasm
 - we can basically run this entire transaction in it's own thread
   - as many times until it completes
   - and then we have a thread that can own a region of memory

 - just kind of insane ramblings, but the idea would be the owning thread would then lock required regions of memory
   and apply the transactions (or some system would do it)
 - it's basically just every thread running on it's own doing things to memory, but someone is trasacting them together or re-running

 - you still need to be careful to mark transactions where they are needed
   - anything that is analyzed to be pure (even like an std vector) can be run without transacting
   - but for example, if the vector had a type that used a global pointer, then it would require transacting
   - say for example push_back
   - you might both write to the value, and change the size
   - but if you don't do that as a transaction...

so the flow for calling a transaction would be like cow memory, and then running the wasm on the current thread
 - record all reads and writes, sort reads before writes, optimize and compact (duplicate writes ignored, etc)

when the transaction completes, we ask the scheduler (wherever that is) to apply it
 - this will cause a lock so to speak, and if it fails because the reads are no longer the same, we try again with new reads
 - we do this until some starvation

basically the advantage would be running transacting wasm on as many threads as you want
 - the contention comes from applying transactions, but we could compile them to make them a very fast function call
 - so once they reach the main thread, they are "atomically executed"

hmm, maybe the other way of saying this is that writes only occur on the transacting thread

kind of interesting that we can also save state (like, it's a database!)
 - replica memory is easy enough to maintain
 - I mean that's a pretty cool advantage right there is if this model scales to arbitrary processes
 - we can sync memory
-------------------

shoot, I don't think I can actually use tree-sitter
 - it uses nodejs to define the grammar
 - and uses a C compiler
 - side note, a wasm based version would be kick ass, compile it's rules as wasm


I could maybe do some weird port that uses like a wasm based JS version
 - and a C compiler that compiles to Wasm

hmm, well I may actually want LLVM...

nah, that's sillyness, that would take forever (we'd have to literally compile the parser rules...)

----------------------------------

Tower Demo page:

Tower solves
 - macros
 - templates
 - memory safety
 - code generation
 - DSLs

the three main features of tower:
 - the parser and ability to define new parse rules in language
 - the ability to run code at compile time (macros, templates, generation)
 - the memory safety, with pointers!
 - (also component based design weeeeee)

-------


A = BC?D



A = BD
A = BCD

vs

A = BZD
Z = C
Z = 

But that above approach means:


A = BC?DE?F

A = BCDEF
A = BCDF
A = BDEF
A = BDF

maybe not so bad, but the number of optionals in a single production can become a lot 2^N


Also


A = BC*D

with epsilon

A = BZD
Z = CZ
Z = 

without epsilon becomes

A = BZD
A = BD
Z = CZ
Z = C


-----------

otherwise I need to know "nullable"
which, maybe we need to compute first anyways so we can just do nullable then
but it seems like getting rid of epsilons might be a good idea

nullable might make the closure algorithm more complicated...
but maybe not, it's really just like goto on nothing right?

I'm not sure which algorithm I should go with

https://cs.stackexchange.com/questions/33999/lalr1-parsers-and-the-epsilon-transition

Turns out nullable does not matter, and I should treat epsilon like any other production
------------------

awesome, now I have the lr0 items, need to continue with the parser
can also just do an slr parser to get started

I should make my debug format into graphviz
 - so I can just print all the states out

----------------------

side thought in tower, could we enforce "const" correctness?
 - there's kind of levels of it
 - maybe tie this to visibility
 - but the idea would be that if there are any const pointers out there
   then it is write locked
 - in C++ const is only enforced as a promise that the individual function will not modify
   that specific parameter
 - foo(a: z*, b: const z*) { *a = ...; }, foo(x, x); // x was passed const but modified
   (but a func that takes A const B, and modifies A, and pass the same X, X in as A, B
   then now B is modified even though it was const)
 - it's obviously easy to spot that but happens in insidious (many references) ways
 - or through globals access, why rust declared fundamentally unsafe without some constraints
 - there was the idea of transitive const that I liked
   - but that's again just the idea to further const
   - a declaration that A's pointing to B's treats B's also as const (D does this I believe)
 - but all of it is just like a farce, it's like helpful and has some more restrictions
 - and I had liked the idea at the time that const could be inferred
 - but again, all just BS until Rust came along and actually solved it
 - it's not BS, I get it, but my brain always was angry at the flaws and compromises
 - like as a guarantee, it's meaningless, there's no guarantee
 - the only guarantee is internal, and AFAIK it's a black box so how does that help the caller
 - basically from the caller's point of view, I'm promised nothing
 - that's always been my hatred of const, and no matter how hard I try nothing makes it better
 - that's also why I liked C#, no const
 - they just do read only interfaces
 - I like it because I feel like it solved my hatred of broken promises
 - and even though I don't love coding in Rust, I love Rust purely on principles
 - ok so how am I fixing const
 - well screw all those non-guarantees, even transitive gives you no promises
 - so const now means hold up, someone says no do not write to this
 - ok but how is that implemented at runtime
 - obviously we can then infer at compile time as many places as we want so that we can optimize or whatever
 - but fundamentally we can protect from it at runtime, so what does it mean
 - and is it transitive? the parts along the path of access (this.a.b.c) can be locked
   - except that we don't know the path that the internals of a function will take
   - the caller is oblivious to this, it may run a full algorithm
 - but maybe there are points where it's like, mutable makes sense actually
 - like the idea I'm thinking is const pointers, inferred pointers (just pointers), and mutable pointers (declared)
 - or maybe we have the concept of inferred pointers, where we have const, and normal which is mutable
 - and then you can say this pointer infers, so that way you can safely point at something you "own"
 - and if anyone makes you const, the things you point at become const

so at least the traversal of memory could stop at some point
 - but it's still so weird
 - this kind of has the same implications we were thinking about for threading
 - like anything marked mutable you can't access
 - but any pointer, transitively will be locked for access

 - at least by having the inferred be a keyword it's kind of like
   a declaration of knowledge that you know it's going to be a lil expensive
 - ok so the idea would be that anyone who constructs a const pointer locks the object
 - I would hate to 'instrument" every write operation, that kind of violates my free reads/writes principle
 - but I wonder if another ok way would be to 'null' out all the pointers, or use some sort of special code
   - and if the pointer access and traps and we catch the value of that special code, we know that someone attempted
     a write to a const piece of memory
 - but that would block access to reads, so every fat pointer would now need a read and write pointer
   - and we only 'const_null' out the write pointer
 - also now that means passing anything into a const does like a freaking O(N) operation with N being number of pointers
 - To fix the N problem we could have writes have a slight overhead, again removed in DebugPanic
 - Such as writes could check a flag on the object instead, something like that (or be a double pointer)
 - that way you can stop all the writes at once with a single operation
 - but now you made writes everywhere more expensive, and I like saying that reads/writes are always fast
 - it could also be an error to pass a thing in to a const function while anyone else is pointing to it
   - basically saying it needs to be the only pointer, or only const pointers are allowed to point rn
   - would need a way to 'move' it from a pointer to a const pointer (nulls out the pointer so that it traps)
 - maybe we'll have to support move or something like that, but non-destructive move maybe...
 - so whilst anyone else points at it, it's not ok to pass as const!
 - but then we have to analyze it for transitive
 - basically it means from inside a const object, you cannot access something that's not marked const or inferred/transitive
 - but that also means that once it's passed in as const, we're going to traverse the graph and do some work
   - make sure nobody else is pointing at it, or panic
   - do that recursively for all transivite / inferred pointers, or also const pointers
   - I do wonder though if we might actually just want a way to say this is also const, but stop the transitive here
   - like maybe transitive is just a keyword, or nontransitive
 - we're basically saying that from a const pointer, only things that are transitive are accessable
 - anything else is not just like 'not const', it's straight up off limits
 - in C, if you had a Player with a Gun*, const Player* -> gun would give you Gun* not const Gun*
 - it's unexpected that something a player owns is 
 - C++ solved this somewhat through privacy, but making members (C style) off limits and providing accessors
 - the accessors basically were a wrapper around C behavior,
 - because C++ could not change that to be transitive and break C
 - so yes, in tower, it's just OFF LIMITs period, you can't even access the members
 - and we mark all memory in the graph as const from that point
 - so now we have a true promise to the caller, that this pointer and anything transitively owned by this pointer
   will be locked and never written to for the duration of this call
 - personally to me, though this comes at a runtime cost, it's one I'm happy to pay over Rust
 - and again, I can always compile out the safety checks
 - the last point here is that this is just for const, but for passing over to threads we actually aren't even making it const
   - this concept is kind of deeper, it's just islanding
   - the compiler will generate checks to make sure that a part of the graph is now an island
     but the user must enforce this
 - const just means we changed the interface to be const only and islanded it, and we can make guarantees there internally
 - it's really that we force islanding any time we change a property about how we look at the island

--------------
bizarre idea of llm safety checks, I guess it's just a linter but it's more like instead of a Rust level proof
 - if you can convince an AI to let your code be unsafe, you're good
 - that might involve code coverage

------------------

really side weird idea for tower
 - I was thinking about how we could have number ranges like [0, 100) whatever
 - and as part of the pointer manipulation, if the compiler guarnateed a range then we could
   possibly rely on static analysis to skirt some or all checks or something
 - but the weird idea I was getting was that if a safety check, like pointer manipulation, actually had a way to
   declare what would "make it go away" or change to something less rigorous
 - like if you can prove X, I'll go away
 - keep coming back to the idea of proofs... so interested in this
 - it's almost like an overload, but probably should be controllable too
 - like hey, we're going to do this expensive stuff
 - but we'll choose this less expensive version if you satisfy these conditions
 - it's not just an optimization that some magical compiler is doing under the hood
 - it's explicit about it in user code
 - but I mean overloads can be like that anyways, so am I just being stupid here
 - that's really what we do with the type system is allow the user to connect the dots
 - we're telling the user to proove it to us that 
 - stupid babble I guess, but I think I was wondering if the compiler could infer it
   - or if we can write functions that have requirements that automatically annotate the parse tree and infer
 - but that could become impossible with indirection, so blehs!
   - but then there we go, we were explicit about it!
 - this sounds like we can just add optimization steps...
 - yeah ok, CFG analysis
 - but I think the idea that I had in my head was kind of like
 - Ultimately the type system is always the explicit way to represent any data structure/graph
 - in my head, it was also stemming from input sanitization
 - this idea that we don't know where an input came from or what it's values will be
 - as long as we were able to prove anything about a range of a number for example after say a loop or some logic
 - like I could go as far as providing number range types (like this number can only be 0 - 50)
 - but in a way, what I really want to say is, this number satisfies this fn(n: number): bool;
 - can be anything, even numbers, primes, etc
 - types are just runtime types that satisfy some constraints, even member existance
 - so then a range is really just us saying possible values
   - and technically because uint32_t, etc, it's actually fininte values (not sure if that matters to us)
 - my question would be, given two of those fn(n: i32): bool
 - can we tell that they will be equatable
 - like can I know that one fits into another, just from it's wasm say

so ultimately what I'm really trying to write is like this:

lets just take the array example where we have a fixed size array

let a = fixed_array(i32, 100)::construct(0);
let pa: fixed_array(i32, 100)* = &a; // ok!
let pe: i32* = &a; // ok! since the first element is i32* compatable
++pe; // ok, no check, this is what I want

for n : 0 to str.len {
  ++pe;
}



yes pe is i32*, but we can also infer so much more
 - like we know right now if we consider ssa form that pe_ssa0 is actually at index 0, provably
 - and we know index 0 is provably safe
 - so we offer up a proof, fn (value: i32*): bool;

&a;

fn pointer_manipulate(n: i32_range(0, 100) {
}

// an overloaded version on 
fn pointer_manipulate(n: ) {
}

hmm

we have to balance making the compiler too slow though

but now we'd need to analyze how str.len got there, kind of slow sounds like Rust
 - everything is trying to trace and figure out
 - it's almost like we carry and annotate types at runtime (to be their specific types)
 - like a[5] = 9, is now part of the type, because we know element 5 is 9
 - if we then to a.remove(3), we know a[4] = 9
 - the idea would be that operations can annotate type information (like a copy, or a layer?)
 - like if I say let a: i32, it's like a copy of i32, and anything can attach and modify info for that specific type
 - I mean, maybe all the template functions can just allocate the full type and return it (not shared)
 - hmm, I'm not sure, we can at least just clone, or do some kind of layering system
 - so obviously, if we're evaluating a function under compile time, it will also change and output type information
 - but if we're actually doing it at runtime, we can opt to have it, but it can be entirely removed / compiled out
   - would have to figure out how to opt in for that kind of info to be attached, same compile-time flag
   - and the reason to do so even at runtime might be that you intend to pass this runtime type to some eval / compile

 - ok this is maybe a really cool concept, I have to think on it for a second
 - we're really just trying to glean out as much compile time info as possible
 - and allowing the functions to affect the types output

let a: i32;

// is it even
if (a % 2 == 0) {
  // a: i32 & proof(value % 2 == 0)

  assert(value == 1); // error, this will never be true as it violates the proof

  if (a >= 0 && a < 100) {
    // a: i32 & proof(a % 2 == 0 && a >= 0 && a < 100)
    // Now we know a is an even number between 0 and 100 exclusively
    

    
    b: i32 & proof(b > 5 && b < 10 & b & ~0x01);

    a = b;
  }
}





interface Foo {
  kind = "player";
  health: f32;
}

interface Foo {
  kind = "enemy"
  lives: i32
}


let a : Foo = ..;


if (a.kind == "player") {
  a.
}


how do we compare two conditionals?
 - it's like you need to evaluate possibilities, find a counter proof, not sure...
 - like how do I know (value % 2 == 0) and (value == 1) are incompatable?
 - one way I can think of, value % 2 doesn't narrow the search range, but value == 1 does
 - we can do search range narrowing with each conditional (using actual ranges, but here it's just 1)
 - and then I guess we have to find a counter proof?
 - have to imagine this with structs too
 - I feel like any brute forcing attempt is going to be doomed
 - epsecially because I believe this becomes permutation based somewhat, I think?
 - well maybe not, maybe it always boils the information down to individual members

note also assert itself could be a construct for narrowing type info

like if at compile time

fn foo(n: i32) {
  assert(n < 100); // this should affect both the type of n, as well as at compile-time any n passed in
}

because really this is an overload in a way, e.g. we can remove the assert if we knew the value coming in
 - we can remove as many proofs as we want

// could be written nicer as an i32_range or something like that, but you get the idea
fn foo(n: i32 & proof(n < 100)) {
}

we both know that foo is going to fail with any n > 100
 - but we also want that if foo succeeds when called with an unknown number, the type will be annotated

so

let f: i32 = ...; // say we read a number from a file, completely unconstrained i32

foo(f); // at runtime, the assert will still fire to ensure the value is < 100
// but now at compile time, past this point f is attached with the constraint/proof

// this call now technically doesn't need the assert, is that going to be expensive to check all this logic?
// that means we need to code-gen and optimize a separate version of that call, sounds kinda nasty
// f: i32 & proof(f < 100)
foo(f);

What's odd is how proofs work with pointer types and indirection
 - like for copy types, it's all easy peasy, they can't change


store(&a); // someone stored a pointer to a

if (a == 100) {
  // we can always make copies, those will hold the proofs even with pointers pointing at it
  // it doesn't make sense to update the layouts/types of the pointers pointing at it
  // you're just checking if it's 100, not forcing every pointer looking at it to know it's 100

  // I really know it's only 100 unless I make any function calls, and those calls may result in indirection
  // unless those calls are pure
  // so any non-pure call invalidates all proofs with references

  // a: i32 & proof(a == 100);


  
  // now any call anywhere will disrupt this proof, because we have pointers to it and we don't know if it can change
  foo();

  // a: i32...

  // 
}



  // the ssa version of a will always be 100 period
  // a can be changed in here, in which the type of a will change below
  // but any pointer to a loses it's proofs, 
}




let value: u64 = ...;

if (value % 2 == 0)

if (value & 1 == 0)

// I want to know those are equivalent without testing every value
// maybe there's a compressed form we can compare, we know this is a repeating pattern
// it's really just a repeating pattern starting at 0 of [0, true, false...]
// but what about prime numbers and weird sequences like that, we can't always just optimize it all away

is there a reversal here with the proofs
 - like algebra now
 - I guess I'm talking about algebraic equivalence
 - turns out it has a name, SAT solver or SMT solver
 - if it has a finite domain then it can be searched but may be very slow (i64, for example)

so we really can do proofs but it might be stupid slow
https://avigad.github.io/lamr/using_smt_solvers.html

might be worth it to do some smt verification on proofs, member by member possibly

but also, maybe we can just add this

- I love the idea that we can just introduce the SMT feature to tower
- it just auto attaches type information, and we can compute constraints from that as well as optimizations
- but you just like, include it, and all of a sudden types can have proofs

https://en.wikipedia.org/wiki/Boolean_satisfiability_problem

it really is just this idea that we can include proof systems into tower
 - ideally just as compiled wasm binaries

// C++, can be compiled down to wasm!
https://github.com/bitwuzla/bitwuzla

https://bitwuzla.github.io/docs/c/api.html

ok so the idea would be at some point we think about how to annotate type information
 - some for optimizations, but some as a straight up requirement

or maybe just like we can output wasm, we can also output SMC-LIB format too
 - then you just need an SMC-LIB interpreter, hence bitwuzla

----------------------

we're really just saying the base is wasm, but that doesn't mean we wouldn't add in llvm for the optimizations
 - as a wasm module!
 - or qasm
 - or bitwuzla / SMC-LIB

wasm is just our common interop and determanistic / safe language
 - from there, we import more libraries externally

----------------------

Should eof/epsilon just be a reserved value at the end?
 - we can ensure it's not valid unicode...

eof can definately be emitted by the stream
 - but it never makes sense to emit epsilon from a Stream
 - epsilon only makes sense in rules
 - so that's why we should switch to a union and do a typed enum thingy
 - eof will just be TOWER_INVALID_INDEX, that's already what's returned from stream on eof
 - but epsilon should be a new value

--------------------

do the uintptr_t pass before it's too late!

also do a pass on memory and sanitization

get it building natively with -fsanitize=undefined -fsanitize=address -fsanitize=memory


-----------
note: we might consider actually removing non-kernel items, it will make the sets faster to compare (I think...)
 - well actually we have kernels first so it really shouldn't matter

Look into WIT (wasm interface types)
and WebAssembly component model

I think it still makes sense that we only execute text wasm as our base
 - but we still integrate APIs for qasm, spirv, smt, etc
 - but just as function calls, not text
 - called from wasm, or our own code
 - basically wasm is our base, since it's platform agnostic
 - llvm could be our base, but it's huuuuuge
 - and ultimately we need a bunch of wasm specific functions (like compile wasm binary)

but maybe we can execute any text code we support
 - but what's odd is that the backend actually affects the frontend
 - like spirv and qasm
 - not all calls are legal with those

next steps:
 - actually perform the lookahead propegation and build the table
 - implement the lr parsing algorithm (or lalr if it's any different)
 - use compact tables where possible

test out ranges, make sure they work and get the full idea down



-----------------
one area we need to look into is incremental
 - I'm not super clear on this part yet, but ideally what I want is that not only is a part of the tree replaced
 - but all things that dependend upon any part of that tree recalculate
   - some of them may stop early (memoize)
------------
What is the minimal state for a table
 - doing a big 2d array just seems bad
 - especially since we have ranges, it's not indexable in that way
 - so really we want almost a hash map ish
 - states can be an array / index
 - but the states will have edges / actions
   - which as observed there are often shared, so we can compact and share these edges
 - but the question then becomes, how do we lookup a character
   - they can be unordered maps I suppose...
 - or a sorted vector that we binary search for ranges/characters...
 - many states have empty actions and don't handle inputs (sparce matrix)
 - I don't mind doing unordered maps, but how can we lookup ranges then
   - it's like we basically need to either walk the inputs or do a binary search
 - just doesn't seem practical to have a real array when it can include ranges
 - binary sort seems the best, shouldn't be that many, it's compact
 - or we even do the unordered_map for individual values, and then binary search sorted vector for ranges
 - it's not as compact, but quite fast
 - ok, that seems good enough for now
 - even just binary search should be profiled itself

Need to fix the fact that I'm not propegating lookouts to states / (groups of kernel items)
-------------------------

when it comes to targeting other languages, I know rust kinda does it where a crate must support a target or not
 - I do think it's neat that it either fully supports it or it doesn't, as a crate
 - meaning that the author is forced to section things off
 - for example, if we had an std/common library in tower, it would probably target everything ideally (spirv, wasm)
   - a very small math library may cover qasm, smt, spirv, and wasm

that said, we really could do it any way, we can even infer everything (only some parts are not usable, etc)
 - might be even better...

----------------------
also might consider integrating Legion (https://regent-lang.org/)
----------------

The purpose of lookahead is to clarifty shift/reduce conflicts
 - so technically we should be able to build our states and edges from the original LR0 items
 - and now we should know all the lookaheads for all the states

side note, we have the LR item in state struct that we used to attach to speific LR items
 - but now effectively we just want the lr1 items with lookaheads
 - I guess we can just keep using the lookahead map and look every item up again

but it's really like we just want a direct pointer to an LR1 item that has no lookahead attached

---------------------
weird idea, I want people to be able to create their own types with their own rules, etc
 - not just talking like structs with members, that's a way to make a specific kind of type
   - even with operator overloads, it's still just a kind of farce
 - to create a real type, such as pointers, quantum, etc
 - with it's own rules about how it can be used, constructed, moved, optimized, etc
   - think creating Delegate in Zilch
 - or even like union types, etc, that's a new type
 - I think we get a part win here with components, there's no longer "Delegate" or "Class"
 - there are just types, which is a tree of components
   - and they can have both, but they can't conflict (or if they do they override and it's well defined)
   - but I think like with component interface only one implements...
 - there aren't really type names though, there are just compiletime type variables (side note can they change??)
   - mmmmmmmmm odd I need to think about that
 - maybe every fundamental thing we can do to a type is a component itself with overridable behavior
   - gotta think that through...

let i32 : compiletime type = ...;

let t = {
  a: i32
};

let f: t = {
  a: 123
};

t = {
  b: i32
};
let i32 : compiletime type = ...;

f.a = 123; // valid
f as t; // technically valid with exact layout match...

// DIRECT COMPARISON OF TYPES, OH MY GLOB ITS SO GOOD
if (type(f) == t) {
}

let z : t* = null; // AWESOME YEEE

Ok but one advantage of types was that we can do them like phases, because types can reference above, below etc
So my next stupid question becomes, how do we do that here, in a way that makes sense and plays well with compiletime
 - I mean, the easiest thing to do would be to just declare there is a pass that handles a special 'type' declaration



let Player = {
  gun: Gun,
};

let Gun = {
  ammo: i32,
};


maybe when we go to look up Gun, we create a task to wait for it to appear as a compiletime within the scope (or above scope?)
 - no, just the same scope, I think...
 - can still use scope resolve operators though
 - very, very interesting
 - it makes sense if it can be resolved at compile time

// I would not like it if this could resolve...
{
  let Player = {
    gun: Gun,
  };
}

let Gun = {
  ammo: i32,
};


// but that means...
let a = b;

++a;

let b = 5;


eh.... that would all be legal, because b isn't initialized
 - but since b can be figured out at compiletime


let a: compiletime i32 = b;

++a;

let b: compiletime i32 = 5;

above annotated as the resolved types, it only works because we can compute it entirely at compiletime
 - but...
 - well when something is compiletime, does it have to be pure?
 - hum... I have to reallllly think about this
 - it could also be that we do enforce order, but when it comes to pointers, they can resolve later
   - simply just by putting a task on it, I don't mind that
   - has to resolve to a compiletime type
 - it's also not really special, it's just any type that want's to act as an indirection
 - OK I LIKE THIS BETTER!
   - I mean, I wish order completely didnt matter
 - it's a bit odd tho still, since A*, the pointer isn't really the thing looking up with the task, 'A' is


what if we did everything in dependency order, and then maybe enforce illegal ordering as an after pass
 - like just as a dumb ass idea, what if code doesn't flow top to bottom
 - what if code just flows in order of dependencies
 - that doesn't work though, because we have memory
 - e.g. order matters because of writes to memory
   - can we identify all those places and enforce them?
   - function calls are just a big unknown then (unless pure)
 - maybe that's it, it has to be both pure and compiletime...?
   - because then we know order truly would not matter


imagine this without compiletime:


let a = globalvar + b;
++globalvar;
let b = something();


if something had side effects that changed globalvar, that would be so odd...
 - basically something would have to mark a dependency on globalvar too, but now we're into perfect semantics...
 - but maybe if we had the concept of pure (is it possible that pure is a part of a type, not just a function...)

have to really think this all through, but I think there's a happy medium solution that makes sense
 - doing the pointer thing where pointers allow a waiting task, kinda cool...


going back to this idea, what if we have scopes that can have dependency ordering, not just top to bottom
hmm I dunno, I just like the idea that our main is really just like file scope
 and types defined in file scope can somehow traverse, or maybe we have a special keyword for it
 const pure compiletime.... CPC

------------------
a side note, I think the idea I was trying to convey is that {} is a way to initialize an object
 - but if we initialize it entirely with compiletime types as the members, shouldn't the whole thing be a compiletime
 - type itself? it would be cool in a way if that was just a valid definition of a compiletime type

{
  a: i32
}

because I'm actually constructing an object! like typescript

except because I used compiletime types the result is:

{
  a: compiletime type;
}

since all members are compiletime, the whole object is known at compiletime

it is a compiletime object where all members are compiletime types
I like this idea, think more on it, especially if like the members were just named tower nodes...

it means that ideally, functions that create types are both pure and compiletime
 - for the same inputs, it will never generate a different type, determanistic
 - because we enforce pointers and safety, then as long as you only pass const compiletime values


---------------------
at some point we should support code-gen for our lalr parser, plus codegen for regex / DFA

I like the idea too that we can create the parser in tower almost like regex primitives

// Something like this...
let a = Parser{
  token foo = [a-z];
}

a.parse("a"); // tower nodes!

I kinda like the idea that types can somehow support their own parse rules, maybe just in initialization... not sure
 - so like, the member access operator wouldn't be special in any way
 - it's simply just a type that registers parse rules


a.foo

INTERESTING

even :: for accessing statics...

so when we get to seeing a, it's like we temporarily push on custom parse rules after a (with $ / empty being valid too)

ultimately they are just compiletime parse rules!

-----------------------------

compiletime bool visible(BoundType* looker, BoundType* target) {
  looker->namespace == "
}

let a = 5 + 5;


let Player: compeletime type = {

};

let i32: compiletime type = 

let a = {
  b: i32
};
-------------
Talking with Josh:

read about rust transmutes
Sean Baxter - circle
inochi 2d

opt into compiletime safety


let a = tagged(Enemy* | Player*) = ;


// Based on https://www.geeksforgeeks.org/topological-sorting/
// as well as https://www.geeksforgeeks.org/detect-cycle-in-a-graph/


compiletime TowerNode* = tree(a)

-------------

if you mutate the tree, isn't it a little odd since things may be relying on it while you're mutating it...
it's kind of like transactions, we want to mutate it at a time where it's ready to be mutable
 - nobody is looking at it
 - like delayed destruction

if we have the language be more like nodejs, with a task runtime that you need to pump
 - we can still just have main start/end program as usual
 - but if there are tasks pending, the runtime runs them
 - and in between is how we do async
 - as well as publish any pending tree changes
 - that means we need to be able to lock trees and perform replacements without reparenting...
 - but I really love the idea that if we wanted to add memory safety, we include it
   - and the include goes through and physically modifies the trees or the definition of pointers and allocate
   - like extreme monkey patching

I think what I've decided is this:
 - it's not like zig, the runtime safety is always enforced
 - however, you can disable it with unsafe sections
   - or alternatively, you can use proofs to ensure that you'll never do anything wrong
   - number ranges, etc

---------------------
Side note, I really liked the idea that {...} was actually just a tower node...
 - it's sort of like a json object, and we can index it dynamically too (has all the type defines to do that)
 - but for us, it's also the struct definition

let t = {

}; // compiletime is inferred since everything we use is compiletime

let f: t = {
};

I think we really are just trying to figure out what types look like as tower nodes
 - because we also want i32 to be a type, and just a tower node itself
 - how is size computed, how are member alignments computed, etc

Maybe {} is a tower node with a struct component on it
 - I love the fact that Struct would be a component that implements some of a Type interface
 - the task associated with computing the layout of the struct
 - 

{
  test: i32,
  Struct;
}
// hmm I don't like that...

// I don't mind this, but I also would love it if it was just {} somehow
struct {
}


or maybe...


let struct = {
  has Struct;
};

// INTERESTING, THIS WORKS WITH OUR EXTENDING SYNTAX!!
struct {
}

// And maybe Struct is implicitly constructable as a component, so we auto add it?

hmm, I like it so far

We have to have layout computed at some point

Maybe all tower nodes need hashes for changes

because our Struct component implicitly has a dependency on every child member it has
 - as well as every newly added member
 - basically any changes causes a recompute

is this at all similar to imgui / react?
 - like, when any input changes, we recompute everything...


it means that yeah, when we modify the struct definition, anyone that relied on it needs to update
 - but a question...


let t = {
  a: i32
}; // compiletime, pure, but not const!

let f: t = {// error: missing member b
  a: 123
};

t.b = i32; // legal


t.b = default(i32, 0); // legal, now no longer an error
// since this specialization of i32 is defaulty constructable (it's also a copy)


NOTE:


let t = {
  a: i32 = 123
};

// This: i32 = 123 is the same as default(i32, 0)
// Note that the 0 is actually like a lambda, you can also do expressions

let t = {
  a: i32 = ++Player.counter
};

// This: i32 = 123 is the same as default(i32, ++Player.counter)
// Note that the 0 is actually like a lambda, you can also do expressions


t.b = default(i32, 0); // legal, now no longer an error
// since this specialization of i32 is defaulty constructable (it's also a copy)

forces the struct to recompute layout, and all dependencies upon that struct must recompute
since `f` is using `t` directly as its type, f must recompute

I wonder if overlaying component trees should be a feature of tower
 - e.g., like lua meta tables in a way
 - I mean maybe tower nodes should just be meta tables themselves
 - but basically if one tower node can have it's own components that come first, and as a fallback query another node
 - would only work if the number of children was matching
   - can have overlay modes, like overlay children by same name/index
   - or add children by indices together in one larger list, A first then B
   - etc
 - I just see & types as kind of being like this thing that we want to extend constantly, but not make copies everywhere
 - maybe that's why we just put them as children in a tree, e.g. A & proof, we're not overlaying proof, we're just making
   a new tree with & as the parent, A is it's own type, proof is it's own type
 - we just have to do comparison work to make them appear like a single type

 - we can make a pass that simplifies proofs together, etc
--------------------------------



a random idea, what if we could promote a runtime value into a compile time value
 - by invoking the compiler on the same code
 - like, we examine the value at the current time and snapshot it, ok it's `123`
 - has to have no external references (value typed, pure)



hmmmmmm!!!
I know obviously about pure functions, but something about pure functions is that their inputs must not have
external references, that could even mean memory...

however, we don't allow our memory to point at unsafe locations
 - so the only other problem is that we need ALL inputs to be the exact same, no reading random global memory
 - so maybe a pure input is one who may have external references, but it's an island, like in physics
 - all pointers only point internally within the island (may be many allocations)
 - maybe one of them is the pointer, kind of like a topological sort... dag
 - maybe like a root owner, internals can also point at the root
 - but two roots can't be joined together
 - roots could be explicit types, but I wonder if they can just be GC roots too...
 - I do like that it's like a movable structure...

so the idea would be that we can call the pure function, but the input must be const, and must be an island
 - so effectively that's what pure is, an island
 - it can either be const (neither side can modify it)
   - or it can be entirely moved into the pure function, in which it can do any manipulation it wants
   - it can even return it, but it basically owns it for a time

we basically need to be guaranteed it's serializable, or static/unchangable
 - can it be changed after we call the pure function?
 - I think we mainly just need to be able to identify it's uniqueness...?
 - for caching / memoization purposes, we really just need to be able to hash / compare the whole thing

---------------------
I think trying to go direct to the final State/StateTransitions
result was kind of stupid, we have to move all the states anyways, didn't save much

I think we should make a "builder" version that has exactly what we need for simplifying the next steps
 - use unique_ptr, etc where it makes sense
 - builder can just use indices for everything
 - can also hold a reference to the states

can just store all the edges in the same set, can optimize it later but just need to compare

Well now that we're on the idea, what should our set structure look like
 - we use it in only two ways, one with kernel items only and one with everything (closure)
 - parser_table_goto

We never need to remove from the set (except when filtering kernel items)
 - but we can just rebuild a new state for that anyways

 - closure just adds items (ok)

do we know a distinct spot where they must be sorted, or is it always?
 - only when comparing
 - but insert needs to check if it's in there already

insert can do a linear walk
 - but comparison of two sets definately needs sorted, for lexographic

set comparison could be faster since we have indices...
 - actually lexographic comparison should be very fast
 - compare that two 

also a side note, once these are built, we can use indices for states so we never need to use a full set as a key


very few kernel items per state, so insertion sort should be fast

if we knew that two sets, even not sorted, had the same count of LR items
 - we could then write to a big bool array or bit vector, like a preallocated thing for each rule and symbol
 - we walk one set, marking every symbol as true
 - then we walk the other set, and make sure every symbol is set to true
 - then clear the first ones back to false (temporary scratch space)

can also have an unordered_map... can we just compare those?
 - technically they should be in the exact same spots with the same keys...
 - unordered_set

I mean, we can just say fuck it and switch to unordered everything

I really just wanted it for consistency but fuck it really
 - it's not non-determanistic, just kinda unpredictable, but... who cares

I mean yeah... why not

if I really wanted to preserve iteration order, I would use a hash map with an intrusive linked list on node's
 - so it can remember order of insertion
 - I really would like this

My hash function for unordered_set might actually suck ass now that I think of it...

weird side note, an lr item points at a grammar symbol, and every grammar symbol is unique
 - technically a grammar symbol can know it's lr item
 - except we don't have one at the end (we can make a fake one at the tail / terminator)
   - and it's easy to get the lr item from that too as long as they have their indices
 - and I think what I'm saying is that the pointer value itself can be faster
   - but now that's reaaaaally non-determansitic
 - funny though in wasm it's a little more determanistic I think, probably not randomized addresses?

the result should always be the same no matter what... it's just the order is a little wonky
yeah I don't like that, it would annoy me
now that I think about it, I should make a tradeoff here for speed
 - at the end of the day, I want this to be debugabble and determansitic

I think the sorted std::vector is just the best idea

TableBuilder holds StateBuilders indexed by state

since these are builders, we can over-reserve (but it's mostly just to prevent move cascades, at least it's moved tho!)

and StateBuilder we add edges, but we just use indices and ranges for everything
the state builder also has the LR items, which will start off first as full sets
 - but then we'll reduce them to kernel items (in place) I think (or into temporaries and then move back over)

 - any time we insert an lritem into the set, we insertion sort it in

and then we use lexographic compare to compare sets (linear in smallest time)

quick sort is going to have to do the exact same thing
 - literally we add 

What if when we hashed, we only considered kernel items
 - after all the rest are the same
 - that actually kinda works?
 - the only reason we had to convert an lr1 set to an lr0 set was to do the lookup

so with this, we'd be able to look up the state

- we can even modify the key (lr items) if we guarnatee the hash and compare don't change

hmm, then lexographically compare needs to skip
 - we already store kernel items at the front...

so its' really just compare up to kernel ranges
 - actually we can do that instead of ing std::end...
 - walk each just to get the first non-kernel item

the idea I had when I was out, the LR sets should store kernel items separately
- delete lr0item comparison, make sure we no longer need it (kernel comparison)

----------
at runtime you should be able to read in a value / values, and then create types from those (even ranges, etc)
 - then recompile parts of code with inlined values
 - when an engine reads in tweakables at the beginning, global "values", we can compile those all out at start
   - I really like the idea of "memoize" except it means make it compiletime...such an interesting concept
 - moreover, if we know the change never affects layout or ABI, we can hot swap code (may need to exit stack?
   - would be amazing if we could support patching low level assembly too (like what wasm jits to)
 - I also love the idea of compiling ABI compatable code in the background and hot swapping it.. so cool
   (while assets are loading, etc!)
   also just turning on optimizations is ABI compatable
 - can compile a debug version first (not even debug, just like direct to wasm)

 - as far as making a runtime value into compiletime,
   it really just means pretending to recompile all the code as if that was the compiletime value
 - the nice thing is we already have the v1 of code (compiled with the assumption that it's not compiletime)
   - so we can always call that version, and then ideally fall back to the optimized version
   - technically behavior could change if someone did something stupid like (if node(a).compiletime)
   - but that's maybe where we just say it's your job to keep it the same
 - and ultimately the user is the one who gets to either wait for the compiletime code to finish, or allow it to hot swap later
 - can really apply to any expression

 const v = await compiletime(readInt(stream));
 // here v is just an i32, we don't know what it actually could be
 // but we do know that the code will be optimized as if v was whatever value we read in
 // kinda weird, if we don't await, we don't want v to be a promise, it should just be i
 // or maybe implicitly convertable to the i32? I dunno hmm
 // or the promise has the value on it, and it's also implicitly convertable, eh...

side note, we should make forking a supported feature
 - I mean you can spin off an empty sandbox if you want
 - or even attempt to copy your own memory
 - but forking would be awesome (still in the same program memory tho)
   - or maybe not, maybe someone is down to come up with a dope multi-process isolated architecture for tower

I just really love the idea of taking a heavy algorithm with inputs that some of them are 'cachable' / don't change often
 - and then recompiling it as if some of those inputs were constants
 - maybe it needs to be pure for that to be ok though
 - but that's not a bad idea to require pure
 - well one thing we know is that this 'optimization' can apply to ANY pure function
   - we just need value type semantics (even for graphs of objects) to ensure the pure function can't look outside of all its inputs
   - and that its inputs are effectively hashable / comparable
 - it's also one of those things where like, it can't just be user comparable, it needs to be like equivalent of mem comparable
   - but even for graphs of objects
   - there can be no single difference between the graphs
   - or if there is, it must not be accessable by the pure function
   - maybe that's what we were talking about with the pure keyword on members...
 - everything that's pure can only point internally, need to read my previous notes and think that all through
 - but yeah pure functions can definately do that sort of memoization
   - then again, pure functions can just remember the results too
   - but that's remembering the results for ALL parameters / inputs
   - this is the idea that like, some parameters may be constants, and like shaders, any compiletime constants can be compiled out
     - down the entire call graph
 - OH MY GLOB this is such a perfect feature for shader compositing with values that are 'constants' but tweakables
   - every engine supports this, this would be built in to tower!

AHH, side note, shaders must be pure!
------------------
another dumb idea side note, I love the idea that tower can support save-stating
 - I need to think it through but I also think we can abuse the idea that some values become 'constants'
 - and when we save state, we take any of these variables marked this way and automatically pretend at that point that they are constants
 - obviously if they change, we need to switch the code we use

but the idea would be that we could run the game, load all the assets, and then save state (everything in wasm memory)
 - we also run it in a startup mode that preserves memory (e.g. vectors don't over allocate, etc)
 - the memory allocator will attempt to compact / defrag, may be slower to allocate
 - an idea there, anything that is technically allocated but now only pointed at by const pointers is open for optimization...

-----------------

next thing to do, make a general sorted vector for grammar terminals (FIRST set)

After we're done, examine every use uint32_t and uintptr_t and size_t

I think I should move the entire parser to it's own place so we can run tests...


A side note on optimization, when we add the LR1 states we also add grammar symbols
 - but many of the LR1 states are duplicates of LR0 states with different lookaheads

GOTO: ==========
[F = '(' E ')' , #]
[E = E '+' T , ')']
[E = E '+' T , '+']
[E = T , ')']
[E = T , '+']
[T = T '*' F , ')']
[T = T '*' F , '*']
[T = T '*' F , '+']
[T = F , ')']
[T = F , '*']
[T = F , '+']
[F = '(' E ')' , ')']
[F = '(' E ')' , '*']
[F = '(' E ')' , '+']
[F = '1' , ')']
[F = '1' , '*']
[F = '1' , '+']
----------

we don't need to add grammar symbols for each LR1 state, only for each unique LR0 state
 - all LR1 states with the same kernel will have the same grammar symbol
 - we could separate out the insert call to insert grammar symbols separately from items...
 - or alternatively, we simplify it into always being LR0 states with a separate lookaheads sorted vector for each...

-----------

I think we can remove the allocated count stuff, and just have one for memory

---------

Also I think we should come up with a better debug output thing, like start with it right away
 - basically output graphviz style structures, but as minimal as possible
 - like writing to a preallocated buffer
 - and somehow what's written can be converted, like debug draw
 - I mean right now printf debugging is fine, but I want a debug stream
 - I really just want graphviz...

------------
Does the table really need to hold on to the grammar?
 - or can the grammar be ephemeral so to speak, only created for the algorithm

Right now beyond the builder, every state only holds on to the grammar rule and grammar symbol per state
 - but holding the grammar symbol, there's no need for that, that just helps us identify the state

and all we actually need from the grammar rule is the index and Rule* component
 - and maybe the rule name, just for convenience

------------
side note, for the debug stuff, we can possibly use that for tests too
 - but we'd need to remove the unordered_map
 - can we know about rehashes?
 - if our unordered map is just an index into an std::vector...

- lets make the ordered map

