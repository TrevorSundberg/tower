It's best to think of tower as a blend of
 - C with familiar syntax for blocks, control flow, loops
 - C++ with familiar concepts to methods and statics
 - C# with the focus on ease of use and simplicity of syntax
 - Rust with the traits and how you can implement them for types not your own (extensions)
 - TypeScript interfaces and types as it's own expression language: (A | B) means it could be A or B

Tower however introduces novel concepts on top:
 - BNF parse rules can be written and extended in tower code itself to extend the language syntax at any pointer
 - The compiler and runtime are combined and can safely execute compiled code during compilation allowing us to use code in tower to generate more tower code, eliminating the need for templates, generics, and other complicated compile time syntaxes.

Tower moves away from the typical AST walking approach when compiling to use a more novel dependency graph with tasks that wait for other tasks to complete.
This allows the Tower compiler to compile full sections of code all the way to code-gen and runtime execution, while another piece of code is waiting for a type check.

It is also important 

Tower is a type safe language with duck typing features that compiles to approaching native performance.

By separating out all steps of the 
By targeting WebAssembly like sandboxes, our compiler can run safely in a sandbox and execute code


------------------------------
When a parse rule matches, do we really want to perform callbacks right there?
 - Or just build the parse tree?
 - We really just want the AST out of it, and then afterwards we can do stuff with the AST
 - But I like the idea of transformers
 - We kind of want any type of logic...
   - Rewrites, transforms, or just running code directly
 - Cool examples of transforms:
   - Calling operator functions: a + b => a.add(b)
   - for loops, etc

We can do everything in passes, but I think a cool generic architecture might be that we just run code until it hits a dependency that isn't met yet, and then like a closure yield, it waits there until the dependency is satisfied and continues

That can even go for type names, like if you refer to a type name but it's not found, as the compiler writer you can choose to defer and wait for the type name to become available

Basically it's symbol lookup (possibly with a predicate?)
If anything is left unsolved, then we omit an error
The symbol we're looking for should be fully qualified, and if that symbol is ever discovered we resume

// This is OK if they are both pointers

class Foo {
  var Bar; // Here we don't know what Bar is, so processing for the AST/parse tree of "var Bar;" hangs until we find Bar
}

// We register Bar here, so know var Bar can resume
class Bar {
  var Foo;
}

We can wait on a fully qualified symbol (by name basically) or for any of the parts of the AST to fully resolve
 - That's how we can do inference
 - Lets think about rust style

fn test() {
  let mut x;
  x = 10;
}

Here the type of 'x' should be incomplete
 - the line "x = 10" should apply a "preferred type" or something like that
 - basically we register an operation to be "inferred"
 - and then later things can register what they prefer
 - and your code handles taking all the preferred usages and turning them to inferred




Lets pretend to implement a basic language



Wasm here, declaraitons and code that runs directly

All code instances run in isolation, but are given access to the parse tree
 - All parse nodes should just be property bags
 - Otherwise it would get unmanagable pretty quick
 - That doesn't mean everything in the langauge needs to be variants
 - Eventually we write an object that binds itself and looks like the parse tree (expose it)

Maybe instead of using strings, we make it easier with 64bit ascii 8 byte "strings"


tower_ast_child(parent_node, index)
tower_ast_remove(node) // Deletes node and all children below it

tower nodes can have properties that point at other nodes, and we will serialize it all properly

Our language should definately support "extern" types that are variably sized and have their own meta interface
 - This way we can bind tower nodes properly

(data (i32.const 0) "Hi")

So part of implementating the language is implementing a generic definition of these AST nodes
 - As well as the parsing algorithm, and WASM emitting / execution
 - I like that this may be the entirety of the bootstrapping for this language
 - Our "Standard" will eventually include the exact details of the LALR(1) parser
 - And then we can make a meta binding library per language, one for C/C++, one for Rust, etc
 - Easy to bind to each
 - Maybe we can translate our language to each language, so we don't need to update, just generate bindings


Like most compilers, we need passes over the AST, it can't all be one pass
 - And we have this idea for the "solver"
 - So how do we register with wasm that we need to execute more passes?
 - In order to think this through, I need to think about how we even compile or include files
 - Because in general the compiler should start with a "prelude" that includes all the core compiler stuff
 - When we add a new rule, we may want to run some wasm immediately to modify the AST
   - But we also want to run some WASM later when we walk the tree (register a tree handler)
   - I had several passes in zilch, including a final code-gen pass
   - Passes solve a lot and are easy to understand, but they have limits and they don't handle general solving well
 - Ideally if you introduce a new type of parse node, you should have to handle it in certain passes
 - We can make up the passes for now, but it's basically like
   - Constant folding
   - Collecting type names first (module style)
   - Assigning types
   - Borrow checker (as an example)
   - Bytecode generation
 - We can blur the lines between these passes by exposing all features needed at all times
   - Maybe we can register our own passes with depencies (must be before, must be after, etc)
   - At any point, technically any of them can code-gen (tower_emit?)
 - Lots of ast helpers (find any parent, etc)
 - In general, I want a super generic solver that works somehow with the AST
   - Imagine that a type is assigned to a node, we can wait for that type
   - Support Rust style inference
 - And then we automatically emit errors if we don't find any
   - Tower nodes should directly serialize to JSON, basically just JSON without any complicated rules
   - They also don't need to be our "variant" in the language, we can handle that later, since we have wasm
 - Unfolded form wast example: https://stackoverflow.com/questions/61399299/how-can-i-make-this-wasm-function-easily-readable-in-its-native-language
 - Might just add an extension to wast for our i64 constants: i64.conststr "test"
   - Could even get away with 9 characters or more if we don't go for ascii encoding
   - If we only allow lowercase and some symbols, 5 bits per letter (26 + 6 extra, underscore, etc)
   - We could fit up to 12 characters with 5 bit encoding, and possibly up to 15 if we get clever with only 27 symbols)
 - https://github.com/WebAssembly/stringref/blob/main/proposals/stringref/Overview.md
   - Can also possibly just support first party strings, might make writing our language easier too
Tower nodes are generic and take an i64 key
 - Tower nodes can also hold a value, basically a variant
 - Can hold integers, floats, any wasm primitive data type, and strings
 - Can also hold children (and be an object) - can't be both an value and object at the same time (keep it JSON)
 - Can also be an array, really just treat a tower node like a JSON object
 - Maybe just take a look at quick js and duktape to see what their api looks like, keep our simple

Since tower nodes are meant to be parse nodes, we may have them point at ranges of begin/end in the text they were created from, as well as possibly a pointer to the source text (or make sure we have some way to read the substrings)
 - So a tower node has some implicit parse node data on the root of it
 - How does that turn into JSON?
 - Technically generated parse nodes wouldn't necessarily need a line associated
 - But it's amazing for errors to always be able to derive a "stack" of where a node came from
   - Basically a member that is an array of infos that tell us how this node was created during tower_node_create()
 - So nodes basically should implicitly store a way to know what tokens they were derived from
 - And a way to print out a stack of how they were created (in expanding rule, etc etc)
 - Most of this should be read only / managed by the compiler, so users can't muck with it
 - Maybe all this information should be savable in the JSON, but it's in a separate table
 - Even if it's json, it would just mean every node is an object, but we have a "value":
 - Ok that's all fine then, does the API use our key value system or is it a new API?
   - Maybe we just reserve those keys, you're not allowed to write over a reserved key
   - Introduced a new collision problem for ourselves, but that's why we have language version bumps
   - It's going to have to happen anyways as we introduce new rules in the language
 - token_start, token_end
 - The root might have more, like source info, etc
 - TypeScript interfaces would give order to this chaos
 - But again, we have many helper methods that assumes we're a JSON style tree
 - tower_find_up, etc treat it like a game engine finding components on an object tree
 - We only store the minimal data we need, no duplicates
 - Tower nodes should be able to safely point at other nodes, but like a weak pointer (node ids)
 - Our types should just be tower nodes too
 - All files are compiled under a single wasm context (maybe we can split this up later by module or something)

So we want the solver to be able to wait for a specific tower node member to be created
 - Basically we want a condition to be satisfied, and once it's satisfied we want our code to run
 - Very simple to think of an idea how we can register a "wait" for this type of node to be attached
 - Does our code run on attachment (potentially during our own code running and making an attachment)
   - or does it run once all code finishes running, like a task
   - I think task based is most suitable for a solver
 - So whatever condition we do triggers a task potentially being activated to run
   - But tasks should have conditions, and the conditions should always be re-checked before we run
   - Since whatever triggered the "check if this task should be run"
 - Tasks have a condition, as well as a trigger
   - The trigger loosely follows the condition, but may trigger more often than the condition
   - The condition's responsibility to only allow it to run if exactly a condition is met
   - The trigger is basically an optimization for not having to call conditions for every task every frame
 - We can do the "every frame" simple solution for now
 - On top of the parse tree, we also want a sort of module / type / member tree
   - It's often a merging and trimming of parse trees, basically a simplified tree that's joined from multiple files
 - Maybe the root has different types of trees, and we can have a 'type' tree
   - How language implementers can do a::b::c nesting
 - Up to your language what kind of extra trees you will need
 - So if we typed in a type name that we wanted to be resolved, we could put a dependency on:
   - root::types::<typename>
   - Or something like that, if it's allowed to be resolved in your module, parent module, etc then
     we could have a multi-depenedency task where "any" condition that gets met satisfies it
   - But not multiple, if we ever see multiple, it's an ambiguity error, unless we have a resolve function
 - Makes our language design really nice I think

If I wanted to make a scripting language that could do this:
```
let p: Player;
struct Player {
}
```

When we came to a node that referenced Player (let p: Player) then we would go to our type tree
and attempt to look up a type, which we would not find yet

Because what I'm going to do is first look up the type by name, and if I find it I will then
tag my own node with that type. So the task I would register would have a condition to find that
type first

And if the task never completes at the end it would emit an error
 - Ideally we have a way to identify that tasks are depending upon the same thing, or upon each other
 - So we don't just emit a huge spam of errors for every single task

So our rule would be like 

Var = "let" name(Id) ":" typename(Id) ";" => register a task with a condition on finding a typename of Id and the code it runs when the condition is met (the side effect) is to add a 'type' member to the Var node (point at the type node)

Conditions can produce a value - e.g. the node that we were waiting on
 - So the side effect code can use that value, like get the type
 - Conditions could be generic wasm, but we might just have a set of supported conditions that we can optimize
   - Maybe support a generic condition... ideally run in a sandbox

Here we actually don't care about "let", ":", or ";" so we should find a way to drop all those as nodes
 - as well as name children nodes

```
let x;
x = 0x01; // consider x as i8
x = 2000; // consider x as i32
```
Since i32 fits both, we use both

We want to be able to implicitly type x as i32 here, rust style
So we also want a way to say that a node 
 
```rust
  let mut x = vec![];

  fn test<T: core::fmt::Debug + Copy>(vec: &Vec<T>) -> T {
    println!("Test: {:?}", vec);
    vec[0]
  }

  let mut y = test(&x);

  let mut z = test(&x);

  y = z;

  //z = 5;
```

When I uncomment `z = 5;`, rust can trace the dependency chain of types

y depeneds on the return type of test
but since the return type of test is generic, it depends
 - it's really just being able to solve the chain anywhere along the way

I want our language to be able to solve types like this
 - As well as TypeScript style situations

When I write the line z = 5, we know or can assume 5 is i32
 - and since Z is a local var reference and is not typed yet, we can set the type of z to be i32
 - ideally this triggers typing for call to `test(&x)` somehow
 - Because the line `let mut z = test(&x);` test's return type should also depend on the type of z
 - Seems like a cycle of dependencies

But at the end of the day, all we really care about is that everyone gets a type
 - So maybe tasks can be satisified and cancelled

like y's type depends on the return type of test, but test's return type also depends on y and z
Since z gets solved, then test's return type gets solved, which solves y's type too

I imagine the task for assigning the type of y to the return type of `test(&x)`
 - test is a function call expression, and it should have a result_type
 - We wait for that result type, and our action sets our node's result type to the same type
 - Every local variable node also waits for their parent's result type
 - But when we do `z = 5;` the assignment operator would check if the types match
   and would see that z doesn't have a type yet, it would assign it to rhs type
 - rhs may not have a type yet, so we might need to wait on that too
 - Our assignment operator would wait on a type for the rhs, and optionally get a type for lhs
 - Since we hit a condition that matches rhs type, but no lhs type, we run and assign
   a type to z
 - Since z gets assigned a type, it cancels waiting on the return type of test
 - Note that y is still waiting on the return type of test
 - Since test's return type also has a dependency on the type of both y and z
 - z got assigned a type, so that triggers and sets the return type of test to i32
 - this also cancels test's return type waiting on test's parameter type
 - And test's parameter type depends on the return type too, so that fires
 - which goes all the way back to the vec
 - y also depends upon test's return type, so it fires and resolves

 - We basically want these conditions / waits to be coroutine style pauses/yields

Conditions are basically like:

What we want to accomplish (adding type, cancels if it's already done by someone else)
What we're waiting on to accomplish this

ideally we keep the way we register these as simple so we can establish a clear dependency chain
 - we know this task will add this member, we know this other task is waiting on that member, etc
 - can be multiple tasks that solve the same problem in different ways, so it's more a graph

and then we get stellar errors at the end when we can't complete something

And we just register wasm functions to run based on these conditions

a compile(expression) feature that runs all the code of an expression (including block or closure)
 - it depends upon the bitcode portion of an AST to run (dependency)
 - WASI functions have well defined ways that they run at compile time (and hooks you can override)
   - Can see any folder under the compiled directory, command flags for adding more, etc

A cycle of dependencies is detected when we can no longer run any of the tasks, but there are still tasks pending
 - But it would be nice to explicitly build these dependencies so we know the second a cycle task is added

It would be great if the whole compiler could fit within the runtime too, since the parser is surely part of it
 - I like the idea that there isn't exactly "compile time" but just steps, like you can precompile a bit
 - and then compile more later with more data
 - But ultimately, the runtime can ask if the current call is during a compile step



  // Rust style type inference is actually simple, the first time someone wants to use an inferred node
  // as a certain type, we just automatically register it as that type, and any future changes error

  // TODO: Cleanup and make it nicer for rust
  // Make cleaner way to print out tables
  // Check if predict rules are correct (try in our own Welp too)
  // https://www.usna.edu/Users/cs/wcbrown/courses/F20SI413/firstFollowPredict/ffp.html
  // Things to discover:
  // - Does adding a new production invalidating things we did before? I assume so
  // - Is there any way to incrementally add a production?
  // Next implement BNF + Sets to NFA
  // And then NFA to DFA with shift/reduce stack
  // Basically get this parser actually working with inputs
  // Not sure how much I love rust currently, I mean it's alright but the readability is bleh
  // I think typescript has been the cleanest language for reading so far
  // Just thinking about how much easier this would have been to write in TypeScript...
  // And easier to write UI for debugging...
  // Graphviz, etc
  // Make the whole thing in VIZ
  // I want the first thing we do to be a very minimal implementation of the compiler

the traits in rust are much easier that Typescript interfaces
 - Because they can have default implementations
 - And "classes" in typescript have so many annoying requirements
 - I love that there is no "new", it's just static methods and you have to constructor the class
 - and no private, just internal / crates / modules

but typescript union / subtract, etc is also awesome

Just like we have compile() to evaluate an expression, we should be able to write any type expressions
type1.union(type2)

so if we compile() an expression, and it returns a typeid, we can use that as a T

function foo(bar: i32, baz: compile(if name.contains("a") { typeid(i32) } else { typeid(i64) })
we should also be able to eval(string) which executes codes with the current set of rules where eval lives

----------------------------------------------

The worst part about the TypeScript implementation was the annoying Map behavior
 - If we could make custom object types with hash and equality
   it would be much cleaner than having to use unique strings
The worst part about the rust implementation is that it's extra verbose and lines are long
 - Because of having to get a mutable reference to data, it meant all the pieces of the sets were
   constructed individually and only combined at the end. This isn't bad at all, but having to take
   each by reference, as well as "has_changed" was super annoying. It really does mean do less with objects
 - I also thought all the RefCell and other garbage caused it to be somewhat unreadable
 - I can see how if I understood rust to the T, I would read it like butter
 - But I think the need to be explicit with nearly everything is taking away from what the actual program is
   (the idea of the program, which I should easily get from reading good code)

To be fair, maybe there are much better implementations now that I understand the rust one better
 - Maybe it's easy to clean up and someone could show me the right way
 - But having to like... deref,deref_mut,borrow, borrow_mut... it's just taking away from the code idea itself,
   it's safety and optimizations rolled into one

Next step is BNF Sets + BNF Grammar to NFA, then NFA to DFA

Once we have that, we can start authoring the nodes

I really just feel like I should support LALR(1) only, but the old Welp grammar parsing was interesting

It's a language whose parser is part of the language
 - I think most languages end up hand rolling their parsers so they can do special tricks


----------------------------------------


// I think instead of trying to store weak nodes within nodes (it kind of gets confusing)
// lets just support weak nodes as children (graph nodes basically)
// they aren't considered owned when attached/detached/deleted
// In all our find functions, we should take options, whether we only consider owned nodes or not
// we also want to make sure find never cycles if it indeed is a graph
// I wonder if we should remove "kind/parse_start/parse_end" and just make those members
// basically we can't declare structs yet

/*

AssemblyScript when I try and write A | B:
  Not implemented: union types

This seems like the entire point: we do traits/interfaces plus union types
 - where possible, we don't use interfaces and just deal directly with data
 - and maybe our vtable even has "virtual member offsets" so two structs with different layouts but same members can work
 - basically members can be direct offsets, or get/set, but the compiler oversees this internal detail

possibility to have stack handles, handles to members, handles to values inside an array, etc
 - All properly checked when we dereference the pointer, and we can only use the pointer lifetime temporarily
 - and we focus on the optimizer eliding dereferences and reducing ref count operations as much as possible
 - Find out how to make arrays and slices safe and fast

no garbage collector, but you can leak
 - we could maybe do something like not allowing cycles of pointers

I like the idea that you have to write a:  A | null
 - We make you be explicit about nullability

And we attempt typescript style:
 if (a) { A } else { null }

And we also need dynamic objects too, like indexable typescript objects, so our meta should have dynamic access operators too

Focus on the language being pleasant to use and easy to understand
all reference counted, with explicit delete, and hard locks
easy thread safe models too
100% safe language with no data races, determanism (no undefined / unpredictable behavior, even for containers)
 - not native performance, but still stellar perf for tight loops, etc
 - basically give C# a run for it's money
 - and make it easily embeddable in other languages (small runtime, wasm interpreter, expose wasi functions)
 - make sure the resulting runtime and compiler binary are small too, run on embeddable



Ok, even if in the future we port this structure over, we still want it
It's still a great generic tree structure, and maybe in the future we'll merge it with our object / trait model
I basically want rust, but with everything being dyn traits
and do it C#/Zilch style, structs are copy, but everything else is allocated automatically
 - it just simplifies move/copy behavior
 - but maybe we can reign in handles a bit
 - typescript style where any matching interface is an interface
 - and interfaces can require data (can they add data too, I always loved that idea)
   - Might make ABI compatability annoying, but at that point you're already in C land
 - And we want to be able to do typescript style A | B


In some ways we would want to write in our own language
tower nodes are generic, support virtual behavior, easy to generically iterate over
but could we just create tower nodes in our language as a struct
 - just a bunch of pointer and arrays (would want a dynamic sizing vector at least)

Fascinating idea, we may be able to solve tasks WHILE parsing
 - if we make the parser re-entrant so it's iterative
 - when we resolve a type, we could potentially even allow the type to introduce it's own parse rules
 - it would have to be after some operator or something on the type, "test".
 - I like this idea as a way we could introduce swizzle operators but entirely using grammar rules
 - maybe we can design it where some member access operators get their own rules that temporarily apply
   - but they type would have to be wholly known, no inference
   - let a;
   - a.something // no idea what happens here since we haven't parsed a=64 yet
   - a = 64;
 - Actually as much as I love this, I think it's better to do everything with a "dynamic" access operator that resolves members
   - It can also error, which sends up a compiler error
   - So we can allow any code to handle any member it wants, e.g. swizzles for .xyzw
 - I like this better, it fits that a type can also be generated at compile time


// Maybe all tower nodes should be owned by a parse object, which holds the source code strings (context)
// Here string may just be i64 with a specific compressed ascii encoding to fit 15 chars and underscores

For our sanity, we almost want a way to describe tower node interfaces (like TypeScript / JsonSchemas)
 - and have them be checked / validated
 - maybe this comes later as we advance the language and start writing things in our own language

tower_node_create(context: tower*) -> node*
tower_node_destroy(context: tower*, node: node*) // panic on double destroy, check free list header
tower_node_attach(context: tower*, child: node*, parent: node*) // automatically unlink and relink
tower_node_attach_member(context: tower*, child: node*, parent: node*, member_name: string) // automatically unlink and relink, any member of the same name is removed
tower_node_get_parent(context: tower*, child: node*) -> node* // or null if it's the root
tower_node_get_child_count(context: tower*, parent: node*) -> u32
tower_node_get_child(context: tower*, parent: node*, index: u32) -> node* // or null if not found, panic if out of range
tower_node_get_child_member(context: tower*, parent: node*, member_name: string) -> node* // or null if not found
tower_node_get_kind(context: tower*, node: node*) -> string
tower_node_get_parent_member_name(context: tower*, child: node*) -> string // returns empty string if not named

enum type {
i32,
i64,
f32,
f64,
string,
bytes,
weak node*, // safe, nullable
strong node*, // panic if we attempt to delete the other node without releasing this
}

tower_node_set_value_T(node: node*, value: T)
tower_node_has_value_T(node: node*) -> bool
tower_node_get_value_T(node: node*, default: T) -> T // uses default if T is not valid
tower_node_expect_value_T(node: node*) -> T // error if the value is not correct

// all the traversal orders, probably better correct names for this
// any combination of these
bitfield order {
  self,
  up,
  children,
  down_breadth_first,
  down_depth_first,
  siblings,
  ancestors_children,
  etc
  // do some orders that walk up, but also visit siblings
}

tower_node_find(node: node*, order: order, predicate: callback) -> node*
tower_node_find_kind(node: node*, order: order, kind: string) -> node*
tower_node_find_type(node: node*, order: order, type: type) -> node*
tower_node_find_member(node: node*, order: order, member_name: string) -> node*

We should find a way to make all the find functions take a previous value, so they can find the "next" one
 - We can use this for iteration
 - callback?

tower values are kind of already arrays because they can have indexed children
they are also sort of string maps and variants too

Solver:
 - We want the solver to be able to wait on children of a specific kind, type, members, etc
 - Almost want paths, or relative paths, to a specific node
 - We want these dependencies to be easily analyzed, so we can track cyclic dependencies and have better errors
 - We can still run manual code to check dependencies more specifically (a test function)
 - Some tasks can run on nodes with no dependencies, basically what it looks like when we do => at the end of a rule or production
   - That defines a task to run when the production completes, but it has no dependencies
   - I guess you could say it has a dependency on the node's existance
   - If someone else runs before and deletes the sub-tree, that task shouldn't run anymore
 - So it just has a dependence on the completed "sub tree root" node itself
   - Kind of the same feature as the cancel, tasks can have conditions in which they self cancel
   - One of those conditions is basically always that the task's associated node gets deleted
     - But maybe we can expand this concept to require multiple nodes, etc

 - Generally when I look at the types of zilch walkers, we really just needed the basic ones that ran on the nodes themselves
 - And then the ones that ran once we figured out the types
 - 

Tasks should run in order of those with 0 dependencies, and the order they were scheduled
 - If it makes sense we can add a priority to differentiate within those levels, but I hate priorities
 - Dependencies make much better priorities
 - The run order makes sense, for example literal constant nodes would run first
 - Local variable references would try and find an associated variable up the stack
   - Is that a dependence?
   - I imagine in Zilch I had something where as I walked down, I pushed things onto a scope stack
   - We could also search the tree, but it's a bit annoying as it involves searching 

{
  let a;
  {
    let b;
    a + b;
  }
}

The reference to a here has to scan up it's parent until it find's any body that can hold local variable declarations
 - Then it must scan all the statements to find a local variable reference
 - I believe in Zilch we did a few passes, one where local variable declarations added themselves to scopes
 - And another where as we walked down, those scopes pushed onto a stack where we could look them up easily
 - This would be a really interesting language toolkit thing, but how do we solve this with our nodes?

`let a` could create a task that only has a dependency upon it's parent
 - That task could push itself into some sort of parent array of in scope items
 - But that's a hard thing to depend on, ideally someone else would want to depend on like "all scope items complete"
 - But that's not a concrete thing, that array would just keep getting filled
 - So maybe another feature is needed here, this is kind of like passes, we need to make sure an entire pass is completed
 - it's like having a dependency on a group, maybe that's the better idea
 - we can group tasks under a single group (or group name) and you can have a dependency upon that group being completed
 - That sort of eliminates the idea of passes, and a group is really kind of just an "and" dependency
   - Maybe that's it, our dependencies can be an expression not just a node
   - And for convenience, we can just label tasks with a group name and depend upon that group names (automatically depeneds on all those tasks)
 - rather than passing scope down as we walk down the tree, because we no longer "walk down", we just execute tasks
 - then we really just need good tree find functions
 - So 'a' really needs to make a task that's dependent upon the scope it's within
   - But that's not an immediate parent
   - Maybe dependencies can be fluid like that, they can output a node...
   - Like dependencies are re-evaluated every time, parents can change, etc
   - And we can build a dependency graph from evaluated node pointers
 - We only need to re-evaluate these when the graph changes, maybe we can keep track of who needs re-evaluation instead
 - I like this as the primary thing
   - Maybe we can even use the traversal's in our dependencies
   - When we add, delete, detach, reattach nodes, we need a way to know who needs to be updated / notified
   - Maybe when evaluating a dependency, we mark all nodes in the path used to get there
     - And if we change anything about those nodes, we know who needs to be updated

I can imagine that tower nodes can actually be pretty complicated, because they know who needs to be updated if they update

I'm imaginging that every time we evaluate a path, we record all the nodes used to get there
 - paths are evaluated left to right, on a find first basis
 - if the path can't be completed (e.g. nodes don't exist) then the last node we found we register a "waiting for this"
 - that way, if the node is ever modified we know who needs to be updated
 - we should call those 'partial evaluated paths' or 'complete evaluated paths'
 - when we modify the structure

this seems cool, but for now lets go with a much less complicated "ask the tasks"

yeah, part of the point of tasks was to say what they output too

Ok maybe dependencies are just functions, but they are expected to output evaluated paths
and since we actually evaluate node, the result is one or more (remember conditions) of evaluated paths

evaluated path:
 - the most recent node* we found
 - the rest of the path that we didn't get to

paths should always be normalized as they will be compared

so when we attempt to ask the + operator if it's task is ready to run, it checks all it's dependencies
 - it may have lhs and rhs, but maybe they haven't evaluated types yet
 - so it returns the most recent pointer to lhs, with a remaining ".type" to be evaluated
 - another task can say that it outputs "self.type", and since it's the child node
   - we evaluate self easily, but ".type" has not been output yet so the partial path is emitted
   - We can match up the dependency with the output to build a graph
 - dependencies and outputs are parts of tasks that we always ask for
   - it may fluidly change with each iteration, so maybe it's best if we don't try to make an efficent graph for now
 - at some point we can make optimizations to the solver algorithm if we need to

the tower_find functions should output partial or complete paths
 - if it's complete, we keep going, if it's partial, we stop and output that (like a rust Err check)

instead of saying ancestor's immediate children, why don't we just allow find to evaluate paths
 - so basically for every node, in the find order, it will attempt to find another path
 - first to evaluate wins

+ operator:
  dependencies:
    self.lhs.type
    self.rhs.type
  outputs:
    self.type

local variable reference:
  dependencies:
    find(up, kind(scope), find(children, kind(local_variable), .name{a}))
  outputs:
    self.type

there's not one single scope, so I guess when we evaluate the partials we need to output an "any of these"
 - the whole point is better error messages, and basically what we're trying to do is point out all the places
   that we might find a local variable
 - I almost think it's better if we maintain some sort of scope concept
 - Maybe this isn't really a dependency, it's not something we really output and say hey we're waiting for this

What would scope look like as a concept
 - maybe there are scope types that are just string identifiers
 - a local variable declaration would add itself to the nearest parent scope
 - we'd need to define if the language allows shadowing, all that, kinda gets messy
 - we could always just do find first / nearest, and implementors can make separate tasks to emit shadowing errors
 - this is also where "using" could come in
 - basically scope is a short hand symbol table
 - I kind of like this concept, especially since they are just nodes in the scope table


dependency:
 - on a path
 - on a node
 - on a group of nodes
 - on a group name
 - on a and/or condition
 - find(order) + path


tower_task(target: node*, test_callback_no_side_effects, callback)


Maybe we also check outputs, and if an output is satisfied, then we don't run it / cancel the task
 - can be an option on tasks

tower nodes can be registered to act as scope collectors
 - tasks can add to the scope
 - local variable declarations could just walk up to their parent scope and add declarations in
 - makes sense, since:
   using foo;
   bar();
   using bar;
 - Here, bar should not be visible, so we should do them in order (top to bottom)
 - Technically just by parse order we will visit things in this order
 - Using declarations should just bring in weak pointers to nodes
 - basically we want a tree that is just visible scopes
 - technically some of the things may not be AST nodes, they could be just symbols
   - that's really what we want types to be

What things do tower nodes need?
 - the start and end indices from parsing
 - the string they parsed? not really, can technically get that from start/end and the source str
   - but a helper to get that string would be really good
 - ability to store values and constants
 - A tower node has children, but it's sort of a variant too
 - how do we know what an "expression" node is? is there a type string on the tower node?

 - I almost want tower nodes to attach whatever binary data they want
 - In json, an object only has children if it has a member for children

 - Start/end for parsing, when we create a new node we may not have a start/end
 - But we can require it for all nodes, it might make it easier
 - Technically all nodes should be derived from some sort of source, even if generated
 - Should nodes just have a pointer directly into the string? I kinda like that, and a length


rust separates out traits and structs, do we even need to?
 - what if there were only traits, and traits could have data members, and you can implement traits for other traits
 - what does this look like
 - this means that our sizes of our structs change as we compile more things in
   - meaning that loading code at runtime might be odd since it causes structs to change size
   - maybe if an interface needs data, we have to create it with & including that data

trait Enemy {
  lives: i32;
  health: i32;

  fn hurt(amount: i32) {
    this.health -= amount;
    if (this.health < 0) {
      --this.lives;
      this.health = 100;
    }
  }
}

trait Named {
  name: string;

  fn hello() {
    console.printline("hello from `this.name`");
  }
}

trait Health {
  health: i32 = 200; // can have defaults
}

let e = Enemy {
  health: 100,
  lives: 3
};
e.hurt(12);

Enemy {  lives: 3 }; // error, no health
Enemy & Health {  lives: 3 }; // ok, since Health has a default

let h: Health = e; // ok, because even though we never implemented Health, it implicitly matches as an interface

trait Debug {
  fn print(self: Self);
}

impl Debug for Enemy {
  fn print(self: Self) {
    console.printline("health: `self.health`");
    console.printline("lives: `self.lives`");
  }
}

e.print();

// Since Debug is a functional only trait, and more importantly Enemy implements everything Debug does, then it shows up
let d: Debug = e;
d.print();
d.lives = 5; // error

impl Named for Enemy {
  // override / custom implementation
  fn hello() {
    console.printline("roar I'm `this.name`");
  }
}

let n: Named = e; // does not work, since Named adds data and e was not allocated with that data

let e2 = Named & Enemy {
  name: "boss",
  lives: 5,
  health: 1000,
};

let n2: Named = e2; // legal now

let eref: Enemy = e2; // sliced off Named from the type
let nref: Named = eref; // dynamically pulls the trait/interface out

and we have 'instanceof' and the other kind of same stuff


I do really like the idea of anyone within the same module / crate as being able to add data without requiring the allocation
 - but we'd have to know everything up front
 - we'd also have to have default values, what are they allocated as?
 - it's more explicit if we have to force the user to allocate the types
 - but I really like that we just don't really have types


It's possible that we effectively get the diamond of death now:

impl Test for A;
impl Test for B;

let x = A & B {};

impl Test for A & B; // can we have this? I don't see why not, we're being way less strict with types
 - it would have to be something where we basically normalize a type (meaning all A & B are the same)
 - and we then have some specificity rule, eg "x as Test" will use this
 - what about like, A & B & C? ideally since A & B is more specific it will use that logic

let t: Test = x; // here this would be an error, or maybe we have some algorithm for choosing which one
 - We can try and enforce the rust style rule for who gets to implement
 - I don't mind some rule about choosing one, and it involves "closeness" to the call site
   - e.g. we pick implementations within the same module first,

Technically we have two different problems:
 - two different people from different modules implementing a trait for a common type
 - but also since we now allow union types, union types introduce multiple of the same trait implementations
 - Maybe the order of the union actually matters, e.g. A | B, vs B | A, it just means when we dynamically
   pull off an interface, we'll go in order of the types as declared when we constructed it
   a = A & B {}
   a = B & A {}
 - I still think we're going to end up in scenarios where it's not all that clear
 - we can always double cast for explicitness, e.g. `a as B as Test` or maybe even `a as B:Test` (B's version of Test)
 - technically we only care about call sites where we extract the interface, but that call site could be in internal code
 - but maybe we care about who is calling... like the actual call stack?

 - I actually don't mind the idea that it's based on the order of A & B, or B & A, when allocated
 - the & forms it's own tree, and we basically just go order first, (e.g. if A is also a combo)
 - we also go most derived first, e.g.


trait A implements X {
}

trait A implements Test {
}

trait X implements Test {
}

trait B implements Test {
}

// Can do this
trait A implements X & Copy {...}
// Or separately
trait A implements Copy;

the only question becomes if we get inherited data, because previously we said that you had to do A & B
 - but I don't want you to have to always write Player & Component {} to create a component
 - this is why data gets weird, because if you have to be explicit every time...

Maybe we can allow you to add data, but if any instances of a class exist then you can't

adding data I think would annoy people, since we can suddenly add data to primitives and standard library types
 - which is cool, but like, also annoying
 - but also cool in its own way, just bringing libraries in might be kinda gross


let a = A & B {};

let t: Test = a; // Here we get A's version of Test, since A is first, and lowest on the tree

and we can't have cycles of trait impls, e.g trait Test implements

it's a little more loosy goosy

how do we decide the whole class/struct thing? if all we have is traits...
 - we can determine if something is implicitly copyable pretty easily (only primitives, no handles / ref counts)
 - we can use & or ref, but I wish we didn't have to, I don't like extra syntax if we can avoid it
 - maybe we can say on a trait, and when we use that trait
 - or maybe all traits are allocated, but we can say "implements Copy" or other special traits
 - And that means the value is now copyable, but we can't have any other non copyable values
   - we can't add any non-copyable members then
 - I don't like the idea that something we do can perminantly change usage everywhere
 - or fine, if you suddenly turn a trait into Copy, we have to update all places in code

The whole idea that we change code and it changes everywhere makes all sorts of weird problems
 - we basically can't compile code in a static way because new members have to be addable (and defaulted, otherwise it breaks code)
 - that means you really can't ever "compile" a library, we're always going back through when types are modified
 - that is unless we make it some sort of dynamic type, but that defeats the purpose, we want like 80% native performance
 - adding members would change sizes, initialization code, etc, it would have to be recompiled everywhere it's used

why do I want that model?
 - basically it's the same as having ability to implement traits for any type and having collision resolution
 - but that happens at runtime, and potentially we can make that into a call so we abstract away the behavior and don't need to recompile
 - e.g. the wasm stays exactly the same since it's all dynamic dispatch as a boundary
 - but suddenly when we change sizes, the actual code itself has to recompile, especially since initialization logic has to run
 - unless initialization logic is like pre-constructor in zilch (sets all values to defaults)
 - but still sizes change, and wasm doesn't have structs, so you really have to do structs manually

we also could get clever and cache instances of compilations, so we don't need to keep recompiling because we know the size
  hasn't changed (but if you change that struct, it causes recompile, just basic minimimal recompilation)

any time dependency-libraries change dependent libraries (e.g. my lib changes std lib) causes recompile, all parts of the tree
 - you can dynamically load libraries into an existing runtime, but if they change parent libraries then it requires recompile
 - if there are existing types... well
 - we could do the zilch thing where we rebase and reinitialize all heap types / new members
   - would have to support default initialization always then

is there ever a case where the standard library needs to recompile
 - I'm not a huge fan of the fact that adding Copy suddenly changes how things compile
 - like previously even if I change sizes (which would mean we need to re-emit bitcode)
 - it did not cause me to re-check types, as we we're only able to add to types (and you could say the whole it has to be in scope thing too)
 - 

if we were able to add to std library traits, can that ever cause a compile error or ambiguity in the standard library?
 - yes in that

module Std {
  interface Test {
    fn test(): i32;
  }

  interface Standard implements Test {
    fn test(): i32 {
      return 1;
    }
  }
}

// Standard brings in Test because the only definition we see of it shows Test
// basically the only ones that see our new additions are those who also see our module

trait Foo implements A & B {} // error, it's known at compile time that these conflict
// Maybe this syntax makes less sense anyway, we're trying to jam two things together (which is which?)
// or maybe A & B is basically a unique type

interface BetterTest {
  fn test(): f32;
}

interface std.Standard implements BetterTest {
  fn test(): f32 {
    return 123.0;
  }
}

// but now this is an error
let s = std.Standard{...};
s.test(); // error, ambiguous, which one does it call? since both are in scope and only change return value
// we can make it so overloading always requires disambiguation for now, or can make some sort of select algorithm
(s as BetterTest).test();

trait Foo implements A {}
trait Foo implements B {}

so generally no, adding a new member will never cause a compile error because the base module doesn't even see it
 - even when it adds size
 - but maybe we can only say implements Copy in the same module it first appears in
 - so some traits that control compiler details must be implemented at the declaring module level
 - means we need to code-gen again, but not semantic analysis
 - note that operator overloading should still work since a + b is just a.add(b), so implementing the add trait is easy

we can only declare copy in the declaring module, but anywhere within the module

right now we can basically do single dispatch with dynamically pulling out interfaces
- e.g. virtualism: sphere prints radius, cube prints extents, etc
- maybe we do allow overloading, and our overloading technique can handle dynamic dispatch
- and since anyone can implement the trait anywhere, free dynamic dispatch

trait Shape;

trait Collider {
  fn collide(a: Shape, b: Shape): bool {
    return false;
  }
}

trait Collider {
  fn collide(a: Sphere, b: Box) {
  }
}

// since there's no self we can call it like a static...
Collider.collide()

collide(a, b) // pick the right version of collide?
// we have to use some algorithm for specificity, but it's cool to provide it
// overloads can just be a way to provide dynamic dispatch, and since anyone can implmement the overload anywhere...

since we have union types and all that, 

What if we do everything like this syntax:

type Collider {
};

Ignore all the += stuff, basically what we're saying is that any {} is a type like in TypeScript
 - but we alias it with names for convenience
 - but also, name aliases act as the group holder for interfaces

Can we grab any interface "just because it works"?
 - well if it imposes no restrictions on the type (it matches) and has default functions, then yes
 - in typescript interfaces don't have defaults (function implementation, default values, etc)
 - so it's not a worry since casting interfaces is always checking that data matches
   - it only makes sense for an interface with function defaults, but members/data make no sense

The way we do type and interface mapping should be entirely by which members fit
 - so for example, when we say type A implements B, what we're really saying is here is an implementation for B
 - that works with anything that looks like an A (it can even have more members than an A, e.g. A & C, or just a new type D that quacks)
 - That means that type names are actually irrelevant, we just use them to identify types
 - so a big operator we need to implement is "does this interface / data layout match this interface"
   - or is there an implementation of it, or can we cover 
 - for allocated objects, why don't we make the pointer at the base of the object able to attach components / new interfaces at runtime
   - component based design basically
   - nah I think rather than making that something we just magically do for users, I like just overloading the operator
   - we can build a standard Composition interface

when we do implements, all data members must be initialized

type X implements Y; // that's just a statement basically, Y must be complete

the component idea is basically instead of actually extending the type with new members, we attach the members as a component interface
 - but then our language would need to bridge the gap between "owner" and "self"
 - I guess in interfaces, that could be a thing, self means the interface itself, and owner means the one composing it
 - But, I think the part we're missing here is that we don't just want to be able to get C# style interfaces where a thing "IS" that interface
 - we want it to work where anything that can be, IS
 - true duck typing, but still very type safe
 - and use strict style TypeScript for explicit null checks, eliminate that issue
 - and add some sort of cool match expression in to match interfaces and destructure
   - we can play to rust strengths

// in type parsing:
// Y {}
// means taking an abstract or complete interface Y and implementing it (all functions must be implemented)


type Collider implements Copyable;

// legal syntax
type Collider implements {fn foo();} {
  fn foo() {
  }
}

we could even do

type {health: i32} implements {
  print(self) {
    console.log("I have `self.health` health");
  }
}

let monster = {
  health: 100
};

// Now any type with a health: i32 matches the definition and gets a print function (if this is in scope)

So that means we should be able to basically do this


let f : {fn: print()} = monster; // legal cast, since we implemented print and it's in scope

normalize all types and make them all comparable



Basically as a parsed type, {} is a trait/interface/struct

traits are all basically compared purely on "if it matches it works"
 - which means that type names are really only a convenience
 - but no, we really do want type names to be a single trait, since we want to attach things to it later
 - and 

I can extend a type anywhere

type Collider += A + B;

anywhere that has an unimplemented interface needs an in place implementation, maybe that's just a { } following type expressions

think of this like JSON and typescript

I want to be able to do:

let a = {
  a: 5,
  test: (a: i32) => {
    console.log("a", a);
  }
};

typescript interfaces are all compile time, there is no "dynamic cast" which is unfortunate
 - we'd love the data to be validated and to fit within the interface

typescript/javascript have classes, and that has instanceof, but that's only a single chain of inheritance check
 - basically what we want is data compatability checks and a sort of as
 - but functions make that kinda weird, since it's like "static data" (kind of the same as two constants)
   - ok so this is a new paradigm, where we can pull out interfaces dynamically based on the data
   - it's sort of like, imagine taking every allocated struct with all it's data members
   - and then meticulously go through all named interfaces and generate a cast if it can be
   - and the confusion comes from when we can have more than one implementation of a thing

we should also be able to implement the is/as/has operator ourselves - component lookup
 - only called when the built in interface lookup fails

// typescript
let a: number | string;
if (typeof a === "number") {
}
let b = a as number;

if (a is number) {
 // a is a number here
}

now ast nodes would make more sense as just compositions that hold interfaces
that seems a lot nicer

basically now we just need to optimize interface lookup
 - and ideally anything that can be statically figured out should be

maybe a "fat" handle to a container locks it, and modifying it means 
 - containers can implement returning a 'fat handle', maybe just an i32 + handle to container
 - any existing fat handle locks the original value (can't call any mutating interface?)

we always return a vtable basically for an interface

so part of our language's algorithm should be to match up vtables
 - but the weird part is that we can extract them even not knowing if they implement that or have it
 - sort of makes sense, ultimately you can in typescript and javascript
 - in C# you can pull any interface off a class, or any derived class (or null if it doesn't implement it)

we do need the allocated object to have some sort of vtable pointer, so we can recast
 - basically we just need to know with any piece of data sitting wherever it's allocated, what is the type
 - because we may be pointing at the data through a random interface pointer and need to change
 - so it's like, what's the actual data type, the memory layout, etc (member types)
   - in a way, do we even care about the type (e.g. any methods) or is it really just data?

lets imagine we're making a player and an enemy, right now the only difference is what they say

type Player = {
  speak(self) {
    ...
  }
};

// OR

// This is partial because it has no implementation for speak, so speak must be implemented (no default)
type Speaker = {
  speak(self: Self);
};

type HasColor += {
  getColor(self: Self): i32;
};

type Player += {
  name: string;
  speak(self: Self) {
    console.printline("Hi my name is `self.name` how are you?");
  }
}

Ok this whole idea is stupid and flawed, we're not unioning types when we implement an interface for a type
 - implementing an interface for a type is fundamentally different than extending the type
 - i'm implementing an interface FOR a type
 - in C#, only the class itself could do this
 - in TypeScript, there is no such thing really, it's all dynamic under the hood
 - what I really want is that "anyone who looks like this can fit in this interface"
 - but interfaces can also be allocated, it's not like C# where they aren't full types
 - so in many ways, no difference than structs (data) and functions
 - but then we ask what it means to implement an interface for another interface
   - especially because interfaces can have full function implementations in them
   - 


let t: HasColor = p; // error since Player does not implement getColor / does not match the interface
uld implicitly cast to Speaker because of the speak function
type Player += Speaker;

type Player += HasColor; // error, missing implementation for getColor

type Player += HasColor {
}; // error, getColor was not implemented

type Player &= HasColor {
}; // error, getColor was not implemented

When you write a type expression, and then you put {} after it, ALL must be redeclared, no partials
you can re-declare without an implementation however (e.g. fn foo();)

type Enemy = {
  health: i32;
  speak(self: Player) {
    console.printline("arrrrgg I only have `self.health` hp!");
  }
// technically OK, but a little silly since player already implements speak, this is basically a NOP
// Player already co
}

let e = Enemy { health: 100 };
let p = Player { name: "Bob" };
let s: Speaker = p; // implicit cast to an interface


// basically, we want our language to be almost as easy to use as a dynamic language with little casts or anything
// but still as close to native as we can get with eliding and assuming as much as we can
// so a 100% safe language, but running as fast as we can make it go without unsafe behavior (like a scripting language)



Lets come up with some examples of how we want interface selection to work

type A = {
  health: i32;
};

type A implements {
  speak: fn(self) {
    console.log("hi `self.health`");
  }
};


// In the same module or if the above was in any of our dependencies, this would be an error
// Rust avoids this by saying you can only implement your traits for other types, or other traits for your types, or if they're both yours
// We could adopt that same rule, but it doesn't prevent the case where there are multiple interfaces that satisfy the same thing
type A implements {
  speak: fn(self) {
    console.log("yoooo `self.health`");
  }
};

if the above were in another module, now we have two conflicting definitions of speak
even if we gave the interfaces names... they are compared member by member
 - maybe we could say something like, if a type is named, then if you pull it out by name it will attempt to pull a complete
   implemented interface registered to that name first, and then member by member (duck typing)
 - I kinda like that, it's a little odd but it's complete
 - but I'm also not sure I like the idea of giving names more meaning
 - or we could say something along the lines of, an exact interface match when doing implements/runtime lookup
   will always prioritize over member by member matching
 - I mean, we could even start with that for a long time, implementation's only (exact matches)

 - the beauty of not just doing typescript style interfaces is that we can implmement interfaces for other classes without
   necessarily adding data
 - like in TypeScript, trying to turn a type into an iterator means most likely wrapping it in a whole data class
 - but here we can probably just implement an iterator interface (no extra allocation, it's all static)

all type expressions have a defined order that they walk to pull out interfaces

basically what we're saying, when you pull an interface out of a type, you're really just doing it member by member
 the fact that you said A implements B, and then you pulled out B later, it's not actually pulling out B, it's pulling out
 every available member by matching name and type

So it means that we basically need to worry about making members and their types unique
 - because we're saying if the members match exactly, then they are considered as implementing the interface
 - conflicts can become pretty strange, I think

Ok so the algorithm first prioritizes exact interface matches first
 - and since we want anyone to extend from anywhere, and we are ignoring type names, then


hmm, if we do it this way we're basically tossing out type names (they are just aliases)

it's also a little odd since we're saying X implements Y, but if Y adds members to X, does everyone
who creates X automatically have to allocate the members of Y, (any type that looks like X?)
 - it makes more sense when we're using type names and it's not just "anyone who matches"
 - X implements Y, X could always have to be a type name and not just a type?



module X
A implements B

module Y
A implements B

module X
casts A to B, we should get module X's B implementation

module X
calls into std code and passes A
std code casts A to B, we should get module X's B implementation

basically we want the "closest to the call site"
 - since STD has no implementation and sees no implementation
 - module X was the last caller that had a clear result

one reason we want to do it like this where names are just aliases is because we already have type expressions
 - and it's weird to have full types with names, and then also a random expression language

we could require that all things we want to "impl" we have to alias with type names
 - like we're attaching to the type names themselves, not to the types
 - then we could enforce the rust rule about impl

I think the closesness to callsite example would be best
 - or we just make it an error for two modules to be imported that both make the same change
 - maybe you just have to resolve it, or maybe it's resolve automatically by module import order
 - kinda sucks but it's less runtime than "by the call stack"

what we're basically going to say is, hey we have this allocated interface with members laid out in whatever form
 - are there any exact matches to the interface that we're looking up
 - if there are more than one exact matches, which one is closest to the call site
 - ideally we do this all in constant time
   - every module has a table that is "if you ask X for Y, this is the Y (or none)"
   - we just walk up the module call stack

if interfaces couldn't add data, (but could add getter/setters) then this might be ok
 - it's not weird because we're never actually allocating anything for them
 - basically when we go to implement an interface that has a member, we can do it with members, or with getters/setters
 - that also eliminates the issue of how we set the values of those added members (default initialization, pre-constructors, etc)
 - in rust traits don't need that because they don't have members

maybe I drop the idea of extending other types with members
 - because without separating data, it complicates that question

something like

type Bridge = {
  length: i32;
};

type Bridge implements {
  height: i32;
};

It looks like I just added height to bridge, but really if I stick to my definition of the language
then I technically added it to ANY type that looks like a Bridge, e.g. has a length: i32
That means it suddenly might add that height to arrays

Maybe this is a reason to use type names
 - We can use type expressions everywhere, but we're encouraged to use type names because they hold the interface bindings

OR!!

when we do type X implements Y {}; you're only allowed to add members in the implementation if X is a type name
 - otherwise it has to be a getter/setter
 - note that means when we do interface lookup, that's a specific scenario, hmm actually not really
 - it's nothing more special than looking up an interface on a type, but we just need to know the TypeName itself has its own that we also need to look through
 - yeah that's it basically, a shared type may have other interfaces registered for it (if it looks like X, it also implements Y)

ideally should have no difference between closures, methods, and functions
 - functions are effectively just statics
 - static means it's part of the type, instance means it's a data member (like a closure)
 - by default it's instance mutable, vs static const
 - normally in C++ a static method meant it's part of the type, but the only reason we had to do that
   was because you don't declare this/self, so static means no this
 - But here it's static because we only need one definition of it
 - single vs shared vs static, which one, unqiue? type (since it's part of the type?)
 - per_instance vs per_type

 - visibility is just a function too, takes type ids
 - default is public (all access allowed)

type Player implements {
  public unique const max_lives: i32 = 64;

  unique const speak = fn(self: Self): void {
    console.log("hi", self.length);
  }

  fn speak() { // shorthand for unique const speak = fn(): void {
  }

  private instance mutable length: i32;
};

If you only ever use a class on the stack, we should find a way to avoid allocation


// this is saying there is a data member foo that is default initialized to a closure

type Foo implements {
  instance mutable foo = fn(a: i32): i64 {
    return (a * 2) as i64;
  }
}

it means it also can be changed
does const imply static/shared/single/unique/type?
 - because what is the point of having a const
 - well really const is just saying the interface only supports a getter for this
 - so if it's a const instance, that means it's a getter

 and any const static function that takes self will appear on the interface too
 since we can access static members from an instance too, then 

a = {
  mutable instance speak: fn(): void {}
}

type X implements {
  fn speak() {}; // this is const static
}

let x: X = a;
 - this is OK?
 x.speak();

I like how simple this language is
lets talk about safety, is it all going to work

how do we safely point at stack types?

we want to be able to basically pass everything by pointer, including structs
 (except small enough data types / primitives)

if I have a Player with a position, I want to be able to pass it in to every function as a reference

the only way that's safe is if either we "lock" the object so it can't be deleted
or we have to increment the reference count

Lets not support "delete", just remove all references if you want to delete
 - weak references

that would mean we always pass fat pointers, basically an owning type that needs to be ref counted 

we can never declare or hold a ref to a copyable/value type
 - it's ok because as long as we inc ref before entering a function, and we have no delete, we're guaranteed it stays alive
 - ideally we can do optimizations to reduce the inc/dec (like if we can see a whole function does multiple)

And we can safely take pointers to structs on the stack because we can't store it (it always copies)
 - so passing values is always by pointer unless it's primitive

What about the whole read/write issue, before thinking about threads, lets think about containers and iterators
,
Can we ever store pointers to members
 - fat pointers yes, because we could store a ref counted pointer to the root of the object and an offset
 - For value types, we always take by pointer, but maybe we can say like "ref value" to mean take a fat pointer
   - If we just have a value, we can't store it in a ref Value

For threading:
 - Maybe it's ok to pass or share an object
 - we can clone/serialize objects to pass over boundaries
 - but also any object with ref-count 1 can be transferred
 - sub objects can have different ref counts, do we need to traverse all child objects?
   - basically almost the same as serialization but only checking refs (can even have cycles and > 1 count but must be contained)
 - maybe non-const statics always have a mutex?
   - ehh, but we can't ensure it doesn't point at something else shared, hmm well actually we can because it would have to grab another static
 - someone could allocate an object and point at a static's member

really just JS transferrable model with workers, can't really think of anything better
 - maybe we can have some specialized containers that support threading, maybe memorycopyable values only (no refs) etc


do we support slices too?

And is there any issue with copy values having ref counted values
 - not really, but we may want to know if a value is MemoryCopyable vs Copyable (optimization)

we can have a ref, but not a ref to a ref, otherwise we would need a pointer that keeps multiple things alive
 - we can support pointer to member syntax so you can build whatever kind of fat pointers you want

moving a ref counted value does nothing, right, since the ref count stays the same, the pointer is still valid
 - can't move something with an internal ref
 - but we don't need to move anything really in this language

weak ref is kinda the way, but I don't like Weak<T>, I almost want T | null to just mean weak
 - but that's kinda weird, the user didn't sign up for that technically
 - but as a simple language I kinda love it, if you want it to be a weak reference, Player | null
 - now if all other references to player go away, this one will not keep player alive (it will go to null)
 - Player | Enemy | Targettable; // this would mean Player Enemy and Target are all weak references
 - type Targettable = Target | null;


function calls are syntax sugar
foo(1, 2, 3)
is actually
foo.call@(1, 2, 3)

call is a sort of keyword, because how does ( not expand again to a call
 - somehow we can enforce that it only expands once, but that's odd)
 - @() is a special function call syntax that does not expand
 - you can in fact write this syntax yourself, but generally it's hidden (same with `a.add(b)`)

a + b
a.add(b)

the only reason this matters is that all types should be compatible
 - a closure / function has a type, and that should be implementable by an interface

type Player implements fn(a: i32): void {
  fn call(a: i32) {

  }
}

let p = Player {};
p(5); // works!

use C# style generics, templates do not actually cause tree copying/expansion
 - we may choose as a final release step to allow templates to be instantiated and optimized individually
 - ideally, our language operates in this mode that in development everything is super fast to compile, like a scripting language
 - but in release, we run as many optimizations, inlining everything we can, eliding ref counts, really keep an eye on perf
 - I want it to be like 85% native perf, ideally beat C# with predictable and no GCs
 - as easy to use as TypeScript with sane containers, in Rust style
 - no invisible nulls, everything force checked, with match statements like Rust too
 - but super easy to read, and everything just makes sense and is easy to use
 - plus expand the language with ability to customize parse rules however we want
 - and compile time steps, evaluates wasm on the fly to get constants (plus WASI well defined way of running in compile mode)
 - it's gonna be a kick ass cool language
 - for each will be a feature you can include, whole parts of the language you can toss out
 - people will build crazy libraries on top, compiler is very much part of the runtime
 - all runs in browsers, all in wasm, completely safe because our language ensures safety, and because wasm sandbox

side note, maybe when you give something a type alias, type aliases cannot be assigned to each other without a cast
type A = i32;
type B = i32;

let a: A = 5;
let b: B = a; // error, a is of type A an alias for i32, but needs an explicit cast
basically compatable for all interfaces and things we can pull, but just that one case helps


the fundamental types we have are objects {}, arrays [], tuples? (), primitives i32, null?, string, etc, and functions/delegates

another type of object we can share in threading are fundamentally immutable types, like our string should be
 - that's how you could share string constants

how do we implement weak ref?

// great example, i32 should actually be a type/interface
// i32 can fundamentally declare members of a size;
// but this is a perfect example where we're saying Player implements everything an i32 is
// in our case, we could implement it all with just functions, or it could actually add the data member
type Player implements i32 {
  // technically because Player is a type name, it implicitly gets the member private value: $i32; without us having to
  // implement it manually
}

type i32 implements AddOp {
  fundamental i32; // special declaration that says this type 
  public fn add(rhs: i32): i32 {...};
}

I would love to see the actual implementation of i32 in the language
 - like how it can emit wasm instructions, everything, it would be so cool if the type could do all that


named types can be forced to implement members, but when it comes to i32, since it is a primitive it actually IS the value itself

i32 being a struct and having a private 32 bit sized member makes sense, but what's the type of that member?
 - where does the recursion end, like what if I try to say I implement $i32 (maybe that's just illegal... weird)
 - I don't like this, I'd rather just have some sort of keyword that's like this is a fundamental
 - I like that it's a member because it's always awkward to say you are a primitive
 - maybe we just reserve a keyword for members
   - it means the type must be one of the primitive types

type i32 implements AddOp {
  fundamental i32; // special declaration that says this type 
  public fn add(rhs: i32): i32 {...}
}

like somehow the "add" implementation here should somehow declare that it emits the wasm add op, I love this
 - I want to see the whole implementation of i32!!!

type i32 implements AddOp {
  fundamental i32; // special declaration that says this type 
  public fn add(rhs: i32): i32 {
    builder.emit("add", this, rhs); // something like this?
    // but this is odd, it runs at runtime or compile time?
  }
}

type i32 implements AddOp {
  fundamental i32; // special declaration that says this type 
  public fn add(rhs: i32): i32 {
    wasm(i32.add self rhs) // or somehow declare wasm inline
    // this way doesn't really let us generate wasm, this wasm is fixed (but at least works with params)
  }
}


maybe we can introduce some fundamental emit step for functions that's compile time

type i32 implements AddOp {
  fundamental i32; // special declaration that says this type 
  public fn add(rhs: i32): i32 emit {
    // when we do a compiletime function, we're saying we're going to emit the wasm
    // we get some fundamental inputs here, maybe we can declare but it's really just provided by the compiler
    // it's like a bytecode builder for a function
    // here, parameters are provided as named constant aliases for wasm, so I can use rhs but it's not an i32, it's like a WasmRef<i32>
    // or something
    builder.emit(Wasm.Add, this, rhs); // something like that
    // this will be run at compile time and the result will be checked to make sure its an i32
  }
}

THIS LANGUAGE IS SIIIIICK

basically the compiler uses compiletime evaluation wherever it can, types, functions, etc
 - I like that we can fundamentally use this with closures too
 fn(a: i32): void emit {}

emit could be like a compile time keyword, and it acts like a closure with refs to values

// Here we know the result of emit must be an i32, and it will do this all at compile time
let a: i32 = emit {
};

inline wasm works the same way, pretty much just short hand for emit

I do wonder if we should be using something more llvm like, with structs

lets just use the binaryen API for now, that's what we'll expose

emit is an expression, emit can be used in place of a function implementation

that means we can write fundamentally unsafe functions
 - maybe there's a "safe" version of wasm that uses no dereferences, or we do all the dereferences

basically wasm emit is a fundamental part of the language

we make binaryen part of our language too

we want to modify binaryen to add our ref counted / allocated types
 - so we can make optimizations to reduce inc/dec

sort of want to add structs in too, or at least just make them really easy to work with

really need to find a collaborator on this language

fundamentally the language should not get in your way with syntax, implicit everything where it makes sense

.t files

main.t

we register tasks for visting ast nodes
 - fundamentally running code at compile time
 - but some tasks can emit, during a pass

Maybe the way we emit is by attaching sub-trees to our nodes
 - it's not all one code builder, we build our little pieces and they get fundementally linked at the end

#BinOp = Lhs(Expression) + Rhs(Expression) => Lhs.add(Rhs) // automatically reparsed as a new tree

I love this whole language, it's so flexible and extensible
 - and we can easily bring in Binaryen C Api into WASM (and compile it all as WASM)

or compile it all in rust too

need to take a look at Mojo to see what they are doing that's different

I love that we can just bind binaryen and wasm stuff, as well as tower nodes to the compiler interface

dynamic indexing should definately be a part of the language

I never like how constants were passed around as almost their own language
 - like the fact that a template type could be a constant

like a.test could be a.index<"test">()

really we're just trying to at compile time find a member by the name of "test"

does that mean that some operators evaluate at compile time?

like a "compiletime" version of a function can be called when all constants are known at compile time

index should effectively be a templated function

but it's odd, it's type completely changes based on the string (because the member type changes)
 - this is a great opportunity to write some compile time code
 - I think other languages like TypeScript handle this with special syntax where you can index a type ["string"]
 - I want to do away with that stuff, it should just be more code, less language

if we want to write index within our own language (member access) how does it look?


type Object implements {
  fn index(value: string) {
  }
}

// Here we know we're evaluating an expression
let a = emit: i32 {
  // builder should be able to eval code in the language, as well as emit wasm instructions
} // means the result should be an i32

let a = emit {
  compiler.set_result_type(typeid(i32));
}

ahhh, so compiletime values are basically directly usable inside of emit code
 - if it's not compile time, then it's just an SSA address in wasm

fn test(compiler value: string) emit: {
  // If you leave off the type of emit, we can attempt to infer it from your compiled code like an inferred return
}

all compile values are visible in emit, I love that

Keep our generics simple, but template meta programming is just coding, way easier to understand

Anywhere that accepts a type we could also use "emit: Type {}"


I also like the idea that we can just leave off as much or as little as we want for an emit function
 - like we should allow any kind of parameter arguments, allow the users to test what was passed in
 - if they specify types then they are checked by the type system, but otherwise you can check them manually in compiler code


type Object implements {
  fn index(...) emit: {
    builder.arg[0] // says whether it's a reference or compile time
  }

  fn index(a:, ...) emit: {
    // a can be either a compile time or runtime value (can be used as a wasm value in both cases)
    builder.arg[0] // says whether it's a reference or compile time
  }

  fn index(compiletime a:, ...) emit: {
    // a is only a compile time value
    builder.arg[0] // says whether it's a reference or compile time
  }

  fn index(a: i32, ...) emit: {
    // a must satisfy i32
    builder.arg[0] // says whether it's a reference or compile time
  }
}

can an interface say that a particular function must be evaluated at compile time?

emit: Type {
}


the only thing I'm not in love with now is how our language operates at any level of performance if it's all interfaces
 - like i32, I'm really just saying I take anything that looks like an i32, add is virtual, all that

but maybe we're also talking about having an aggressive optimizer that inlines
 - maybe at runtime / development time everything is virtual, but at production time we optimize all output including inlines

or maybe there's a way to say we take concrete things
 - heck, even at development time we could do something where we generate two versions of every function
 - one with all parameters as interfaces, and one with them as exact types (if only type names are used)

maybe even as the development version, we still code gen if all types match exactly

I could imagine a syntax where we're saying like foo(test: @i32), where test must match exactly the i32 type, not a lookalike
 - in this case since we know they type is exactly i32 and not just "anything that looks like an i32"

we're basically saying all things are templates at that point
once we got to a call where we did foo(123) we would know that 123 is exactly an @i32, so we could call the exact version
 - it's bascically like there are points where we know things concretely (the types)

or if we did it rust style, separate data from interface
 - then functions could explicitly declare that they just take a struct instead of an interface
 - i32 would be the struct data

right now if I said

type Player = {
  health: i32;
};


// then this would be legal
let p: Player = {
  health: ObjThatImplementsI32Interface {}
};

But technically doing Player { health: 123 } is @Player (exact type)

Doing things the rust struct way is nice because we can easily just take the data itself
 - it's not an interface, it's just actual concrete data
 - and interfaces can be declared to work on that data (operators, etc) statically

what about the idea that there aren't value types, just automatic promotions from to heap objects
example:

let a = {
  count: 100,
  lives: 9
};

let p = Vec::new();

p.push(a); // we either need to copy a, move a, or share a reference to a

technically with our language only pointing at heap objects with offsets
every object should be movable, copyable, and heap allocatable

how do we do weak references?
 - technically adds overhead, but maybe it's small enough to not care

So the idea is for any allocated object, when we request a weak pointer we allocate a single object that has it's own
reference count (any attempt to get a weak ref to the same object returns the same weak ref count object)
 - weak ref is a heap object itself
the weak ref is lazily created, but takes an extra pointer of space on each object, so now objects look like
 - typeid (maybe, might be on the pointer), ref count, and weak ref object pointer

weak references only work on non copy types

what if it worked on all types, what if even stack types could have ref counts
 - it would work by the stack creating a single shared handle object (one pointer for it)
 - null it out when it's completed
 - could do it only if you get a ref to it, e.g. the stack doesn't generate one if it's never taken by ref

What if objects still had ref counts on the stack, and when they got unrolled/destructed if anyone still had a ref
it would exception

always a copy or move depending on what we can elide

let p = &Player {
  
};



if it's on the stack, then passing it to any function will elide whatever is possible
Maybe when we declare a type, we can declare if it's primarily by ref or value, so & can be implicit, maybe & just flips it lol
 - I kinda love this, it's awesome
 - could be one of those pieces of info that's only attached to TypeNames, since it's just syntax sugar And
   doesn't actually affect the type comparability, and also TypeNames are already unique types

How can we tell if a stack object needs a ref count and all that?
 - I don't want every i32 and random struct to be ref counted
 - only if we ever use a type on the stack as a ref, maybe & operator or ref (or maybe use ref and val)

let a = 32;
return a; // never passed as a ref, no need to allocate overhead


let a = 32;
foo(&a); // a passed by reference here, could all be implicit and no need for &?
return a;


fn foo(a: &i32) {
  let b: i32 = a; // implicit convert / copy
}

all types copied or moved as possible


let a = SomeBigValueType{};
v.push(a); // if that's the last reference to a, it's moved


Oh actually & is really cool if it just flips, because it is actually part of the type, not just a convenience

type Player = &{
};

Player {} // this will allocate because Player produces a reference

So when we say & types vs copyable types, what's the diff
 - Maybe this is where we differentiate the whole "exact type" thing we talked about before
 - value / copy types we don't do interfaces for, or basically we only take the exact object
 - or maybe we differentiate whether it's an interface or a concrete type by a new keyword, "like"
 - basically just dyn in rust but cuter I guess

fn foo(bar: like Player) {
}

I like the idea of "like" meaning any kind of interface
Or exact meaning only the type itself, copy types are exact only (must be a ref to be a like)

But I also like that code is basically always templated, if it looks like a duck...
And we use monomorphism / inlining
like and exact can be keywords you rarely see (like is implied)

Love it, allocating a like type produces an exact type
Player{...} always produces an exact player
 - returning a copy and then allocating it can move it



How does that work efficiently in rust, if a class is ::new() and returned by copy, and then put into a Box, we have to move it right? Or does it optimize to the point of allocating directly in the type itself (probably HIR/MIR and all) move is still cheaper, but not as cheap as allocating/constructing all in place
For now we won't worry about it, if rust can we can

When we get a ref to a temporary, we should have the option of ref on the stack or allocating

Hmm, in Rust there is no Copy because copy makes objects weird, except copyable types
 - can copy constructors be implemented?

Can copy be deleted or removed...

Maybe once you mark something as ref you can't unmark it, except in the crate it exists in maybe

So it's assumed all non ref types are copyable
 - maybe if you do the opposite, it can only be moved

I do like that copy types are exact types

We can also just have non-copyable be different, maybe ref types imply non-copyable but you can have a value type that's move only?

Mutex has data that is normally only modifiable when you lock and unlock
 - only get a ref to it when it's locked, ref count must be zero on unlock

All statics and global should be read only / immutable or mutexed

Actually when you unlock a mutex, all objects in the graph (mutex as the root) must be internally contained
 - graph walk? Hmm let's look more into thread systems here

Immutable objects really just mean no interfaces modify our own / self object
What does it mean to modify, do we introduce the const concept?

I don't mind having to mark mutable parameters as you won't need to do it often if we assume everything is mutable by default, or just self keyword is mutable by default

How does rust handle mutable / const iterators?
 - always hated this in c++

I do like the c# doesn't really have this, you just have to make read only interfaces if that's what you want
 - I like that, just easier
 - never really missed it in TypeScript

simple is the goal, less syntax
 - would be nice to know if an object is entirely immutable tho, not sure how to signal that

Match statements vs rust, it's interesting that their match is basically enum destructuring, and fundamentally no other way to do so
- basically have to store type ids in a pseudo union
- since it's typeid we should be able to use instanceof, but also match

Can we do value | ref type?
Well we should be able to | any 
Yeah, but non copyable infects so to speak

What about typescript isms like their control flow weirdness

Like instanceof fundamentally changes compile type

let a : string | Player;

if (a instanceof Player) {
// a is now player, a.attack() is legal
a = "foo"; // exception, is is locked as a Player (is this even legal too if a is now typed as Player...
}

I see, rust fixes this by fundamentally not allowing you to mutate while having a read only ref to 'a'

In C# instanceof is only pulling interfaces off objects which cannot be changed at runtime

But here | types are a bit different because they can be one or the other

My only thought is that maybe | types can't be assigned over (immutable once created)
Hmm kinda hate that but it could work

Or alternatively it's a little more runtime, type unions have an "instanceof" / match lock
- basically just a ref count of who has an instance of a particular type

When I say A | B (string | i32) just remember we need to specify interfaces or not (like string | like i32). What does like (string | i32) mean? Anything that looks like the common interface between string and i32

Can I promote a member or function parameter I don't own to be a like type? Or maybe an event sender for property changes?

Also for like types, when I declare a member am
I making a concrete type or like type. Might just have to do like as a keyword, and it's part of the type (anything like this).

type Foo = enum {
  A,
  B,
}

Is just syntax sugar for:

type Foo = u32 {
  static const compiled A = 0 as Foo,
  static const compiled B = 1 as Foo,
}

When we put statics on an interface is that part of the interface? Like does the type need to have the same statics to be considered an interface match? It sort of makes sense that a type could reimplement its own statics, but when you pull the interface with instanceof, how do you access overwritten statics, see that's kinda weird. 

Maybe we should treat statics like their own interface that the type implements? Using a Type name is basically like using a singleton instance

So we can have a static or shared keyword or whatever, but interfaces actually can define two interface, the static and the instance. Don't have to implement the static, but you can static({}). By default using any interface leaves off static members (not part of equality comparison)

I kinda also like the idea that static is a type expression keyword, so you can do {...} & static {...}
Using static on a member or fn is just syntax sugar for the above. static(...) extracts all statics

Hmm why do we like doing Player.staticmember When Player is a type. Could put those constants anywhere, but since it's associated with a Player it makes most sense to be on that type.

type Player = {
  instance_members: ...;
};

let Player = {
  compiletime readonly max_lives = 9;
}

Player has no actual size or any members
And you can now do Player.

Maybe we can do a shorthand

type X = Y static Z;

So static isn't part of type expressions, just shorthand for type names. Sort of makes sense, where is its storage? But can we just magically 'let' in the middle of a module to create the static type?

I like this too because now Player is both a type name and a value

Should start organizing this all into a spec, maybe hand it to gpt 4 lol!

Why a new language
 - memory safe, but also 90% "near" native performance without much thought
 - no garbage collection, deterministic performance
 - duck typing and interfaces / rust like traits
 - easy to build DSLs and custom syntax
 - wasm first approach
 - checked/ enforced semver with public interfaces
 - shading language? Need to really understand cuda / compute langs / mojo
 - typescript style interfaces and type expressions (A|B)
 - unique compile time features (visibility, compile, emit, etc)
 - crazy good inference
 - reflection

I also like the studio / easy gui interface solution
 - basically like you can make small prototype apps with property grid and some simple automatic Ui components - see anything in the app

Events and notifications for property changes
 - this is where we can now write super sick type wrappers that just replace all setters with a wrapper that notifies about property changes

So I guess we should start by making our own IR
And then lower that into wasm instead, can be very wasm based

So now what do we want tower nodes to be, if we have this new interface concept working
 - must support dynamic composition
 - or being forced to implement interfaces at least
 - dynamic seems better, it's not one giant bloated type
 - and we want to attach things like types
 - we can almost do bloated static style composition, just add T | null pointers to the tower node type for each T component type
 - I mean, it works surprisingly well actually and fast
 - We probably won't have that many and who cares about bloat
 - but if we wanted we could implement an operator to look up components by typeid at runtime (exact? Can it be efficient with all "like" interface queries?)
 - typeid.instance_of(typeid)?
 - can also maybe implement instanceof operator
I also love the component operators we had in Zilch, can do a lot of this with type expressions and implements + custom syntax (language extensions)
 - but that could also become standard
 - same with Zilch style event systems, all these should be types that are automatically generated and use implements cleverly to add members to other types

Has takes a compiletime typeid, any compile time typeid can be turned back into a type (for the return type too)

fn has(compiletime t: type_id): t {
  return this.has(runtime t) as t;
}

Or maybe shorthand

fn has(t: type): t {
  return this.has(runtime t) as t;
}

I almost wonder if typeid can be a typed expression to limit typeids to those that implement a specific interface, same with type()

Basically trying to get away from either complex templates or introducing template syntax

I Really love the idea that a templated type is somehow just a normal function call:

I want our templates to be:
let a : container(i32);

Because array is literally a function that takes a compiletime typeid (or type for short) and returns a compiletime typeid 
Like we said above that 'type Player' ... also has 'let Player' for statics.
So the idea here is while an array 

// leave off the return type so it can be inferred, but inferrence depends on the passed in type t, which is required to
// be a compiletime value, so it will all depend upon the instance when it is called and the t passed in
// since we return a typeid, and typeid is always a compiletime value, then the return type will be compiletime
// Because we're returning a compiletime typeid, this may be used as a type anywhere in the language
fn container(t: type) {
  return typeid({
    val: t
  });
}

// So now I can do
let a : container(i32) = {
  val: 123
};

// or shorter
let a = container(i32) {
  val: 123
};

// Can also do

type container(i32) implements {
}

// how do we do this...
type container(T) implements {

}

in my head this is almost a case for code gen
normally in languages like Rust we can say we're only implementing this for T where some-condition
and the compiler handles the details of who all that actually gets implemented for based on who has what and satisfies what interface
 - can we use like types here...

// does that mean anyone satisfies it, because it has no members (basically 'any' can be a keyword for `like {}`)
type container(like {}) implements {

}

type container(like Player) implements {

}


Lets try this experiment, evaluate container(like Player):

fn container(compiletime t: typeid(like Player)) {
  return typeid({
    val: t
  });
}

so this returns a type like this:
{
  val: like Player
}

Technically an 'exact' that accepts anything with a member val who looks like a Player

I guess my question becomes one about container(like Player) vs container(Player)
 - if I impl for container(like Player), does that impl also for container(Player)?
 - I assume so because I'm basically saying anyone who looks like this
 - or maybe I need to use like here... hmm!!!

type i32 implements {
  // whatever
}

// can we do this? anyone who looks like this implements this
type like i32 implements {
}

maybe thats the difference between how we implement interfaces

Technically here then, container(like Player) is an exact type, so we're saying no implement it for that type


Ok I'm still thinking that type names introduce exact unique types, they're not just aliases, and we can also attach interfaces to them specifically
 - or maybe when it's an exact you have to make sure to "capture it" in a type alias so that everyone refers to the same one... yikes
 - I like the idea that you're saying you're implementing for exactly this type (allocated) not just a lookalike, but that can be anyone who looks "exactly"
 - So type names introduce a uniqueness concept


Note that means that container(like Player) used in two different places technically runs and creates two different types, but they are equiavalent and therefore shared
 - typeid must share them automagically

type container(like Player) implements ... // means anyone who looks exactly like container(like Player) gets this

type any = like {}


We could introduce Template syntax but just for easy container cases

We kinda want to say that we only accept types that implement a specific component interface like CogComponent. C# generics uses concepts or traits or whatever it's called, rust has similar "where" clauses. 

type Cog implements {
  readonly Player { // getter shorthand with self
    return self.has(typeid(Player));
  }
}

Cog implements an instanceof operator or just its own .has, really whatever. I kinda like in a way this feature isn't a "built in operator". We can just make it our own operators.


Q: is it type names that implement interfaces, or exact types? Now that we have the exact type concept I wonder if type names should go back to being aliases, and exact types are unique.

alias A = {
  a: i32;
};

type A = like { // is this ok, should type names always be exact types? I guess it sort of works still
  a: i32;
};

type A = {
  a: i32;
};



Investigate MLIR, and maybe talk with River: https://llvm.org/devmtg/2020-09/slides/MLIR_Tutorial.pdf


Do we want slices?
Start with npm as our package manager
Tests built in like Rust
Destructors / Drop trait

Rust traits can have sub types:
  impl Add for Point {
      type Output = Point;
      ...
  }

In a really weird way, we can just output a compiletime typeid
 - I guess in reality all of our type expressions are really just syntax sugar for compiletime typeids

type Player implements Add {
  static Output: compiletime TypeId = typeid(Point);
}

Really need to organize this all into a spec to keep track of features

Topics to cover:

https://doc.rust-lang.org/reference/introduction.html
https://doc.rust-lang.org/stable/book/

Intro:
  Hello, World!
  Show off templates
  Show off bnf rules / foreach
  Implement a simple program

The base:
  BNF Grammar Rules
  AST
  Tasks
  Compile Time

The language:
 - Lexical
   - Keywords
   - Operators and Symbols
   - Upper camel names vs lower underscore names (enforced)
 - Compile Time vs Run Time
 - Variables
 - Expressions
   - Literals & Constants
   - Operators
 - Statements
 - Functions
   - Methods syntax sugar
 - Closures
 - Comments
 - Control Flow
 - Modules
   - Scope paths
 - Scope
   - Using statements
 - Name Lookup
 - Declarations
 - Types & Interfaces
   - ref, exact, like
   - Instantiating a type
   - implements (also discuss like types)
 - Type Expressions
   - Or types and memory representation
   - instanceof / is
 - Match statement / destructuring
 - Exception Handling
 - Memory (Representation + Ref counting)
 - Strings
 - Collections
   - Arrays
   - do we have tuples? or can our arrays support this like TS [string, i32]
 - Templates (or more importantly, how we don't have templates)
 - Standard library
   - WASI
   - IO (files, printing, etc)
   - Threading
   - Containers / Collections
   - Iterators / ranges
 - Inline WASM / IR


What if we also turned tokens into tower nodes
 - so the token stream is basically a big stream of tower nodes just in an array
 - they already have all the start/end info and all that
 - When we parse them, we can attach them to our parse tree, or turn them into real tree nodes
 - This also kinda makes sense for how complex our tokenizer can be
 - We can even parent things to the token, it will still sit in the stream (forest)

Little bit of brainstorm here on tower nodes, tokens, etc, we want replacements,
all that, but ultimately it should really just boil down to syntax sugar for running custom wasm / tower compiler API calls

how do we want the replacement and tree transform API stuff to work?
 - it shouldn't be too complex, just enough for simple stuff otherwise use WASM / user code

// Two separate references to expression
parse Expression = '@' Expression "," Expression => ...;

// N number of references to expression
parse Expression = '@' Expression ("," Expression)* => ...;

We basically just want regex features for capture

$0 means the whole thing, $1 means the first capture group, etc
 - parentheses are captures in regex, I always hated that
 - we should do named captures only

I never liked how regex replace worked when a group got matched multiple times, I think it just takes the first

As we parse sub-rules, the handlers for those sub-rules will run (either replacements, or they generate their own parse tree/AST)

The tokens become the parse tree becomes the AST

I think it's possible to compile time analyze a rule to see how many matches we expect
 - if it's a single, then it refers to the single node
 - if it's multiple, then it refers to a group (all placed under a single tower group node)
 - do we need indexing here? kinda don't think we want to go that far

named captures will capture non-terminals
 - Do rules always capture non-terminals?
 - If a rule has no logic, it always just outputs it's parse tree
 - Otherwise when we refer to a name like Expression, it just outputs whatever AST nodes that the handler for Expression output

I like everything I'm seeing here!

Ok, tokenization vs parsing, there are some things we can't really support in parsing like character ranges
 - string literals should actually create tokens in parse mode

This is a great example of why we would want the tokenizer to create AST nodes
 - the rule range should show up as a single token when we parse, but all the information about the parse should be contained in parse nodes
 - I can see where we don't want parse nodes just for invoking rules
 - maybe we can make like an inline operator or something

// Character Sets: [abc], [a-z], [^abc], [abc-], etc
token RuleRange = "[" "^"? (RuleRangeChar ("-" RuleRangeChar)?)+ "-"? "]";

// Match any character except ] or \, but allow escaping \] or \\
token RuleRangeChar = [^\]\\] | "\]" | "\\";

Part of our language should maybe automatically remove useless tokens from the input, if there's no variation on them?
 - e.g. they always have that token ( at the beginning...

dumb side note, what would a language look like where all objects are components / compositions
 - any object can have any number of components, children, 

Right now we separate tokens from parse rules
 - part of the reason is so we can define white space, and separate out words and symbols
 - the question is, should parse rules be able to use characters?
   - Or can parse rules only refer to token rules
 - I think trying to combine them sounds neat, but it's sort of confusing
 - When it comes to whitespace, I like that we can define the ignore rule
 - We can still have that though, since it's entirely a character rule...


So the idea is basically if there are spaces between token characters, then spaces are allowed
 - do we also allow spaces between any sub-rule? that seems annoying because now tokens can't define sub-rules
 - in pascal, white space does literally nothing (like it's removed)
 - but in C, whitespace acts as a token separator
 - int foo bar; is not the same as int foobar;

I could introduce something gross like token groups or something, but I think just having two passes is better

 - But the passes should be driven by the parser, the parser should reach the end of the token stream and request another token
   (which invokes the tokenizer, skips any ignored rules / whitespace until it gets a token, or EOF)

Also make sure the parser is iterative

Foo = 'for' '$'

always results in only "for$" being a valid token, not "for $"

The main issue is just that we don't know when we're defining a token that should have no whitespace
 - 'for' could be a single token, or 'for$' could be a single token

So basically we could do some sort of grouping operator that means "this is a whole token"
 - but I kinda hate that, we're adding new operators just to remove the token phase
 - it's also really weird, we need to identify which rules are contained in {}
   - the same rule could be used for parser and tokenizer, that's weird


Foo = {'for' '$'}


Ok, just separate them, it's better
 - but usually for a tokenizer, there isn't a single root token rule
 - like in GOLD parser builder, all tokens are just separate rules
 - So what's the root rule look like for tokens, maybe we just have a special one called root
 - but like... we have to add every token we want to add to it? That kinda sucks for tokenizing
 - and as far as the AST goes, we could always remove the root if there's only one node (that node can have N children tho)
   - I like that, it's kinda like captures
 - For parsing it makes a lot of sense since the parser must parse the whole file
 - But the tokenizer is basically just a splitter, so it makes no sense 
 - Keywords are odd, since they are technically identifiers
   - So how do we know when something is a keyword or an identifier?
   - exact match afterwards I suppose

We should probably add ways of doing tests in the language, specifically tests for the parser / tokenizer
 - Basically "we expect an error" or this should compile
 - maybe even can specify the kind of error expected

Do we have to add token rules to the root?
 - I'm not in love with the idea, but yeah we could
 - We could do something like any orphan token rule is auto applied to the root...
   - but if it's not orphaned, e.g. it's referenced somewhere, then it's not added to the root
 - makes writing tokens easier
 - I think if our langauge is going to use +=, we should too for the parser
 - Actually no, |= makes a lot more sense

But I could write:

token Foo = "foo";
token Root |= Foo; // This line is optional, orphaned tokens are automatically appended to the root

For named captures, how do we discard tokens we don't care about?
 - if it's not in the named capture, obviously you would have to pull it out yourself
 - could make up some sort of 'discard' which just means get rid of these, e.g. discard()
 - but now we're like into function calls in our BNF... I guess BNF just uses operators otherwise...

token Whitespace = [ \t\r\n\f]+ => discard;
 - This means when the token finishes parsing, the action is to discard it

token Whitespace = discard([ \t\r\n\f]+);

 - technically no action, but we're saying please discard all the nodes
 - in that regard, we need to think about how the token phase does parse trees

I think the basic idea is to do some minor cleanup of the tree nodes before they get to code
 - look at some example, binary operators, etc

Grammar rule examples that we should look at:
-----------------------------------------
```ts
// An identifier is any letter or underscore, followed by any number of letters, numbers, or underscores
token Identifier = [a-zA-Z_][a-zA-Z0-9_]*;

// A string literal is quoted, and allows escapes
token String = '"' ([^"\\] | "\\\"" | "\\\\") '"';

// Define a new rule that is either a transform or has a code handler
parse Rule = ("token" | "parse") Identifier ("=" | "|=") RuleAlternation ("=>" RuleHandler)? ";"

// Alternation: A | B | C parses any one and only one of A, B, or C
parse RuleAlternation = RuleConcatenation ("|" RuleConcatenation)*;

// Concatenation: A B parses A first and then B
parse RuleConcatenation = RuleUnaryOperators RuleUnaryOperators*;

// Unary Operators: A*, A+, A?
parse RuleUnaryOperators = RuleValue [*+?]*;

// Character Sets: [abc], [a-z], [^abc], [abc-], etc
token RuleRange = "[" "^"? (RuleRangeChar ("-" RuleRangeChar)?)+ "-"? "]";

// Match any character except ] or \, but allow escaping \] or \\
token RuleRangeChar = [^\]\\] | "\]" | "\\";

// A capture group captures all the AST nodes in a parse
parse RuleCapture = "$" Identifier "(" RuleAlternation ")"

// Values and grouped expressions
parse RuleValue = Identifier | String | RuleRange | RuleCapture | "(" RuleAlternation ")";

// Handle the rule in one of the following ways
parse RuleHandler =
  "replace" "(" UserCode ")"  | // Replace parsed rule with with parsed user code
  "wasm" "(" Wasm ")"         | // Run WASM code that has access to the Tower compiler API
  "run" "(" UserCode ")"      | // Run user code that has access to the Tower compiler API
  "discard"                   ; // Skip/ignore the parsed text, used to skip whitespace
```


RuleRange is problematic since '-' is a valid character

S = A+;
A = C;
A = C '-' C; // How do I say that this one has priority?
C = 'a' | '-';
https://stackoverflow.com/questions/21858092/conflict-resolution-in-lalr1-parser

Ok pretending we have that all fixed and there's some way to specify conflict resolution...

```ts
// Character Sets: [abc], [a-z], [^abc], [abc-], etc
token RuleCharacterClass = "[" $Not("^"?) (RuleCharacterClassRange | RuleCharacterClassChar)+ "]";

// Character ranges, such as "a-z"
token RuleCharacterClassRange = RuleCharacterClassChar "-" RuleCharacterClassChar;

// Match any character except ] or \, but allow escaping \] or \\ so we can write [a-z\]]
token RuleCharacterClassChar = [^\]\\] | "\]" | "\\";
```

What should the parse tree look like?
 - By default, we create child nodes for all terminals and non-terminals
 - none of them are named, just in order children
 - named capture groups and non-terminals can be added as named members

What's the ideal that I want here?

Well, in general I want to be able tp refer to a type of token by name, like "RuleCharacterClass"

so if I'm looking at a token stream with all it's input discarded

"foo" [a-zA-Z_] "bar"

would be:

String       RuleCharacterClass       String
            /    /        |    \
         Not RCCRange RCCRange  RCCChar
            /    \      /    \     _
      RCCChar RCCChar RCCChar RCCChar
         a       z       A       Z

And each of these nodes hold the start/end

[a-zA-Z_] makes sense as a single token because whitespace inside would change the meaning (like a string)

Should "Not" exist if it didn't capture anything?
 - tower nodes need to say which rule they were parsed by

Also, since Not is named, should it be a named member?

Should non-terminals be named members?

I guess it's OK, they're not a map, it's all in an array anyways
 - So maybe we don't name the node, we just name the members?
 - naming members is kind of a way of collapsing nodes, since the "Not" could just be a name and not a whole node

I guess since the node has a parent pointer, we can always do tower_node_get_parent_member_name

Hmm, ok so how do we know what a token is, if it doesn't have a completed rule name by itself?
 - I think we should only use member names for named captures (and maybe non-terminals)

But nodes need to know what rule captured them

Ok, lets just pretend for now that nodes have a rule name or rule pointer on them
 - as well as the start/end of where they parsed (or start/length)

So will all children end up being named?
 - Is there any case where a child ends up not named?
 - It really depends if we make non-terminals named members
 - Almost thinking not, since those nodes have their own name/rule which is the non-terminal
 - But it would be kinda nice if iteration worked like that

We can, I mean if we had the rule pointer, we would just take the rule's name as the member name (ref counted)

So now AFAIK, all nodes have an array of children in the order of parse (or whatever code re-arranged them)
 - and all children have a member name, as well as an accepted rule themselves


---------------------------------------------------------------
My idea:How to write a safe type container
Oh, and parameter types should be able to be listed in any order (called by name, or by position). Optionals must go to the end however for positional calling, but types can be inferred in any order (param 2 dependent upon 5, 6 dependent upon 1, etc)
This allows us to write templates like this:
fn foo(a: t, t: compiletime typeid) {}

Also another idea, what if objects or structure in our language could be like JSON kinda, but for component based design. The principles of one type per container (type map, type multi map)
There is a reason our serialization language was a little different. So then my next idea or questions is can an interface in Tower be used to specify the structure of our component trees? Much like JSON and JSONSchemas / avj
I wish TypeScript could validate, ya know?
So what’s the idea now, components must be exact types (right?), type lookup must be O(1) for mapped types and mapped interfaces (interfaces must map explicitly, may only have one). Component addition can fail
Dependencies, how as a language do we also guarantee things, like always having a parent pointer
A type interface could require a specific component tree, or maybe it safisfies interfaces
Maybe they can also declare what kind of types can own them (do we merge owner statements in type expressions?)
Maybe we just know that type operations are O(1), and others are expensive (you can if any type implements an exact interface, like interface, etc)
Component based design language - has operator - parent type declaration - child type declaration - children can be named (json members)
We’re basically saying what if everything in our language was a tower node? How can we do that and not be horribly non-native?
For starters, features on an object header should be based on what traits they use - we only get memory for children when we use the children statement
We can always walk children and components, and possibly index them?
Do we really even need like interfaces anymore? - it’s true duck typing, but in a way component based design with interfaces solves pretty much all that… can still add it in the future, or maybe there’s a really good reason it makes sense to keep in
We should be able to check if a sub tree matches an interface, that’s what cast should do
So now maybe has is a statement on interfaces too. - has exact types should be fast, has like types should be slow walk (first in order to satisfy the condition)
Component interface paths? - we have indices for children - names for named members - has for components - dependency
Dependencies are always on siblings, and are 
(That could be how arrays are implemented, just object with children templated on T)
So this is built in - or, built up from the ground level of tower
If all members were lower underscore, then we know when we do dot .UpperName it must be a component, so implicitly that’s just a has operator
Must return ‘Component | null’ though
If you do dependencies, then it doesn’t need to be null (guaranteed and ref counted)
I like it better that you can just . off a potential nullable and get an exception, rather than being forced to handle it. Hmmmm
Construction and interface syntax should be similar.
{  has Player; // in the interface  has Player {}; // in construction
  owner foo}

Maybe component types are only constructable as children of a parent (e.g it can’t ever live alone) - you could potentially move them, but must always be to another living owner
When constructed under 

------------------------------
I got some crazy fun ideas about how the compiler should work

if it's all component based trees

when you declare you can have components of a type (or maybe it's implicitly from owners being declared)
 - it also means you need to be able to enforce dependencies, replacements, etc
 - can dependencies just be pointers, I would love this
 - Order of releasing references, etc

So one question, does casting into an interface mean that the interface can hold some components?

like my weird situation:

let a = {
  test: 123,
  owner: ...
};


In TypeScript, you can cast away, the underlying interpretation may change

let a : {
  foo: string | number
} = {
  foo: "hello"
};

let b = a as { foo: number };

That's all legal, and unchecked

casting in most languages is either just on the type, or on a runtime where it's exact and simple
 - and more importantly can't change configuration

Here we would say something similar

has A & B, has A | B, what does that mean?
 - I do like this concept, we can have both, or one or the other
 - When we register a component with a type expression, we're saying it's these things
 - For A & B, we would walk the type expression and register the component as both
 - For A | B, we would error? You don't register A | B...
   - I guess it would just resolve what it actually is and register it as that
 - 

I do wonder what and types should do, do they just create a new exact type, or are operations on type expressions run over the expresions themselves?

When we make interfaces, we say has Type
 - We can also say has BaseType or another explictly implemented interface Type
 - has can come in any order, it's not specifying an order of any kind
 - it's simply just saying it has this
 - 

maybe we can only use named types?

I already had something similar with finding interfaces, where A | B instanceof would do something different

We need a way of pinning data
 - Or maybe when we cast, for things that can vary, we capture references?
 - 


has A;
has B;

would technically be different than 
has A & B;

that implies a single item satisfies both A & B.

has A & B;

however would satisfy

has A;
has B;

but

has A;
has B;

may not satisfy 

has A & B;



previously it was all interfaces on struct values, like typed enums basically

dependency also means we cannot be constructed unless the dependency is satisfied
 - is that just an exception to add then?

constructing a component with an owner means it's automatically added to the owner
 - Owners are implicit when constructing under a parent type

BoxCollider {
  owner cog,
}

owner also establishes dependencies
 - I guess it's just kinda weird, do I establish dependencies?

I'm not totally in love with component based design as part of the language
 - but not against it either, still thinking about it

it makes sense for my language to be based heavily around the tower parse tree as the general object form

also recognizing patterns and subtrees seems really important
 - https://stackoverflow.com/questions/14425568/interface-type-check-with-typescript

Ok, so either it needs to lock it, all members need to be functions that retrieve and throw if they fail, hmm

The allow modify and throw if they fail is the nicest towards writing any code
 - But it technically could be slow if you grab the same thing over and over, but who cares I guess

into {
  has RigidBody;
}

let a = b as {
  has RigidBody;
};

Or when we get an interface, it could be all functions, but maybe we capture everything as it's current type
 - kinda weird that the cast adds a bunch of ref counts, but I guess it makes sense
 - Then it doesn't matter if we modify, it's captured
 - So basically an interface captures, like a closure...
   - Hmm I don't like this, that would end up being confusing
   - If you replace a component suddenly the interface is storing it's own stuff and doesn't reflect the change??
 - Ok so interfaces are purely functional when casted to
 - That means they basically only store the root object that they were grabbed from
   - Then all other functions of the interface must traverse the hierarchy
   - kinda goes back to paths again...
   - yeah I like it, then they fail if it no longer exists
   - interfaces throw if they don't hold anymore, but at the time of retrieval they will match
 - and obviously, if it's just a type itself and you're casting it with as, that is also fine

We'll need to make sure that every form of type we have can be represented by an interface
 - TypeScript for example can represent tuples [string, number]
 - This is because JavaScript's arrays don't care which type they hold

I guess it's odd, does tower need arrays?
 - I can declare that I have children of a type
 - that can be implemented as arrays
 - but maybe I want something closer to real arrays?
   - I kind of never care about fixed arrays... do I?

How high level is tower?!? lol

maybe lets just say that components are an addition to tower
 - The final language of tower can support components and events
 - But ultimately it's sort of an addition to just basic interfaces version of tower
 - thinking of it as an addition, then no, it's not a replacement for arrays

I think the main question floating in my head was, can components be a replacement for arrays?
 - Obviously with TypeScript, we can kind of path down to any member or type
 - Since arrays support any types, it could be [BoxCollider, RigidBody, Model], etc.
 - As long as we strongly type the object (take a snapshot of all it's exact runtime types)
 - Pathing down is important for interfaces (structural types)

is everything a tower node? no, tower node is just easily built with our component constructs
 - tower nodes can have all sorts of components, and anyone can ask for them
 - I really love it, we can also get specific with the requirements

why are children a special construct? Why not just an array called children?
 - I guess it's because we want owner to also be special and the two are linked
 - and we can optimize our implementation of children/owners
 - we would need some way of marking that array as the children array
 - maybe children is always an array?

See this is what I'm trying to consolodate, is array it's own syntax?
 - Or is children the replacement for array?
 - Or does the children syntax require an array?


maybe owner and children are just reserved keywords, but used like members with :

is it children? or components?

{
  owner: Cog;
  components: Component[];
}


What if components isn't special, it just is any array with children
 - if the children have an owner, it's set to the parent automatically, but types must match
 - And there's an order to things


// ref wraps the entire type, note that ref ref is idempotent (ref ref = ref)
type Cog = ref EventHandler {
  owner: Cog | null;
  contains: Component;

  name: string;
  children: Cog[];
}

type Component = ref EventHandler {
  owner: Cog;
  dependency: RigidBody;
}

let c = Cog {
  name: "Player",
  owner: null,
  children: [Cog {
    name: "Gun",
    // don't need to set owner
    children: []
  }]
}

// All named types that implement the same interface Component appear on Cog
// any named types are enumerated and shown here

c.RigidBody.owner


EventHandler is a construct that uses owner, and maybe children too


Something we should think about, how can this whole component based design thing be just an include
 - the concept of type containers / has
 - how it's stored
 - keywords, has, etc
 - lifetime logic

everything should be able to be configured or included, like we extended the language big time

including also structure interfaces, like when we do a cast
 - normally for duck interfaces, they just look to make sure the type has all the correct functions/members
 - but now we're also asking to make sure it has the right components too
   - these components are runtime, not fixed at compile time
 - in some cases, like owner, we could probably just do it fundamentally as a getter property
   - that will match with no issues
 - but has is a different story, since owner is fixed at compile time
 - so has is kind of like saying hey your interface has a child type map and one of those types is X
 - Maybe we can do this all with macro style expansions in interfaces
   - e.g. has RigidBody, expands to something like components(): type_map(with: RigidBody)

 - and maybe typemap has a special behavior where we can cast a type map into one that contains components

let t: type_map(Component);
let z: type_map(Component, with: RigidBody) | null = t.with(RigidBody);
let b: type_map(with: RigidBody) | null = t;

b.  //nothing shows up except RigidBody

if (z) {
  // It's a type map that's guaranteed to have a rigid body... is there a better way of doing this?
  // maybe type maps implicitly satisfy any interface cast
}

Ok, well I like all these ideas anyways, I'm sure the full thing will come out in the future

-------------------------------------------------

Ok, next topic, what's the MINIMAL set of tower that we need to start building tower?
 - Technically I keep going back to this thought that the tower compiler can all be WASM
 - so ultimately for binding to other languages, they really just need a WASM interpreter

However, there is a still a minimal set of tower that we need to start building tower ourselves in tower
 - it's the tower bootstrap
 - grammar rules BNF, pointers, ast, wasm execution

basically it's if I was to run the tower compiler without including any of the "prelude", no-std if you will
 - what does that version of tower look like
 - before interfaces, before ref counting, before anything

it's very C like, even the concept of interfaces should be something we extend

well, what about just the token/parse rules + wasm + tower nodes?
 - I guess in order to properly support tower nodes, we need component based design
 - that's the ideal correct, to have everything work from component based design?

the basics of tower could start possibly as an interpreter
basically just directly translating actions to immediate wasm execution
kind of like syntax directed translation
there's no parse tree, just the parser directly runs code on reduce

so the beginning of tower is just a direct interpreter
 - it lets us start building tower nodes from within tower, instead of outside
 - and the concept of interfaces, all that
 - what can we do with just wasm, no syntax nodes

Normally when we do => we get parse nodes, tower nodes, but we don't have them yet?
 - we could maybe have some simplified wasm form that only passes like start/end pointers
 - easy enough to write in wasm, but tokenizing gets kinda weird
 - and I don't like the idea of rewriting rules constantly (like starting with a completely different language)
 - I like the idea that each piece is a subset of the next pieces
 - obviously we get rid of things like pointers for safety, but they're not gone, and they're fundamentally still there


we want to define tower nodes within our language, how the heck do we do this
 - can have some sort of compiler callbacks that tell the language to do everything (defined in wasm)
 - so maybe phase 1 tower nodes are very simple (tokens and parse tree, no interfaces...)
 - tower nodes evolve in type??

 - and then we ensure the language dumps everything, and implement phase 2 nodes

 - the main ideal here is to have as much as we can written within the language, and minimise the amount
   of hand rolled wasm needed
   - basically I don't want any algorithms written in wasm, only like primitives that it makes 100% sense and
     we'll always need it around, like how pointers work, or maybe structs, etc

 - or maybe we can make tower nodes and reserve exactly what's needed in terms of space
 - or maybe we can run an upgrader on them to upgrade them all to the latest definition... hmmm

the entire point of this is so that we can write the fundamental architecture of tower within tower
 - we really just need enough of the basics to code interfaces and components

I don't want to have to rewrite all the rules again as an AST form after the SDT form...
 - Ideally want to re-use the rules, but do something else with them
 - Maybe I need a way for rules to have a name, so we can reference them and redefine an action
 - I can do that, just give them all unique names

I'll figure out the best way to do all this as I go, I'm sure I can find a way to reuse it

With wasm linking, I can sort of rely on a top to bottom execution
 - Ok I like this, the compiler asks the wasm to define what a tower node looks like
 - the wasm callback passes in the start/ends, raw info we get from the parser

That means the parser is implemented externally

This kind of makes sense, there are two ways to bootstrap:
 - wasm interpreter + implement the exact parser algorithm / compiler callbacks into wasm
 - we also write the parser algorithm in wasm, first using any language, but then eventually using tower
 - so you also only need a wasm interpreter to run everything

But maybe we write all of tower in wasm and tower itself
 - Hmm, basically saying to write the parser in wasm first
 - so it's like, everything is the same, there is no just "run the wasm"

And if that's the case, the wasm that it starts with defines

so that means we don't ever end up rewriting the parse algorithms
 - I bet we can make them really small and easy to define in wasm

once the parser is running

we also need to get really clever about how it allocates, all that
 - I like that tower nodes are callbacks, because in the beginning there could be no allocator

maybe write an allocator in wasm

wasm is a subset of tower

it should basically be normal wasm + WASI, but with some extra compile functions that expose the wasm compiler / execution
 - expose self compiling wasm to wasm
 - should be able to replace existing definitions with new ones

wasm allocator
 - simplest possible form

the start will look like wasm + wasi file reads

parser implementation in wasm too, creates tower nodes
 - rules finish parsing, calls attached wasm to rule
 - passes in tower nodes
 - as rules are parsed, we add wasm
   - wasm can lookup existing functions, etc
   - can we support forward declarations in wasm?
     - we should be able to add declarations apart from definitions
 - C like language to start, everything must be seen (forward declarations)


Function = 'fn' id '(' paramlist ')' statements => wasm(
)

fn foo(a: i32, b: i32) {
}

is this approach bad?
 - don't we ultimately want to emit binaryen or something more sensical?
 - shouldn't we be writing this in C/C++, and maybe just bringing in WASM
 - compile it to wasm so we can just import it as part of our stack
 - replace it in the future with our own implementation maybe


if we do direct wasm, somehow we have to be able to return sub-expressions when code generating

so when we ask it to compile wasm, we need to get expressions

ultimately we do want this mostly all written in our language
 - I don't want to rely on binaryen to extend anything in our language

that means we'll basically be running debug wasm, but maybe the implementors can
pass to binaryen and do optimizing - like it's part of the wasm host
 - we can even pass in a flag that says if we want to optimize it or not

Maybe we don't even need to return a pointer, maybe we just build a string as we go
 - can we just concatenate wasm together?

I suppose we're actually saying that we also need a full wasm parser implemenation too
 - because we're not outputting direct wasm codes...
 - I suppose we could?

 - Would be kinda neat to write wasm binary output in wasm itself
   - not as easy as text, obviously
   - but certainly more direct
 - maybe it's not that hard...

so we output assembly that poops out assembly
 - I like this

if the beginning of our compiler is wasm text, doesn't that imply our interpreter should deal with text wasm?
 - but we can compile the wasm text to .wasm with wabt or binaryen...
 - at that point, what's the difference between using C or something else, we still have to interpret text
   - unless we're hand writing the binary...

browsers cant deal with text wasm, I could compile it

so even for browsers, I'd have to compile wat2wasm or binaryen...

then kinda who cares... I already will eventually rewrite this in tower anyways

ARRRRGHHH I DONT KNOW

maybe we only compile the foundation with wat2wasm or binaryen
 - we do it offline to get a wasm file
 - from there, it's all running in wasm itself
 - need some basic utilities to build wasm byte code, can be written in wasm

the browser

I don't mind running a tool offline to produce wasm
 - eventually this will be our code anyways, so lets not try to make it in wasm then

Or, build a wat2wasm parser myself using our parser logic
 - we could write the text parser in our own language
 - it kinda makes no sense, since we need our language first

alternatively, instead of wasm, we could make our own
 - either represented by the tree, or 

If we make our own, we'd want to lower it to WASM at some point anyways

I think the primary pain point is that text form isn't a good AST/IR
 - Returning values in expressions isn't straightforward

But maybe we can do it anyways...

should it be part of the platform, or should we just incorporate it?
 - lets just compile it to wasm, expose some specific C calls
 - wrap it in C and only expose our compiler API

Making it a requirement of the platform should be a non-starter, especially when we could do it all in WASM itself
 - the compiler we ship should contain the whole wasm

Ok, so it has to be part of our language, but we're going to basically use a wasm that we will scaffold
 - maybe we write it in wasm lol

And then either we output wasm (binary wasm)

One dumb question, do we want tower nodes to be our IR?
 - It would make sense in a way, however the IR doesn't need to be flexible
 - technically the IR should be something that's fixed, doesn't change
 - we know at the end of the day it has an exact structure that doesn't change
 - still could be done with tower nodes... hmm

the only advantage to tower nodes being our IR is that whatever low level language we invent is a "subset"
 - like, we can make it very C like with calls, ops, etc
 - everything is a call
 - but also just emitting wasm is easy too

 - I also like that we can just keep up with wasm, and people who know wasm can contribute
 - wasm has a lot of IR features
 - but binaryen api is just pointers, not safe to expose...

But suddenly now the version of binaryen that we integrate changes our language and what wasm we support
 - Still ok

Technically our C++ compiled wasm can host our allocator, and lots of other functions
 - we'll make it very explicit which are provided, generate a header based on binaryen exports
 - it could also implement our parser, until we rewrite it

Ok, so we're going with a C++ base then, built to wasm, with very explicit and curated exports

is the IR all just pointers
 - or do we somehow just emit wasm text
   - emitting wasm text seems unessesarily slow, but possibly nicer to deal with, and safer

at the end of the day, we really want our language to be able to output it's own optimization loops
 - should be able to easily extend optimization passes, etc
 - really build most any language you want in tower

so how do we start then...

I love the idea of writing a wasm parser and outputting wasm, but it has to be in a style that's getting ready to BE binaryen

Ok, what we need to do now is start to author the language

Lets just go with WASM text for now, we can always change this later
 - Like syntax directed translation

we write the whole parser in C++, including all the extension stuff
 - expose all the functions for creating tower nodes and callbacks
 - everything will be rewritten, but for now it all runs in WASM so it's good
 - we do rely on binaryen, but we state the intention to eventually remove it
 - we also specifically are built on top of wasm for it's mostly deteramistic characteristics and safety sandbox
   - but may someday opt for an entirely safe and 100% determansitic IR

it can just be pointers for now, and we can regard that as a generally not safe part of the language
 - but our future iteration will switch to a fully in language represented IR that's safe and ref counted
 - would need to be sort of generic, who says what tower nodes should look like if they don't include components??

maybe we could support a primitive lowering pass

ok so with binaryen, we also get text parsing

but you know what, we're making fucking tower, it's all built in itself
 - I want to one day be like, yeah bro look at it all
 - the baseline can still be wasm, in fact we can make that explicit

But I'd rather get to writing, the faster I get into writing tower code that's not just wasm, the better
 - that way I don't need to implement the parser yet, 

shoot, I just realized my compiler also needs to run wasm too, does binaryen simulate?
 - seems like it does have the ability to simulate
 - so we technically could ask the OS, or we could rely on binaryen...

at the end of the day, this can all be rewritten, keep it fast and loose

so we start by writing in C++, the tower API is the creation of tower nodes

WE JUST NEED TO GET STARTED, C++ IT IS!

it's more like a context has all these virtual functions about how to deal with tower nodes
 - and so our "passes" are just us dumping a context
 - but it's not like we want the context to stop existing
 - we just want to upgrade it, and ideally it should be compatible

lets think of the example here, we get our new version of interfaces all written within tower, structs as well
 - now it's time to "upgrade" the language to v2
 - do I need to specify all the rules again?
 - are we deleting everything before (pointers included?)
 - when do we discard tower nodes, or do we always keep them around?
   - 


tower_node_create(context: tower*) -> node*
tower_node_destroy(context: tower*, node: node*) // panic on double destroy, check free list header
tower_node_attach(context: tower*, child: node*, parent: node*) // automatically unlink and relink
tower_node_attach_member(context: tower*, child: node*, parent: node*, member_name: string) // automatically unlink and relink, any member of the same name is removed
tower_node_get_parent(context: tower*, child: node*) -> node* // or null if it's the root
tower_node_get_child_count(context: tower*, parent: node*) -> u32
tower_node_get_child(context: tower*, parent: node*, index: u32) -> node* // or null if not found, panic if out of range
tower_node_get_child_member(context: tower*, parent: node*, member_name: string) -> node* // or null if not found
tower_node_get_kind(context: tower*, node: node*) -> string
tower_node_get_parent_member_name(context: tower*, child: node*) -> string // returns empty string if not named


lets just try doing this in wasm, why not

Ok screw that we're doing C++ to get started faster
 - along with binaryen

we'll build out the wasm
our end goal is to have a parser that can parse the very basic BNF rules, as well as support

one of the first things we'll want to implement is structs and types
 - and determining how pointers to types work (members, etc)

ah fudge fine, the whole reason of tower is to build from the ground up so lets start with wasm
 - it'll force us to use super optimized structs

Now that I can add on to the root parse rules, what's the first thing to do?

all I can really do is execute wasm, so now I need to start building my tree
 - tower nodes are implicitly created during parse
 - but we can also create them by hand in wasm

do we need the tower node executor framework setup yet?
 - can dependency inputs and outputs just be strings for tasks, and we determine what those strings are later (paths)
 - for now we can do the terrible dependency graph, that just checks if every task is ready to run
 - maybe we allow tower to hook into and redefine this execution model

since we will be reducing leafs first, those tasks will be the first registered
 - so as long as we basically keep things in order, most tasks will complete in their first iteration

maybe when tasks fail, they are automatically placed on the back
 - technically when we finish a sweep, only failed tasks will be left
 - we keep iterating over the lists until nothing changes

lets not get clever until we get stats, we'll know better what to do

- I think this task model where we check our dependencies first is golden
  - does the check need to be separated from the task, e.g. two different functions?
  - In the future we'd want one to be immutable

We still need to know if a tower node is alive, and kill it when the associated tasks fail
 - otherwise those tasks would either keep the node alive, or just continuously fail to run (stuck)

lets worry about that when we get there, assume nodes don't delete
 - and if any task gets abandoned, we'll know about it

So a task has a condition, an action, and a failure action that produces an error

When a task completes, it's removed (do we ever expose tasks?)
 - can you hold on to them, cancel them? etc
 - tasks can self cancel, or complete
 - not sure if they technically need a state for that or not

basically if the function returns continue, it continues, if it returns complete, it stops

tasks may need to be closures at some point, may need to keep their own data

I think this is kinda everything, right?

is => wasm() actually a task, or is it immediate?
 - does the parser just keep going?
 - when do tasks get executed, after parsing completes?

I like that the tokenizer is lazy, but is anyone driving the parser, or is it just driving until it consumes the rest of the input?
 - does it ever pause to execute code, and if so why?
 - code can add parse rules and affect globals
 - maybe that's reason that the parse rules should run the wasm on parsing immediately, not a scheduled task
 - we can make adding parse rules during scheduled tasks like an error or something
 - but we can add them while directly parsing

So our tokenizer and lexer need to be able to rebuild
 - I love the idea of lazily building a DFA from an NFA (only building it for rules we go down)
 - but not required for now

so the bare minimum C++ is this:
 - parser/tokenizer implementation
 - callbacks for creation and linking of tower nodes
 - basic language and tokens for BNF rules (subset of tower)
 - parser rules should just be specified as tower nodes if we register them (or maybe reparsed as strings...)
 - tower API for emitting and compiling wasm (can just be text for now)
 - run and parse wasm text format
 - parse wasm after the end of a rule, and in the main context at any time
 - some defined way that different wasms modules run and link together
   - maybe don't need to specify an entire module
 - tasks and dependencies, cancel/complete
 - allocator

With that we should be able to write everything we need

We could do direct syntax translation as we walk and parse rules
 - but now we have dependency tasks, so why do that when we can do proper codegen
 - I would want to do a basic type pass, even for our early language (again, subset...)
 - maybe we specify the binary format of tower nodes long before
   - so we don't replace them or change them in any way
 - this would mean the concept of type ids needs to be figured out
 - our initial C like language would need to decorate tower nodes like usual
 - if we did component based design, that means attaching components

I don't like that our object format changes (header, etc)
 - what if we add weak refs
 - we should make every feature we add to the header an extension
 - I want to be able to include type safe object systems
 - as long as we can extend remote types, who cares what we do
   - since at any point, if say we couldn't get a weak ref because they didn't declare they supported it
   - make them support it!

Maybe the same approach I take for hot compiling will work for updating tower nodes
 - we would need to reallocate
 - move all "features" from one to another
 - can figure this all out later

my main question is now, without just doing basic syntax directed translation, how do we do types?
 - ideally I want to attach types like components
 - we can reserve a type id that's literally for the type id structure
 - type is basically just an interface though that gets the actual typeid
 - we can just start with a tower node for now for primitive types
 - we also want a type id to just be tower node itself

 - we can also attach this as a named member, kind of like 'pre interfaces'
 - or, all types of types (pointers, primitives, interfaces, etc) all implement a single type interface
   - and we can add that to the tower node, mapped by the type interface
 - yeah, that would also work, both are OK
 - whatever we do, ideally I want it to be a subset (we don't change how we represent types on tower AST nodes)
 - attaching a 'type' interface makes sense
 - like adding a component would register every interface

you know, maybe we just support "queryable" interfaces?
 - when we add a component to a composition, we don't want to have to register every single type we want as fast lookup

can we pre-compute component configurations?
 - kind of want a map at this point, constant time lookup
 - instead of having a map on every object, we can share maps of similar component layouts
 - basically lazily pre-compute interface lookups
   - the map just tells us the index, or memory offset, something like that

ok, but either way for getting started we can just map the single base type interface
 - as if it was a component that declared "interface Type"

C++, binaryen, compiled to wasm with no-std
 - possibly just use the wasi SDK for this
 - ideally minimize wasi calls, we shouldn't import them if we don't need them
 - may need like wasmtime or something, not just binaryen
   - if we want to execute WASI functions and not have to get them all working in binaryen
 - browsers would only need binaryen
 - really, the wasmtime part should be the "host" platform, not part of our wasm
   - unless we really want to have an entirely emulated version of wasi
 
checkout binaryen as a third_party sub-repo
 - create scaffolding cpp project with cmake
 - add binaryen to our cmake project
 - get WASI sdk docker container building our scaffolding (no-std, or minimize it ideally)

copy welp code over just to get things working quickly (can rename and all)
 - get parser running with tests and samples
 - show that we can execute our wasm


-----
for minimal rebuild in the future, I really need to dive in to see how it would work as I haven't done it
before and don't know the pitfalls (I can only imagine). So the idea would be that we can basically save state
at any point in the parse, including the states of tasks and their wasms, etc.
 - almost like docker container layers, we 
 - basically if we see a change in source, it rebuilds from there down
 - but that's sort of a very basic minimal rebuild, how can it know just to ignore whole subtrees in the parse?

note that we must call _initialize on the scaffolding, we may want that in the future too (maybe we can rename this)

tower_allocate
tower_free

-----------

what does our parser definition look like?
 - do we really care if it's tower nodes or not, ultimately it's just going to come down to a BNF API

If we used tower nodes, how would it work
 - we'd really just be building the parser from a tower node configuration
 - and ultimately, it would only work on specific BNF type nodes
 - I think it's like trying to create an IR for no reason

ultimately we have productions, non terminals, terminals, and grammar symbols
 - character sets are probably important

what would the tower API look like?

I think it should be tower nodes, a non terminal node with a bunch of children

in order for a node to "be something" 

one thing I like about using tower nodes, it simplifies our API
 - otherwise we have to be able to create and delete productions, add and remove grammar symbols, etc

tower_production* tower_create_production(non_terminal);
tower_production_remove_element(tower_production*, index)

push, pop, insert, clear, etc

basically array like functions

We pretty much need all those for tower node children

so if tower nodes need to be component based, is that insane?
 - ultimately parse rules are pretty small
 - it's not like we're making a ton of them

Ok, so in order to support components we just need the type map structure
 - again, this can all change


TowerNode* non_terminal_type = tower_node_create(context);

TowerNode* non_terminal_node = tower_node_create(context);

struct NonTerminal {
  string name;
  owner TowerNode;
};

NonTerminal* component;

tower_node_map_component(non_terminal_node);

type ids need to be perminantly kept around...

Ok, we'll start off with component based design, doesn't matter what the implmementation is
 - We can make it match the future implementation of components
 - and we can static assert that all the member positions match so everything lines up
 - but also, we can totally do the thing where we swap out all the pointers on the tower table

are captures really just mostly syntax sugar for generated capturing rules?
 - like captures ultimately just make a new rule right...


maybe we also want all tower node components themselves to also be tower nodes

What would this look like in tower...

can you add more than one of the same component?
 - if not, then it's not really a substitute for children
 - we can allow it, but it only ever maps the first one (kinda like, it's only useful if it's useful to the component)

// Adding an owner line implicitly adds a children: TowerNode?
type TowerNode = {
  owner: TowerNode;
};

// So now tower node can have an owner and children
// When we do this with a single chain, we should mimic C++ inheritance (with no v-table)
// Basically structs packed one after another so that they can be memory compatable
type CharacterRange = TowerNode {
  charStart: u32 = '\0';
  charEnd: u32 = '\0';
};

random thought, what if the ability to get interfaces off a type dynamically via pointer was a feature
 - like a vtable feature, you can add it to your type
 - normally in class/struct in C++ with a virtual function gets one
 - I guess we can still mark functions as virtual or not, that tells us if we get a vtable...
   - but I kinda hated that, like typeid doesn't work cause no virtual functions... gross
 - or all of our allocations have vtables, at the end of the day tower should be easy
   - value types don't have them, reference types do, something like that

new problem, should components be able to not have owners?

type TowerNode = {
  owner: TowerNode | null;
  component: TowerNode;
};


We're only able to extend exact types (or named types), basically types with a fixed single type definition
 - they're basically like a reference to an extensible type

so effectively owner can work with any extensible type or null
 - is that just a built in thing for owner...
 - that's probably a reason owner shouldn't just automatically extend children for components
 - maybe for any non-extensible type other than null, we require children
   - basically we always require the type to declare that it has children that support our type
   - but if the type does not, and it's extensible, extend it instead!
   - kinda like that...

dumb side note, if we can get a good dependency system going, the tasks could possibly be kept alive for monitoring if types change
 - e.g. if a type is extended, then it's like a minimal rebuild only for things that are directly dependent upon that type
 - we just need a really solid dependency system (also it shouldn't run all the time, ideally only when it needs to)

-----------------
ok, so how should tower node allocation work

at first I wanted to just allocate it on the heap
 - but it gets odd if we do 
   type CharacterRange = TowerNode {...}
 - because how do we extend a type that is only a pointer now (unless tower node allocates children)

originally, tower nodes were not supposed to be extended so it didn't matter,
but I'm grappling hard with what I want the tower node format to be

since TowerNode is a named type or exact type, whatever we settle on, then those are hard requirements.

whatever it is, it must match structural interfaces

I need to be able to match an interface as it will be very common, say for example I want to easily
check that this tower node represents an expression between two integers

// IF A VALUE IS USED IN A TYPE, THEN IT IS ASSUMED TO BE THE TYPE OF THAT VALUE (THE TYPE OF A CONSTANT IS A CONSTANT TYPE)
// In typescript: 'a' refers to a value, but is being used as a type here. Did you mean 'typeof a'?
// you could only ever use a value within a type if the value was in scope of the type
// but ideally it could be part of it, and we would actually check for that specific value

let rootExpression: TowerNode = ...;
let binaryIntegerOp = rootExpression as TowerNode {
  has: BinaryOperator;

  left: TowerNode {
    has: typeid(i32)
  },

  right: TowerNode {
  },
};

let binaryIntegerOp = rootExpression as TowerNode {
  [0]: TowerNode {
    
  },
  [1]: TowerNode {
  },
} | null;


----------------
I almost think that keeping tower nodes as individuals themselves (not extensible) is better
 - yes the api is larger, however it forces a sort of nice structure where tower nodes are just the skeleton
 - and components are just components, not children
 - we can still allow components to have sub-components, at the end of the day you can declare anything in tower
   - so maybe I shouldn't try and make it generic, it's just kinda weird

I actually don't really like the idea of single chain inheritance either for tower nodes
like I feel people are just going to start getting odd with tree structures


Ok now one last thought here, lets revisit our token stream:

String       RuleCharacterClass       String
            /    /        |    \
         Not RCCRange RCCRange  RCCChar
            /    \      /    \     _
      RCCChar RCCChar RCCChar RCCChar
         a       z       A       Z

This is just the raw parse tree
 - We could do it where these were all tower nodes, but what does the raw parse tree mean in component form

our nodes should be pretty generic as just the parse tree, since we don't even know what components to attach
Parse trees haven non-terminals as parents and tokens as leafs

ah, so it's just a component that's like NonTerminal, and Terminal
so if I just read the roots of each token in the stream, they should all have a single NonTerminal component


basically our token stream is [TowerNode{ has: NonTerminal }];
 - So now because of that, every element in the array I can go tokens[0].NonTerminal // and that's a component!
 - AWESOME!
 - This was a great example of the parse tree as it comes in raw (RuleCharacterClass), but then we transform it
   - we compact the RCC ranges into 'CharacterRange' classes as a single component
   - we extract the two RCCChar values (should do a neat interface style cast in the future here)

WAIT, IN THE FUTURE GRAMMAR RULES CAN DEFINE REALLY SLICK TOWER TYPES!!!
 - since we know the structure, we can actually apply TYPES TO THE PARSE RULES

Holy cow that's awesome, that means when we do 


"elixir implements the language constructs such as defining modules, functions, other macros etc in the form of macros"
 - phew, almost thought it was going to be a nice looking language... lol



token RuleCharacterClass = "[" $Not("^"?) (RuleCharacterClassRange | RuleCharacterClassChar)+ "]" => fn(node) {
  
  type NonTerminalNode = TowerNode {
    has: NonTerminal;
  };
  type TerminalNode = TowerNode {
    has: Terminal;
  };

  // The type of node here would be
  node as TowerNode {
    children: [
      TowerNode {
        has: NonTerminal {
          name: "Not"
        }
      },
      
      TowerNode {
        has: NonTerminal {
          name: "Not"
        }
      },

    ],
    RuleCharacterClassRange: 
  };


  // We can detect everything, like if a rule is nullable (locally)
};

// OK LET ME COME BACK TO THIS, MY BRAIN HURTS

how about before we do ANY type syntax, we just start with compile time type ids, so we can start immediately generating
types


it's like we really want to be able to say children also has a child that looks like this
 - possibly by index, but 


Ok one more question, how are we using member names, I guess for the parse tree are we even?

what if types are always just functions that return compiletime typeids, and functions with no arguments are called implicitly
 - then I can really just use that compiletime typeid anywhere

let Type = fn(): compiletime typeid {
  ...
}


let b: Type ...;

I'm basically just arguing that the phrase type

```
type X = ...;
```
is just syntax sugar for
```
let X: compiletime typeid = ...;
```
OOOOOOOOOOOOOOOO OK

except I don't want X. to give me typeid members
 - unless typeid implements MEMBER ACCESS OPERATOR :O!
 - no that's not good lol
 - I think it's more that X is the statics

```
let typeid_X: compiletime typeid = ...;
let X = typeid_X.statics;
```

Trying to think of how to make statics feel correct when associated with a type
 - I've always felt weird about this concept
 - like ultimately, accessing a static through a type is using it just like a namespace
 - that's probably why C++ did :: instead of .
 - but . feels so goooooood

it's almost going to be so common to specify has... I wonder if has can just become a shorthand

let t: has RigidBody = owner;
// same as
let t: { has RigidBody } = owner;

------------


------------
when it comes to destructing a type, we need to do it when the ref count reaches zero
 - at compile time, the compiler should look to see if the type is virtual or not
 - if not, it calls the destruction logic directly on the memory before releasing it
   decrement reference
   placement destructor on memory
   free memory
 - if it's virtual, it needs to consult the vtable or typeid to see what virutal destruction needs to occur
   - for tower, this also involes destructing extension members, etc
 - does that really mean every path needs to generate an if == 0 and inline the destruction logic?
   - or, every type can have it's own dec-ref logic that simplifies the whole process
 - then it's up to the optimizer if it wants to inline that function or not, sweet

------------

maybe we don't make any of the tower functions automatically add references
-------------------------

I think our idea should basically be this:
 - we define the initial version of tower nodes in C++
 - right now, lets not worry about the memory layout or implementation too much
 - in the future when we fully settle on how the object layout works, we'll make a matching type definition in tower
 - we can even do the whole replacement thing where we replace all the tower functions with ones implemented in language


---------------
ok so this is a case to consider, normally we wont' allow construction of a component without an owner

in Rust, we would require the owner member to be filled out
 - but also, we'd probably mark it as private, because we require construction logic to be run for the component
 - including adding itself to the owner, dependencies, etc


---------------
next problem, for component based design, reference counts are kind of strange
 - since the owner holds a refernece to the child, and the child holds a reference to the owner
 - it's a unique pointer situation, except we do want others to be able to store references

that's why we did handles for Zilch, Components were OWNED entirely by Cogs

we could do something odd like the stack thing we were going to do before
(any ref count non zero on stack destruction is an error)

So in this case, if the component has a reference count that's not 1, it's an error

is that... ok?

I mean in most cases we don't store components anywhere
 - dependencies... actually that's kinda cool, it's implicit!
 - as long as the other component stores a reference, we can't remove the dependency
 - do the components also store a reference to their owner too (cyclic?)

Maybe components don't increment owner reference count
 - but then how is it safe?
 - when an owner reaches ref count 0, it will always detach all components whose reference counts must be 1

e.g. it's illegal to let all references to an owner die, but keep a reference to the component around

kinda weird to throw during destruction...

I guess we can also have the component store a "weak reference" to the owner
 - and throw if it's ever non-null
 - and call some sort of detach logic, effectively akin to a destructor but like, you've been removed from the component...

ayah....

we pretty much want components to die exactly when their owner dies
 - we honestly want the references to kind of just... be the same?

 - like a reference to a Component really just increments the owner reference...
 - whoa... that's interesting...
   - other than just using pointers under the hood, I wonder what would be a way to declare this in language generically
   - like, my object is fundamentally linked to the lifetime of another object
     - my reference is their reference

- maybe independent of components, we can "own" child objects, and their references are references to the parent
- therefore the must exist within the context of the parent only, even if dynamically

Somewhat interestingly, since it's reference counted, we know all references are gone once we release the last one

Technically components only need a parent pointer to be SAFE, owners don't need to point at components
 - Once the reference count reaches 0 on the owner, we also know that no components are still referenced
 - HOWEVER, we would have no way of calling component destructors or freeing component memory if we had no pointers from the owner
 - so the owner must hold direct pointers to components in some fashion and guarantee that they are freed and destroyed too
   - fascinating, can we make this concept generic...
 - 

it's really just unique ownership but with reference counts
 - and the idea of redirecting the ref count


So what are the fundamnetals here, its safe IF:
1. The child has no reference count and refers to the parent (child must have parent access)


Ok this is really cool, one thing though that also breaks down is ownership trees of tower nodes
tower nodes point at children, children point at parents, etc (another cylcic reference)

we almost just want roots to have reference counts here...
 - like, I don't want children to keep parents alive so parent should be weak

hmm...

what happens if children did not increment their parent
 - it would be OK if when the parent died, it removed the child's parent pointer (nulled it out)

hmm actually I forgot about this, parents are already nullable!
 - so when the parent dies, it can null out the child's owner pointer

cool, so basically now we have this ownership concept too that involves weak parents
 - but rather than doing a whole crazy thing for weak parent pointers
   (like python allocating another extra object with it's own ref count)
   we just declare owner: TowerNode | null; and somehow this means it's weak?

it works, maybe we can formalize these ownership safe models

as long as a type removes all references to itself... hmmm!

ok but now dependencies get weird
 - need a separate count for them to hold them
 - weird wierd

I want tower to also be an exploration of alternate, less restrictive forms of memory safety than Rust / borrow checker
 - I love what Rust is doing, but I would sacrifice 10% of my speed to have the language feel like C# (but not GC fuck GC)
 - unless there's a way to do GC objects, but have it be less intrusive (manually called or something)

---------------

so right now the model is this:
 - lets start thinking of object's as just having hidden intrinsic features that you add in
 - so like reference counting should be one
 - when you attach that interface to an object (maybe implements?) you also somehow affect the storage of that object
   - that includes what kind of memory restrictions you need, like can it go on the stack?
   - you're basically defining all the parts of a type, fundamentally what we can do with it in memory
 - Originally I was thinking of this as 'ref' or & on the type, but now I'm thinking of this a little differently
 - like how can an interface modify a type behavior? defining a handle

I do like the idea that Tower can have many different kinds of objects for different scenarios
 - with a really good default of ref counting
 - handles that go null when the object is deleted
 - handles that always return a valid object and throw on access
 - unique handles that only grant temporary access or are moved/nullable
 - raw pointers
 - but the point is, these features are added to the object piece by piece, along with the type definitions

--------------------

so now rather than having a "TowerHeader", we need to pick
what's appropriate for each, like TowerComponent has no reference count
 - hmm, that does make things a little harder to be generic
 - like, it's nice if we know the first 4 bytes of every object is a reference count
   - otherwise dynamic features like interfaces need...
     well actually they just need to consult a virtual object anyways to say "what happens when I make a new handle"

maybe in tower we can have this handle concept, almost like a built in operator
 - basically you can return another type that the compiler will use as a fill in for storing a handle to your object
 - the fill in object is always given a pointer to the original object
 - it's like we have the "make an initial handle" and then "copy/move a handle"

-------------

ok lets go back to ref counting, it's now tower node specific

ok for example, we don't want people to inherit from tower node
 - so tower node is not virtual
 - and we would require virtual destructors when implementing an interface
 - also tower node is sealed so we can't add features to it to force it to allow inheritance / extension

hmmm

 rust fundamentally does not have inheritance

check out rust deref tho:
struct StructA;
`````
impl StructA {
    fn name(&self) -> &'static str {
        "Anna"
    }
}

struct StructB {
    a: StructA, 
    c: i32,
    // other fields...
}

impl std::ops::Deref for StructB {
    type Target = StructA;
    fn deref(&self) -> &Self::Target {
        &self.a
    }
}

fn main() {
    let b = StructB { a: StructA, c: 123 };
    println!("{}", b.name());
    println!("{}", b.c);
}

`````

I think the issue is we're still trying to have it all. In rust, structs are just data types
the fundamental question is, why do we need virtual destructors, isn't that a relic of C++ inheritance
 - but rust DOES have destructors
 - how do they work in memory??
 - because I can create a Box<Thing> and thing has a vec, which must be dropped?
 - hmm, but it's never virtual
 - so I think rust kind of gets away with it because the type that's dropped is never virtual
   - or even if it is, it's only got one single destructor to every call
     - well, it technically has some built in ones since it needs to destruct members

so a question would be, in rust can I hold a dyn interface to keep a reference counted object alive
 - and then have it destruct through the dyn...

------------------------
https://users.rust-lang.org/t/v-tables-differences-between-rust-and-c/92445

I would draw it like this:

&dyn Foo (Trait Object)
+--------+----------+
| Data * | Vtable * |
+--------+----------+
    |        |
    |        v
    |     +---------------+--------+----------+-------+
    |     | drop_in_place | size   | method_1 | ...   |
    |     +---------------+--------+----------+-------+
    v
+---------+---------+--------+
| field_1 | field_2 | ...    |
+---------+---------+--------+
And the C++ version:

Foo* (Pointer to derived object)
    |
    v
+--------+---------+---------+--------+
| Vtable*| field_1 | field_2 | ...    |
+--------+---------+---------+--------+
    |
    v
+----------+-----+
| method_1 | ... |
+----------+-----+

------------
interestingly, Rc in rust is always on the heap, it's like Box
 - there is no RC where it's not allocated

hmmmmmmmmmmmmmmmmm, I don't hate rust's way of doing things (no inheritance, no including types within yourself)

what would components be?
 - a type component that wraps one single type, no inheritance

still has a virtual destructor

Anything dyn is a fat pointer, so it has to be a vector of fat pointers to be polymorphic

HUMMMMMMMMMMMMMMMMMMMMMMMMM

how does an Rc work with dyn objects
 - I see, so they just specialized Rc and Box to work different for dyn object than pointers

ok so now lets really think about 

I started brainstorming what a purely component based language would look like, difinitively
 - we make a statement with tower, the cool parse rules, the awesome compiletime syntax, how templates and generative types work
 - and then we wow the world with a new(ish) language paradigm: Component Based Design

My declaration is that the only kind of pointer or tree traversal should be with compositions and components
 - the language has no other way to represent anything

all structs are pods

We do need some sort of way of doing mixins however, and I still like A & B for this
but what if we went simple, all types must be named, like you can't do {} & {}, has to be C = A & B
 - I don't really know where I'm going with this, let me just think a bit


or like, what if we stated that all heap allocated structs are always colocated
 - even member by member colocated? we can do that weird thing in arrays where the first member of struct[0]
   is grouped with the first member of struct[1], and so on, like each member of a struct is an array
 - this can be a hidden detail
 - so inheritance and interfaces is really just a single pointer to the super


I guess the idea here is there are only owners, components, and pod data types/structs, that is the only data types

that would basically imply that interfaces are tied to data, like a component is an interface
 - 


One thing I want to think about is GameSession, Space, etc, how do we solve those?

in Zero, Space was a Cog, but it was kinda awkward, there were space components that had a dependency on their owner being a Space

Space being a Cog meant it could have normal Components too (like ZilchComponents)

owner Space

what are all the types we want
 - owners (cannot be components themselves, lets just put our foot down, it's too awkward)
 - components
 - pod data types
 - ref counted pod data types?

we're also saying there's no dynamic cast or interface stuff, there's only asking an owner for a specific interface
 - ok yeah lets be weird with it, lets say that 100% for sure there are no other interfaces other than 'base' component interfaces
 - I'm just thinking of a world where there are no Rust style traits or anything, it's just very simple
 - like maybe BoxCollider actually just has a dependency upon Collider, but Collider is never constructed by itself...
   - the only advantage I can think of is that you can swap from a BoxCollider to a SphereCollider

Lets also imagine a world where the owner tree/hierarchy concept is extremely lightweight
 - then we may refer to objects just by their components, but really we're saying 

like when I allocate an object and point to it, I'm really always pointing to an owner that has a child

and maybe we can make optimizations for owner declarations that specify they always have specific components (allocated together)

 - this was the true component based language I thought about a while back, lets try it as a thought experiment

should derived components just be sub-components?
 WHOA

so like... Collider is basically a component that declares that it ALWAYS has at least one sub-component

Collider = {
  has: ... hmm wat
}

I really do like sub-components for some cases, 

the whole structural interface matching part of tower is where I feel like it starts to diverge from being native perf
 - but maybe we can make this extremely fast, and practical/useful

the main problem we're trying to solve with Collider is that while it may also have data (we don't know or care) it
does have an actual interface, with un-implemented methods
 - and now the question becomes who gets to implement those?

moreover, one part of Zero that I think is paramount is events
 - basically we're declaring that owners should always have parents and children, regardless


when would I want to use a heap allocated value that wasn't a composition?
 - lets try and think of how we can make it so you would never know the difference...
 - I'm basically saying what if there was no struct/interface keyword, just components, no pods

we're basically saying that any interface on any object should be swappable

a: RigidBody

is the same as 

a: { has: RigidBody }

we only pass owners...

I think at the end of the day we're still going to want pod structs, containers, etc


all of this brainstorming is great, but I think the main blocker I have right now is that I don't know the data
layout of tower nodes and tower components

because Rust gave me a serious pause with thinking about how vtables in C++ work vs fat pointers in rust

wait, one cool thing about components, they don't need to store their RTTI type this way
 - if it's always only stored by the owner, kinda cool


So the weird thought experiment is that components could basically require
that an interface gets implemented, like a pure virtual function with inheritance
 - it means that the component isn't valid without being added along with a component that implements that interface
 - this sounds exactly like a dependency, EXCEPT
   - A dependency is offering an interface
   - This is a dependency that is asking someone to implement it's interface
     - it's odd because it's still going to call it in the same way, gah it's so similar
   - it's really like BoxCollider is saying it implements Collider
   - BoxCollider cannot be constructed without a Collider
   - Collider cannot be constructed without a type that implements Collider interface
   - That means it's cylic
   - Normally in C++ the base can "access/do work on" the derived members through virtual functions
   - and the derived class can just directly modify the base because it IS a base
   - but here we're saying in order to communicate, we need pointers on each to each
   - but obviously, in the case of collider, it's like an opaque interface

really is just a dependency upon an interface... right?

Hmm, it's odd then, is it...

------
interface ColliderPureVirtuals {
  fn get_mass(): f32;
}

component Collider {
  dependency ColliderPureVirtuals; // except these methods should also appear ON Collider too
}

component BoxCollider implements ColliderPureVirtuals {
  dependency Collider;
}

------
and then maybe we have a shorthand for the same thing above
------
component Collider {
  fn get_mass(): f32; // because this is not implemented, it means this component implicitly
                      // has a dependency upon any component that implements our interface
}

component BoxCollider implements Collider {
  // implicit dependency Collider;
}
-----
and in the same way, we could have done
------
component ColliderPureVirtuals {
  fn get_mass(): f32;
}

component Collider {
  dependency ColliderPureVirtuals; // except these methods should also appear ON Collider too
}

component BoxCollider implements ColliderPureVirtuals {
  dependency Collider; // collider methods could appear on BoxCollider too, correct?
}
------

component BoxCollider {
  implements Collider {
    ...?
  }
}

so then what if we just don't have interfaces, just components with functions only

I guess the kinda cool thing is since we can always separate out interfaces into separated components


lets just imagine tower not being so low level anymore, except for when we're starting to build it
 - but just imagine it wasn't built in all these layers yet, just direct to component support
 - we'll break it apart where it makes sense, but for right now just picture a purely component based language

so basically interfaces are really just questions answered by compositions

components can implement multiple interfaces
 - remember, when we say implements, we have a dependency upon it too,
   and we offer an implementation for their unimplemented parts
 - lets examine diamond of death (bad example but yeah):

component Stringify {
  fn toString(): string;
}

component Damageable implements Stringify {
  int damageTaken = 0;

  fn Stringify.toString() {
    print("Damageable");
  }

  fn takeDamage();
}

component Breakable implements Stringify {
  fn shatter();

  fn Stringify.toString() {
    print("Breakable");
  }
}

component Player implements Damageable, Breakable {
  dependency RigidBody;

  fn Damageable.takeDamage() {
    ...
  }

  fn Breakable.shatter() {
    ...
  }
}


so if I look at it, Player implements Damageable and Breakable, which both implement toString

when we add a Player component to an owner, we must also add Damageable and Breakable
 - both of them also have a dependency upon Stringify
 - however Stringify has a reverse dependency upon whoever implements its interface
 - in this case, Stringify will lookup and get the first component that implements it

component based design doesn't make much sense when you start duplicating components, unless you use that functionality for meaning

so that's it, thats component based design in a nutshell

ok so what's the object layout look like

oh and owners are basically just basic ass data types, always ref counted, deletable, etc
 - treat them like cogs, but you can't abandon them either

cogs had members like name, so maybe we allow owners to declare info like that
 - but I almost rather that be a component, go all in
 - because the second I allow owners to have data, I need to suddenly have syntax for owners implementing interfaces, etc

I'd almost rather that all compositions are equal in binary memory,
but the type system differentiates them by which components they are allowed to have

 - We would make spaces just by creating a component, and that component can hold a new root tree

yeah, if you want to "Tag" a composition as a specific type, use a tag component, it's light weight
 - and the component can even have data
 - e.g. this is a Cog, the composition is never a Cog
 - compositions are just magical creatures with a fixed set of functionality and API
 - we just define exactly how they work
 - so we're saying all the components just have the same "dependency Cog;" line

some components implement a specialized tree of pointers that point down the heirarchy
 - lets worry about hyper optimizations later

having all compositions be equal except differentiated by type only is really fascinating

do I even need to declare compositions or give them a name?
 - lets just imagine for a second that compositions are completely generic
 - you can't declare them, they have no component types
 - then what makes up different compositions? maybe header components
   - just basic components that provide some shared features like object names, etc
 - so we fundamentally don't have compositions that can only take a specific type of component
   - all compositions take all components
   - components don't have an owner type, it's generic and always the same
   - but we can be a CogComponent by just having dependency Cog
 - we can call those like Header components
 - Space might have a dependency on Cog, and TimeSpace has a dependency on Space, etc

- ok this hierarchy along with safe handles describes all heap objects

so compositions have children, parents, all that jazz and are ref counted
 - basically they have a single form and definition, there is only one composition
 - but you distinguish types by their pieces
   - it's almost like if it looks like a duck, but more like if it has a mouth and some eyes
 - we make sure compositions support all the best types of handles (weak, strong, throwing, deletable, unique, etc)
 - we also make sure the memory layout is kick ass for game engines, cache, etc

now the last little hole is how we handle struct types, really just pod types
 - I think we just support pod types? it doesn't really make sense to have component based design on the stack

maybe my "frozen" objects concept... basically the idea of freezing a known composition definition
 - but I think structs are just better for this
 - the freezing advantage would have to be like it's minimizing the object
   - it would have to look exactly like components in memory, but it's statically composed together
 - eh... I don't feel it plays well

 - I think just pod structs is fine
 - initialized entirely by name
 - maybe not pod, we can have vector<T>... fn vector(t: compiled Type): compiled Type {}

 - how do we handle containers and all that?? full component based design would scream and say EVERYTHING POLYMORPHIC!!

 no arrays, only compositions
 - no containers, ONLY COMPOSITIONS!!!
 - bleh

well, maybe it's not "frozen" per se but I do think the idea of declaring an interface with "has" means it must have that
 - which means that as a type, we can reserve memory for it and all of it's dependencies
 - (the ones we know about that cannot be dynamic)

so I guess my weird point about frozen things is that they could also remove features (sealed)

is trying to throw component based design at everything too much of a hammer?
 - I'm trying to make it ok so that there are no arrays or anything
 - like I really want compositions to be your arrays too
 - ah see that's where defining a composition starts to make sense
   - I want to be able to say what my children type can be
{
  [{has RigidBody}]
}


that's saying a composition whose children all have a RigidBody component
 - can I do better?
 - casting is just comparing interfaces, or doing runtime casts

compositions are tuples too

{
  [0]: { dependency RigidBody };

  [
    { has }
  ]
}



// is this what they look like?
{
  RigidBody,
  [
    {RigidBody}... // array syntax
  ]
}


when we say the name RigidBody, it is also usable in places as {RigidBody} (any composition that has a RigidBody)

[RigidBody]

so RigidBody is shorthand for {RigidBody}
 - but RigidBody in parameters actually does take a RigidBody pointer
 - and declaring it as a member actually does hold a strong ref count to the composition that holds the RigidBody

- but what we're saying is that technically the two are one to one convertable (when it delcares that it has RigidBody)
  - so we implicit convert and treat them as interchangable
  - well we may end up needing to always pass the composition anyways to keep it alive


[RigidBody...]

is actually just

[{RigidBody}...]

which is actually just

{ // composition
  [ // children
    {RigidBody}... // composition of RigidBody
  ]
}


component RigidBody {
}

let a: [RigidBody];

a.add(RigidBody { // shorthand to construct a new composition with that component
});


composition as [RigidBody...] // this cast would check that all children have a RigidBody




that's really just saying `a` is a composition whose children have RigidBody components
 - may have a lot of other components

so for this composition, 


who cares, people still use JavaScript to date...
 - and ultimately these are at least structs and assembly level implemented

WOW I LIKE THIS LANGUAGE KINDA!
if I stop thinking of it as being bad.. I mean look at JS arrays, they're arrays of variants
 - so much allocation and random overhead, GC, prototypes, etc

this is not rust!

we can also do tuples too, [A, B, C]

can we do compositions on the stack?
 - or are we just going to be happy enough about temporary component allocation

technically if we do structs, it won't make any sense with using compositions as arrays
 - since we can't do [Vec3...] if Vec3 was a struct (it has no parent, etc)
 - that's kinda gross though, we can't even have just an array of vectors?
 - But I mean, we can they just have parent pointers... who cares JS is worse!!
 - so everything is a component, it's higher level
 - again, just look at an array of vec3 in python or JS... this is leaps better

 - also does it always have to be heap allocateed?
   - hmm, but maybe that's ok, especially if we're using a fast typed allocator for cache
 - lightning fast amortized O(1) allocation and freeing
 - yeah ok, whatever!
 - basically we're saying it doesn't matter if we're heap allocating or not
 - ideally we want an array of Vec3 to be colocated together
   - but they may not be contiguous, while they are all allocated together with all other Vec3s
   - and the likelyhood is that if you allocate a bunch in a row, they will sit nearby each other
     (they first occupy free spaces, so there's always that issue first)
   - but otherwise they're mostly together, I can live with that

yeah, it's kinda crazy, everything is structs...

and then really serious code gen for templates and stuff
 - maybe we can define new tricks with reflection type + compiletime generics
 - like whatever we need for Rust style enums/match, or TypeScript type expressions + typeof/instanceof

it's not a native perf language, but it's pretty damn fast
and it's crazy extensible

everything is a component

hmm, what if I just want
  position: Vec3

value types again?

component Vec3 {
  // implicit owner, dependency count, etc
  x: f32;
  y: f32;
  z: f32;
};

component Foo {
  position: Vec3; // hmmm?? does that create a whole composition?
}

I guess this is the C# part where it's still nice to have proper value types

but virtuals, interfaces, etc only work with components

hmm, well dang I really wanted [] to be syntax for just compositions / tower nodes

well fundamentally we really just need a few kinds of data structures:
 - hash maps, arrays, lists

maybe at the end of the day, a component based language isn't one that forces everything to be component based
 - after all, C is part of C++, you can always go back to structs

I guess that's a part of rust I really like, they only have structs, pure data
 - and then interfaces implemented for those structs
 - it's very simple and beautiful
 - although they do have tuples, arrays, and a complex generics syntax

so then if I take a step further back and ask myself, with a language that has component based design built in
can it be broken down into pieces or features, instead of having one type of composition?
 - The only reason to ask this is that it could be interesting and maybe break the difference between struct/component
 - part of me kind of likes the idea tha a struct can be a component with the right members
 - hmm, but maybe composition is still fixed?
 - I love the idea that component is just syntax sugar
 - that's one thing in my language, I never mind syntax sugar because it's shorthand!
   - it's not a "new way" of doing things, just shorthand for something larger

maybe compositions can store structs that don't have owners?

I guess we haven't figured out component layout anyways yet, kind of want to talk with Chris...

I mean, the wacky ass way this langugae would work is that everything is a component, including like Vec3
 - component just means it has owner and like next/prev pointer or something


Lets take a look back at this problem:

component Foo {
  position: Vec3; // hmmm?? does that create a whole composition?
}

yes, yes it does!

is there any way to do value like semantics though?
 - value really just means it clones every time we move the handle
 - I mean... yeah we can, that sounds horribly slow but whatever!

what are f32, i32? I feel like those have to be structs...

how do I do [i32]?

I mean even in JS that's a variant, so at best it's an array of variants
 - basically just saying a composition doesn't sound that crazy
 - can compositions be stack allocated and copied?

 - technically the pointers to a composition

And even though we're allocating it, we're doing so with a lightning fast allocator (amortized)
 - but cache coherency becomes the issue

I think if I go all in on component based design, I don't know if I'll love it as a language
 - part of the reason I love TypeScript so much is how flexible it is
 - even just native TypeScript + Rust style traits would be a big win in a langugae

let me just think more about component based design languages

we could also do the equivalent of "boxing", where any value type that gets used in an array or whatever gets boxed
 - C# does pretty much that anyways with allocating objects

basically even if we have value types, an array of value types would be

struct Vec3 {
  x: f32;
  y: f32;
  z: f32;
}

Hmm wait maybe thats the idea, maybe we only have structs, but any struct can be used as a component?
 - it would implictly wrap the struct
 - hmm but what are dependencies, unimplemented interfaces, owner pointer that the user can access...
   - that should be the things you get when you declare you are a component...

so maybe we still have structs and components, but structs can be components (just no owner, etc)

[Vec3...] // is just a composition whose children (not components) all "has Vec3"

same as

{[{Vec3}...]}

lets make it illegal to add more than one component of the same type, I like this design
 - if you want children, use children, not components, components are a map functionality

I don't mind where this is going for a high level language, but for a native language, eh

components listen for events on their composition

events can propegate in many ways

events are just a composition themselves

we can drop the has keyword too

cog.send({
  RigidBody {
  },
}, Propegate.Up);

hummm, not quite the language I wanted to work on...

I mean if it just had the type expressions and other native "handle" features, I think I would not mind

hmm, well now that we have an idea for our composition structure, can we start structuring tower nodes?

component based design is really just a piece of tower
 - the main reason it's so important right now is I'm trying to find a way to represent it in language

what I'm overwhelmed by is implementing Rust style trait generics, C# where clauses, etc
 - type constraints
 - A where clause should just be a compiletime function

lets plan to have components, but lets still just focus 

lets stop worrying about making tower nodes this perfect layout
we can always change it later

and ultimately, even if we dont match in memory we can always just wrap the tower node API
with a structured / componentized version of the tree...

For tower's foundation, we need both a generic parse tree (to hand to wasm)
 - as well as a tree structure for specifying productions/rules
 - we also have no types


instead of ever exposing how tower nodes actually work, lets just give them all the facilities we would ever need
to wrap a tower node with our own type

 - Tower node's can actually take pointers to our types, but lets not assume anything
   about memory or how components work in the future
 - this is just tower nodes, their own thing, yes they are extenisible
 - we don't try and assume an rtti layout or anything, just give it what's needed
   - like for a destructor, tower nodes just have to take in a destructor

so then our goal is really the easiest extensible structure in wasm
 we can make it match later!

lets just gooooooo

ok so one last thing to remember here, our parent pointers are weak pointers
 - child compositions don't increment parent refs


--------
random other dumb question, can we make a language with safe fat pointers?
 - basically like C, but you can use pointers freely because we know they'll throw
 - even the concept that at runtime everything is a refcell...

can we also enforce single writer multiple readers?
 - we just need to know which chunks of memory are considered "together" and mutate with each other
 - this would be data you'd normally wrap in a mutex

Basically our version of this data always has a header
 - for multithreaded, maybe we can do mutex or atomics
 - basically it's like wrapping a thing in a block, but if another bigger block contains it, it removes the inner wrap

basically the idea for pointers is that we can always point to the head of an object
 - we will guarantee that these allocations are done on some aligned boundary


we don't want think about sizes or sizeofs, because ultimately we could have a void* or u8* that just points anywhere
 - how can pointers to anywhere be made safe?

I think to answer that, we have to think about what memory is even ok to point at
 - generally when thinking about a safe language, we can say that a
   pointer can never be initialized to point at an unallocated location (stack or heap)
 - And also that if a memory is freed, the pointer should become invalid or nullable
 - and that even if a new memory is allocated in it's place and the pointer coincidentally points there
   it should still be considered invalidated/nulled

the most granular way of breaking this up would most likely be stack frames and malloc allocations, alloca's
 - since the stack is reserved usually by call frames (unless things like alloca exist)
 - so the idea would be that at the header of every one of these is an intrusive list of all pointers

 - I would love to do handle style, but with pointers that gets difficult
   - and I think fundamentally a well behaved program should not have outstanding pointers
   - so a well behaved program will have O(1) deletion (only one reference left to it)
   - though it makes the pointers "fat", it only does so by next, prev, and then the pointer itself
   - the intrusive list also acts as a reference count, we can either auto-delete or mark it as an error that it wasn't deleted
     - you get to pick your behavior, some of them are lowerable to release mode variants

 - ah but it's a bit more, because technically we also want to point anywhere inside of a memory block, so we also need an offset
   - we need to also constrain any memory moves to be within the block, as well as reads

I think most of our work will be in reducing these pointers and the cost of moving them around
 - reads are more likely, so read cost is not inhibited

 - we do know that pointers have a fundamental width (the size of the type)
 - meaning that no matter what, all reads from a pointer (except ones like memcpy)
   will have a compiletime size associated with how much they can read
 - we do NOT want reads to have to check if they are within range every time
   - ideally we make an assertion, we do NOT allow you to ever even point, without reads, to invalid memory
   - that is because it is far more common to read from a pointer than modify a pointer
   - so we want to do the bulk of our safety constraints on pointer modification, not on read which is more likely
 - so because of that, it is possible to ensure valid pointers
   - casting from a u8* to a u32* will incur a check, if it is within range of the memory it's pointing at

 - so that means the pointer does not need to be any fatter, just next, prev, and ptr
   - because we know the size we'll read, upon construction of the pointer (or cast to pointer, etc)
     we do the checks to ensure that pointer never points outside the memory

Moreover, a pointer can only point within it's allocation, not to other allocations
 - that would be considered a different pointer and constructed from another location

this all ultimately means that you cannot construct a pointer from a constant (such as foo: i32* = 0x1234 as i32*;)
 - I think it's probably a security benefit to not ever allow that, we can ensure only some things can be pointed at
 - at some point in our language, we could offer a way to point at 

this can be the default style for allocations, it's almost all C++ like but safe with an overhead
 - but then we can introduce other handle primitive types, which can ease pointer restrictions but still be memory safe
 - reference counting, etc
 - and ultimately, we can always compile in a way that lowers some primitives to actual pointers (almost like "release mode")
   - technically "unsafe" but great for non restrictive environments and sandboxes like wasm

ok next issue, if we can point anywhere in the class and at anything, then we can point to internals of pointers (next, prev)
 - we could mess with those, which is unsafe

I think then it just matters what we're pointing at, and if we're allowed to point at that thing with that kind of pointer type
 - is it reading on boundaries?
 - is it a type that makes sense?

maybe we can come up with a very fast table lookup for if the pointer is viable or illegal
 - and I could see how we could enforce rules, even with structs, on certain layouts
 - I wonder if there's a fast way to generate all these, maybe they cache on the first occurance?
   - if it ends up being so big, then have a way we can declare and pre-cache (this memory here will be casted to X)
 - but that also means when we allocate memory, we need to give it an explicit layout
 - stack memory is also weird, doing it by stack frames makes no sense as it will change with optimizations

We also need to think about arrays!
 - does it matter how the memory was allocated then?

I guess we're saying that the way pointers work should be up to the struct itself

but in C, memory is just memory, the interpretation does not matter
 - our language is basically taking a step and saying that there is a physical layout
 - in rust, is it just considered unsafe to re-interpret memory?

I think we can declare that every stack allocation and every malloc has an
interpretation of memory that cannot be changed without deallocating
 - it's kind of like it's type, but really just says what memory exists where
 - it's only needed if we get a pointer to that type (which is always the case with malloc)

so that also means an array of u8's can be re-interpreted, but not as pointers!
 - I love that... 

imagine there was a pointer inside a struct to a Thing
 - and then we reinterpret the struct itself to a new struct, which now has a pointer to a Foo
 - we would check that the pointer to Thing is compatible with the pointer to Foo


some memory should be opaque, and never pointed at
 - for example, if we did rust style enums, the tag value is not technically exposed (unless we did expose it...)
   - but that memory shoudln't ever be pointed at or modified, it's unknown!

it's all simply about binary compatability

what about arrays or uninitialized values?
 - we also have to guarantee that all memory is initialized, not really important for pod types, just pointers
 - but, how do we say allocate an array of memory and keep track of what's allocated and what's not?
 - well at least we know we only really care about the vector/array case,
   which can use a moving index to let us know the capacity vs size
 - I don't love that this would be a special case, but otherwise we'd need like some duplicate memory to keep track



Well, so far I really like this C style version of Tower
 - we make it so some primitives are effectively invisible to the eye (maybe, we'll see)
 - I kinda like the approach where a type can say what it wants, but then it's overridable on the pointer level

------------------
lets think about the reading from array while modifying it case
 - if the iterator was pointing into a vector, and then the vector resized, it's now pointing at invalid memory
 - I think the primary issue here is that it could have invalidated the pointer, but you don't know until it lazily happens
   - our code is technically still safe, but has a sleeper bug in it
 - can operations know if they might affect memory (the allocation) and therefore check if any strong references exist and bail?
   - we really want push to throw while we have an iterator already to it
   - and the thought is that maybe push can declare it affects memory somehow
     - or maybe this can be inferred somehow


I'm not sure about threading though, or about data modification guarantees
 - I guess the nice thing is that, at least manually right now,
   we can invalidate any references at any point to a piece of memory
 - so even if a hash table operation may have added something to the hash map which may cause it to rehash
   - a pointer to anywhere inside there could be invalidated on every add, even if it didn't break it
 - again would just be cool to mark functions as "affecting memory"


so for threading, in a way we can guarantee that nobody else can get references to a piece of memory
 - if there is only one reference
 - but really, we would need to scan the tree to see that the only references held are all internal
   - e.g. if the tree/graph is an island
 - this is basically like a lightweight version of serialization of state for workers
   - but instead of serializing, we're just guaranteeing nobody else is reading it
 - I do like the pure sandboxable worker tasks that you can basically farm out (all copied in out)
   - if something needs to be lockable, maybe it must be it's own memory object
 - like an object doesn't need to have one ref count if it's always a mutexed object

 - thread safe, sandboxy threads too (launch threads with only dependencies needed, pulls out the wasm)
   - I love the idea we can send this over a network and create a networked wasm protocol for massive parallelism
   - the compiler/runtime should support this, pure tasks! (copy in out!)

note that if a type is wrapped in a mutex, because of the intrusive linked
list we can't technically even move around or modify pointers to it
unless there's some sort of magical atomic intrusive list that you can add/remove links from different threads?
 - how could something be accessible by different threads
 - instead of intrusive list, it could be atomically reference counted
 - basically just changes the kind of pointers we're allowed to use

side note, two threads should't be able to point at the same thing and have it null out, that should be an error
 - only one reference left to an object when we delete it for threading purposes...

---------------------

We really do want memory to be contiguous
 - maybe for arrays, in memory we separately contain inlist headers per element
 - it means array allocations are actually larger (and must support resizing)

maybe when we declare that a memory has a structure, it must all be initialized simultaneously to a layout that is binary compatable
 - that means memory can even be uninitialized (which also destroys all intrusive linked lists)

note that if we don't care about getting references to the elements in the array (e.g. copy is fine)
then we don't need that extra data
 - it's not that the array is immutable, it's just that we only ever copy in and out

hmm, technically as a feature memory would be relocatable
 - obviously if you supported relocatable memory, then you can't lower the pointers into a release mode style
 - but it would allow us to re-allocate arrays and keep pointers in-tact
   - so you can iterate through it while pushing to it
 - basically we can allow re-allocate, and if it fails it can even move all the pointers and re-allocate
   - the re-allocation would require a binary compatable definition if we were to re-allocate

--------------------

last odd side note, instead of the memory storing anything about it's layout, can that be stored by the pointer?
 - like rust style fat pointers, obviously this will make our pointer even fatter
 - we could certainly do this, so then the memory itself never needs to know it's own layout
   - but what gets odd about that, we already need an intrusive linked list
   - we'll think about it, may still need it for rust style interfaces

in our language, we're saying anything you can get a pointer to anything
 - it's almost like it's changing the type of the value we got the reference to
   - it's wrapping it in a structure that has an intrusive list
 - we can infer this for local variables
   - we can also get a reference to any child member of a struct, as it will always get the reference from the parent with an offset

our pointers look like this:
  next
  prev
  pointer // always points to exactly what we want it to, reads are FAST
  inverse_offset // offset in reverse from the pointer to the header that holds type info, etc


with these odd but safe pointers/shared contstructs, we can basically make our language look very nice
 - especially with compiletime constructs

 - then we might consider just adding rust/dyno traits


if we start off considering memory as uninitialized, and then any initializing operation on that memory
marks the memory as being initialized

I am thinking what it might look like to support a dynamic layout in memory
 - maybe we can support it but it's slow?
 - my dumb thought was we obviously need some data that tells us what the layout is
   - what if that layout could be dynamically constructed
   - but that kind sounds slow...
 - that's really if we just wanted to allow anyone to use placement construction anywhere

fundamentally we could have different kinds of memory for optimization:
 - an N layout, always instantly initialized (needs no flags)
 - an N layout, uninitialized but initializes in contiguous array order, needs a number to say how many initialized

memory needs an intrusive list next/prev (does it technically need a prev?)
 - in a weird way, pointers could all point to themselves, and we only need a pointer from the memory block to the first
   pointer in the list (any pointer really, they're circular)

so a memory block that can have references looks like:

head_pointer
layout_pointer
initialized_count

hmm wait, do we need to have the head_pointer per array index?
 - since we want to keep track of references to each individual array item
   - but we want a pointer to be able to move anywhere in the memory block right...
   - simple, even if we keep a head pointer per array index, if the pointer moves within the array
     we just relink it to the correct position!

- for the initialized count, the only check we do is that the placement construction only happens
  on the end (or placement destruct / back to uninitialized)
- it's actually the act of placement construction that validates it's in the right place for that type of memory
  - same with placement destruct (again, only adds O(1) overhead...)
I like this concept that the overhead is always fixed, possibly always constant (at least we can argue it's constant)

I had this idea that for uninitialized memory where you "placement new" anywhere inside it
 - my dumb idea is that we just update the layout pointer with a new layout pointer
 - but this is kinda... like almost just maintaining a separate map again

or the memory maintains a sorted list of layouts, still need up to N of them...
 - the layout would also need to store the location, now it needs a layout pointer AND index...
 - it could always resort to allocating if it needs more than like 1/4 it's space, that wouldn't be terrible
 - keep in mind we also need a head pointer per too!

I'm OK if this memory layout ends up taking up 2x memory, but not 4x that's crazy
 - 2x would only work if we character by character specified primitive types
 - if we could do everything in 4 bytes aligned chunks, and somehow only needed 1 byte to represent...
 - for example, we could have special bytes for all the primitives, and then all byte values above
   went to a table (like 8 bit color pallette tables)
 - that table could store the layout pointer, only needed for pointers
 - technically, our memory could have more than 256 types of pointers in it
   - so we could also have a 2 byte table version, etc
   - we always pick the lowest memory size we can get away with
 - I think in practice the sorted linear search is going to be the best...
   - it's worst case scenario is the same as the indexed table


there might be something clever I can do, because while I know u8 is a problem technically any struct of size 1 (u8)
 - can only be composed of a single u8 (struct foo { test: u8; })
 - a pointer at minimum is 4 bytes, and if we have our fat pointers they are much larger
 - so maybe we can do everything by 4 byte layout minimums
 - but we have specialized layouts that represent the only ways to get to 4 bytes
   - 



I think the ultimate problem is that imagine we had two types, u8 and i8
 - maybe we could boil them down to the same type, but just imagine different
 - if we initialized an uninitialized block of N bytes of memory byte by byte, we would need N layout pointers
 - think if we did u8, i8, u8, i8...
 - I guess if you really need that type of memory, go for it

is this a DFA recognizing task??
 - the idea of representing memory in small words/characters
 - because every time we need to reinterpret a pointer inside a pointer, we need to do the same thing again
 - it's almost like a grammar... I think? pointers are like grammar rules / non terminals, they can be recursed
 - the job of checking if two layouts match is really fascinating
   - I'm sure we can come up with this algorithm
 - it's kinda crazy but I think it almost is, like in one form we can have the memory boiled down to primitives
   - and then run a dfa/grammar to check if a layout matches those primitives, sort of, interesting...

an even dumb thought too
 - we could even support storing function pointers, and cast those too... craycray!
 - it would ensure it's all correctly memory compatable, safe, and initialized

---------------
ok well that's the whole idea, safe memory!
 - we can invalidate any references to memory
 - we can ensure we only ever point to initialized memory
 - pointer reads and writes are as fast as in C
 - pointer manipulation however is checked (we make every attempt to make this fast)
 - allocations are also fast

 - have many other different safe memory models to choose from
   - reference counting (allows weak, strong, deletion/handle)
     - pointer is only 4 bytes
   - unique pointer / moved
     - pointer is only 4 bytes
   - child/parent pointer
     - a relationship between two allocated objects that says the lifetime of the child is tied to the lifetime of the parent
     - but these relationships must be kept, the child may not exist without the parent
     - children can also have children, so long as their removal keeps the child attached to the parent
     - somehow declare and enforce this relationship...
 - That gives us a ton of options to memory manage and control our pointer sizes
   - and the language is maintained as entirely safe
 - this does mean that any time we pass in a pointer to a function, the pointer itself must be copied (added to a list)
   - even calling on self.something()
     - if self is a normal safe pointer, a new one must be copied/added to the intrusive list
     - if self is a shared pointer, we can pass the pointer in directly as 4 bytes, but must inc-ref and dec ref as we leave
 - that's fine, it's a fixed and quite small overhead
   - inlining will be our hero here

If you use this special kind of pointer, in a release mode it can be treated as if it's just a normal pointer IF:
 - It's the normal intrusive memory model safe pointers
 - The pointer cannot be invalidated / is set to error/panic on memory deletion
 - This also means that any attempt to manually invalidate memory, such as a container add, would immediately error/panic
 - I say panic because this type of error should not be catchable, we're basically using this as a debug mode only assert
Same thing with reinterpret casts, if you do one that panics on failure, that can be release mode
 - release will not check if the type layout matches, nor will it panic

Work with JD to come up with other memory safe data structures
 - mainly just thinking about that crazy intrusive list structure
 - really want to be able to write an engine in this, and then go release mode if we need it

 - when we specify an object, we should specify all the traits of it's lifetime
   - is it deletable
   - does it have a reference count?
   - weak references?
   - etc

---------------
dumb thought, we were already planning that `a + b` is just `a.add(b)` approximately
 - why not the same with type expressions?

like isn't a type expression really just a binary call

A | B is really just A.union(B)

it's just taking in compiletime types...
hmm!

---------------

for tower nodes, right now we have it where we call alloc and then construct in place
 - this means the user is also responsible for deleting component memory
 - but that kinda sucks, I'd rather have it where we allocate a tower node with extra space leftover

-------------
note, one interesting place that might make sense for a borrow checker is unique pointer and similar...
 - we could potentially also use the borrow checker to know when we can use just a raw pointer,
   or if we need to pass the full fat pointer
 - an interesting idea, lets think on that...

-----------

double initialize of the same region also needs to be illegal
 - I think we just need a concept of an initializer that covers an area of memory, and sets up the layout pointer
 - it's illegal to call an initializer over any area of memory already initialized
 - the single object and N array cases are very easy to check
 - the general area memory case is more difficult
 - technically we could support initializers overwriting pod data as it doesn't matter...
   - that can be an option

we can also support memcpy
 - it will copy initializer data / layout maps and ensure it's not trouncing anything
 - we can support overwriting pod data too (u8s to u8s...)
 - again, we can support a version of this that just panics, but in release continues (debug panic)

I love the fact that you can always code in a style that is exactly akin to C,
 - and ideally as fast if you stick to exact C primitives
 - as long as you compile down to a release mode setting
 - we can do the same with alloc and free
 - just always look for these opportunities, it will make the language attractive

and ultimately you can also just use pointers too

so we'll have the concept of "uninitialized" pointers, pointers that are allowed to point at uninitialized memory
 - maybe that kind of pointer (like a void*) can point at anything
 - we can also use it to uninitialize memory
 - note that they don't have a size, other than a single byte for convenience (and because that's the smallest)
 - that means they aren't checked when you point them around
   - I do like that, it's basically just an integer
 - Hmm, not exactly, tbh it never makes sense to allow them to point at dead memory, just any live memory
   - they just don't care about layout/memory format or initialized/uninitialized
   - they are specifically tied to a block/region of memory
     - so they still act the same as our fat pointers
     - but you can use them to initialize memory, with placement new, etc
 - as much as we could support initializing a pointer from an arbitrary int
   - I think it defeats one of our security guarnatees, that a pointer from one object can't point into another
   - and pointers only point to valid places in memory
 - (unless running in unsafe release mode)
 - so these void*s still need to point at valid memory, but we can move them anywhere around inside
   - initialization of a type, e.g. Player {}, should require one of these pointers
   - almost like placement new, allocate(memory) Player { }
     - maybe this is also how we indicate we want a heap object...
     - I wonder if I can find a tasteful way to make the memory layout a type in the language
       - and you specify which type of memory block you're allocating
       - basically allowing the user to extend memory block types
       - but we do want them to be very fast in execution
   - we'll think on that syntax, something that I never liked about C++ was that you kinda had to pass that down
     - e.g. if you wanted to do placement new, you had to pass down the buffer for every function that wrapped the placement new
     - I guess it's not that bad, but Rust solves this by moving everything
     - Things are moved into the memory, but theoretically their optimizer and aggresive inlining/templates
       will cause it to actually just initialize the memory in the correct place

What comes out of the allocation should be the Player*

so basically memory blocks have this virtual behavior (but ideally we want it almost unvirtual for perf...)
 - check if a given uninitialized pointer manipulation is legal within the memory block
 - check if an initialized pointer manipulation is legal within the memory block, and pointing at a matching layout
 - whether the memory supports deleting or not
 - on deletion of the memory, ensuring that the memory has been fully uninitialized (or that it's pod...)
 - also responsible for constructing the initial pointer handle
   - this is probably how we should implement reference counting / shared ptr
 - on initialization of a type from an uninitialized pointer
   - validate that the size of the memory's uninitialized space will fit the type
   - if our memory block is an array block (contiguous initializations, only need a count)
     - the first initialization at 0 determines the type
     - because technically the memory can be unallocated
     - if we deallocate all the values, the the layout type will vanish (the next 0 will set the type)
   - store which area is now initialized to what layout

 - I also wonder if uninitialized pointers should only point at uninitialized areas
 - Technically, if you need to UN-initialize something... use an initialized pointer!
   - how do we ensure that the last pointer you delete dissapears...
     - this is where the compiler keeping track of moves is kinda nice, we can "move" the pointer out
     - we could still do this in tower, like after deleting
     - How does rust know when a value has been moved out of a heap type, is there a runtime component that knows it's gone?

-----------------
fn test(b: Vec<i32>) {
    println!("woooo {:?}", b);
}

#[derive(Debug)]
struct Foo {
  z: Vec<i32>,  
}

fn main() {
    let f = Box::new(Foo {
      z: vec![1, 2, 3],
    });
    
    if rand::random() {
        test(f.z);
    }
    println!("Hello, world! {:?}", f);
}
-----------------
error[E0382]: borrow of partially moved value: `f`
  --> src/main.rs:18:36
   |
16 |         test(f.z);
   |              --- value partially moved here
17 |     }
18 |     println!("Hello, world! {:?}", f);
   |                                    ^ value borrowed here after partial move
   |
   = note: partial move occurs because `f.z` has type `Vec<i32>`, which does not implement the `Copy` trait
-----------------

Basically as far as I understand, because they completely keep track of borrows, they know if anyone has borrowed the parent
 - while a child has been potentially moved out (illegal)

For us, we're not going to keep track of any of that
 - at best we could do so inside a single function, but this is a full fledged feature of Rust

we can make it so you can only delete Player* | null, and it nulls after...
 - Actually I do like the idea that it always must be initialized, so either you declare it as nullable
   or you fill it with something else
 - how do you delete a value inside an |?
   - maybe that's just something delete implicitly works with
   - Player* | Enemy* | i32 | null; // how do we delete this?
   - I guess you can cast it first?
   - or delete just works on any union, and will invoke delete on any pointer values stored within
   - but then you have to set the union value afterwards

let a: Player* | Enemy* | i32 | null = ...;

delete a; // since null is present, it will implicitly pick null

let b: Player* | Enemy* | i32 = ...;

delete b; // error, b will be left uninitialized and must be set to a value

delete b (123); // eh... we need to come up with a syntax that's like the allocate syntax kinda

--------

let mem: void* = malloc(sizeof(Player));
let p: Player* = new (mem) Player{};



delete p; // error...


Or deleting p could set the leftover single pointer to null
 - obviously if you have Player* | null, the type will change to null and will force you to check it everywhere
 - at the end of the day, nulling out P would result in a panic if we ever read/wrote to it
   - and we shouldn't be able to perform any pointer manipulation on a null pointer, other than assigning to a new value

- this is one of those 'release unsafe' things, where the Player* doesn't need a check on every access, we just read
  - our language may need to handle traps
  - in wasm, null pointer dereference isn't invalid
  - maybe we make it -1? that way we can actually trap on out of bounds access and see it's "null"
  - I like these panic behaivors

- we can also introduce "unsafe" pointer | null checks
  - basically you can say we assume this pointer is not null in some cases, and the check will vanish
  - it's when we "know" a value is not null
  - oh, better yet you can just do '.' off any pointer, but that's implicitly a panic if it's null
  - basically we're saying that in order to do . off a | null type, we can 

 - as a side note, we could introduce uninitialized typed pointers, basically a pointer that will eventually become typed
   - it would just be something that users could use for type safety
   - Player*, Player uninitialized*... something like that
   - The reason for this would be that you aren't allowed to do pointer manipulation if it goes beyond the size of
     uninitialized memory
   - so technically it acts a little different than just a pure uninitialized memory pointer
   - I wonder what Player* | Player uninitialized* would be...
     - basically just a tagged union of the two identical structured fat pointers
     - still have to check the type...
     - I don't know why this thought is interesting to me...

So a safety guarantee in Tower is that, provided you're not using the raw pointers (we'll have some sort of true unsafe)
 - you can basically provide true access control:
   - if you can't reach an allocation in code, you can't mess with it
   - we can then define "visibility" or access, for compile time features
 - so then the cool thing is unless someone finds a way to exploit one our types (a bug)
   - then people can't mess with memory that isn't theirs
 - then the pointers are all checked and safe
 - and you can ultimately work in this "debug panic" mode that sheds all checks in release (C mode)
   - maybe we just require code coverage tests, lol!
   - I guess that could be a mode, for areas that are covered by code coverage, we allow release optimizations...
   - too far and too fancy I think (basically just PGO...)
--------
when we do array.push(123), we're implicitly passing array as Self
 - at some point, internally, push is going to check if it has memory capacity
   (side note, can we just get this from the memory object, kind of like C++ delete on arrays)?
 - some function is going to eventually operator on memory, either resizing it (deleting and mallocing)
   or it has enough capacity and we don't touch it
 - ultimately, our compiler CAN track that a particular function will attempt to resize

is this something we should push onto the user... basically that memory references can be invalidated...
 - like in this case, `array` obviously owns the memory pointer
   - in fact, it's probably just a unique ptr, but we can get our intrusive fat pointers to it
 - when the array dies, the memory itself will be destructed

we really just want to make sure this case is always detected

can we just avoid having references if pointers always automatically dereference
 - hmm, well then we can't have any methods on the pointers themselves...
   - or we do that Deref thing rust does, where it hides it, sure why not

let a: array(i32) = [1, 2, 3];

i32* foo = a[1];

foo = 123; // am I assigning a pointer here, or a number to the pointer value?
*foo = 123; // should we still have to do this

// no comparison between pointers and i32, so it must be the value of the pointer
if foo > 100 {
}

I guess as long as we're careful with pointer types, we can kind of perform the above
 - but it would be weird if 123 was a value with implicit conversions to either...

that's why references exist in C++
 - we could always introduce references, but I kind of don't want to

I like the idea that you can just use pointers interchangably, and address-of can be implicit too

Self pointers technically can be ignored on cleanup, as they will be released
 - but that's a rare case anyways (maybe sentinel node in a data structure)

one thing, I do like the idea that we can just allocate "memory blocks"
 - but technically the array-like memory block needs to know how many elements it can store
 - because that's how many inlist head pointer's it needs to reserve (one for each element that is initialized)
 - that kind of also tells us we need to know the actual size of the element (because if we know how many bytes)
 - I know initially my concept was that memory blocks could pre-declare the layout
   - maybe that's just required for array like memory blocks
 - The arbitrary memory block will use some reserved avilable space to represent an in memory data structure
   - but if it runs out of space, it will expand memory (realloc first, but otherwise point to a new block of memory)
   - it could technically realloc in place..., that's one of those odd features we can support

one memory type could be copy in out only, no references
 - the only thing we need to make it safe is to ensure that it can't be indexed out of range
 - so we still need a count of how many things initialized
   - normally we do placement new on a type with an uninitialized memory pointer
   - but in this case, we can't get that uninitialized memory pointer
   - we really just need to ask the memory block for the next place to allocate
   - or in this case, maybe we do support an uninitialized memory pointer
     - because technically we still have pointers to the base, there's no issue allowing an uninitialized pointer
       to point anywhere in there
   - but the difference is that when you initialize a type there, you cannot get a pointer to it back
     - e.g. new (that_kind_of_memory) Player() results in a Player, not Player* (or maybe void)
 - we could also allow this type of memory if the pointer is actually just an offset
   - but now the cost is not zero to read/write

I'm really just trying to make the use case for pod data... I don't want a u8 array to be 5x the size in memory
 - the only reason I need an intrusive list header pointer PER array elment is because I want to make sure to null out
   exactly the pointers that need to be (or error) upon uninitialization of the end element(s)
 - the reason for that is because we never want to allow pointers to point at uninitialized data
 - this is so that we know the memory read/write can't corrupt (dereferencing a pointer made from random uninitialized data)
 - we really care a lot less for anything but pod data
   - at most, there are maybe some weirdness with float format and word boundaries
   - but other than that, it's really just pod data is pod data
 - so if the struct is pod, an optimization is that we kind of don't care about tracking initialized values
   - but we do want a guarantee of at least one time initialization, this can be memset, initializing all elements
   - and only pod data can be put in that memory
   - we can point anywhere in it, and there's no invalidation because it's always valid

Hmm, interesting concept
 - if the memory is "always valid" memory, meaning it's initialization is tied with allocation
 - and uninitialization is tied with deallocation
 - then it means the data structure doesn't need to track anything
   - in this case, it's like we're making a fixed array of u8's all initialized to 0
 - fascinating, I love this! this is probably how we do fixed arrays too
 - we can still have an array wrapper that wraps this type of memory and pushes/pops
   - but it means we can get references to anywhere in the array, even when we pop (it's just pod data...)
   - kinda don't like that, but you know maybe we just don't do arrays like that
   - I mean it's fast, we don't really touch the pointers
   - Yeah, actually, it's a good thing for really perf heavy code
   - and I'm sure knowing if a struct is pod (whether it has pointers/memcpy able) is a good optimization feature
 - pod array, cool


we can also have static array types (initialized to a fixed N, all is initialized at once)

it's really just when we have pointers I think

what about structs that have A | B types, are those pod?
 - I think the requirement is that we can't be allowed to put anything in an invalid state
 - well actually, no that doesn't really matter, you can absolutely mangle your own stuff
 - but you can't mess up other people's memory lol (NO GASLIGHTING)
 - We can make sure the format of A | B things is very well defined with tags, and what those tag values will be
 - So you can always predict what the memory layout will be
   - but technically, you can set that tag to a value that isn't valid
   - upon trying to read it or do anything with it, our code will panic
   - I guess one bummer is that we could say the compiler could make optimizations for that
   - I'll see when I get there
 - I know the idea is that any opaque thing we don't want to give the user access to gets it's own layout marker
 - it's basically just that thing that if we muck data, we might muck an algorithm, but that's sort of up to us

Tower, a programming language without gaslighting
 - You can mess up your own memory, but you can't mess up anyone else's!

is const and transitive const more meaningful here?
 - it still really doesn't mean that it's const
 - but does it mean we can have different interfaces that can return const pointers
 - const is just saying whether the pointer is mutable through that pointer
 - in Rust, what makes it so novel is that you can't mutate something while someone else has a borrowed read
 - so it'll never be something where you're looking at something with a "const pointer" and then it suddenly changes

maybe const is a bad name, mut/mutable kind of makes it a little better, because then I have a Player* and Player mut*
 - in my head, it doesn't imply that Player* won't change, just that *I* won't change it

ok I like that, as much as I want it to be C like I think it's better to be explicit about mutability
 - does it ever make sense to keep track of mutable pointers vs read only pointers differently?
 - some sort of enforcement of multi-reader OR single-writer...
   - for threading, we could have as many readers as we want to the same memory
   - it would have to be like a magical intrusive list that's thread safe to add to
 - hmm, just something to think about!

let player: pointer_to mutable Player = ...;

with C style syntax it would be

let player: Player mut* = ...; // pointer to a mutable Player

Yeah, I really like this...


when we get a mutable pointer to a type, in C we could obviously just re-interpret cast it
 - but I think it might be something you want control over
 - almost like returning a safe reference (basically just a pointer, but you can't do "unsafe" stuff to it)

basically our root level security guarantee is:
 - wasm sandbox
 - secure memory access (only get pointers to objects that have granted you access)
   - cannot move the pointers to other objects in memory
 - secure type or struct level layout enforcement

so basically even though we offer "raw" memory access, we can return restrictable types that don't allow
the user to circumvent type enforcement
 - is that what references basically are, just a way to preserve type safety (no pointer arithmetic, only reads/writes)
 - must be an error/panic to destroy any memory that's still being referenced
 - you can usually get pointers from references, but not in this case, it needs to basically be guarded

---------------------------

side note, it's almost like pointers need to say what kind of memory they could point at
 - changes the pointer layout... maybe can do A|B to mean the pointer needs to support either... heh...

-------------

ok, so the tree and components are ready for prime time
 - now we need to just write the parser part
 - the parser needs to produce tower nodes to represent the parse tree
 - and we need to be able to consume tower trees with parser components for building parse rules
 - random idea, we can support sub-rules if we encounter a Production within the rule definition
   - We can use the Production as a NonTerminal (maybe even have them inherit so the layout is the same)
   - But since we know its a Production, we special case it and recursively walk it's children
 - This is how we can define sub-rules, we can even have a parsed format for this too
   - so that we can just define our rules as replacements
   - hmm, doesn't exactly work, we want sub rules that we don't have to reference
   - we really want to do this all in replace
 - and in this case, we're only replacing B*
 - it means we need like scope blocks or whatever


For Kleene closure
  parse E = AB*C;
into
  parse E = AZC;
  parse Z = BZ;
  parse Z = epsilon;

I need a syntax to specify inline rules
 - does it ever make sense for this syntax to be shared anywhere else
 - or like, treated as a scope, is it code?

  parse E = A <parse Z = BZ; parse Z = epsilon;> C;

We (almost) generically want to say add this stuff to the outer scope:
  parse Z = BZ;
  parse Z = epsilon;
- and then result in just Z...

what if the "replace" rule was also just wasm?

parse E = A <parse Z = BZ; parse Z = ;> C;

Expression = Expression '*' => replace(<parse Z = $Expression Z; parse Z = ;>)

This problem of whole statements inside expressions is not a new one
 - I sort of like the idea that we define a syntax for this
 - But I don't like making random gross new syntax
 - () is always used for grouping, and valid in every expression syntax
 - {} is what we use for scope normally
 - in regex, {N, M} usually means, N to M of the previous
 - lets imagine we didn't do that operator

parse E = A {parse Z = BZ; parse Z = ; Z} C;
parse Expression = Expression '*' => replace({parse Z' = $Expression Z'; parse Z' = ; Z'})


Because we make the rule under a scope, the rules are not actually referencable?

----------------
Rename non-terminal, since when we're in the parser we can reference token rules by name, but that's
technically a terminal, not a non-terminal

How about just CharacterRange, CharacterSequence (for 'aaa'), Reference, Production

---------------
one of the earlier primitives we need to introduce for tower is error reporting
 - basically just the error stack like we had in zero for attaching context information (and eventually a real stack)
 - can be a very simple push/pop api, with user_data and a callback
 - or a simple string version too (maybe a string format/interpolant version?)



